{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f07d62f310>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import matplotlib\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "matplotlib.style.use('ggplot')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightning.pytorch as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "torch.manual_seed(33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CELL_LINE_NAME</th>\n",
       "      <th>1:134999</th>\n",
       "      <th>1:135191;1:135218</th>\n",
       "      <th>1:135203;1:135208</th>\n",
       "      <th>1:713376;1:713388;1:713400;1:713448;1:713450;1:713454</th>\n",
       "      <th>1:713901;1:713921;1:714178;1:714182;1:714199;1:714254;1:714258;1:714261;1:714264;1:714277;1:714278;1:714293;1:714301;1:714491;1:714511;1:714566;1:714584</th>\n",
       "      <th>1:715390;1:715392;1:715405;1:715415</th>\n",
       "      <th>1:804993;1:804999;1:805282;1:805284;1:805290;1:805327;1:805338;1:805341;1:805352;1:805445;1:805450;1:805467;1:805468</th>\n",
       "      <th>1:805474;1:805477;1:805479</th>\n",
       "      <th>1:805484;1:805486</th>\n",
       "      <th>...</th>\n",
       "      <th>Dactinomycin</th>\n",
       "      <th>Daporinad</th>\n",
       "      <th>Dasatinib</th>\n",
       "      <th>Rapamycin</th>\n",
       "      <th>Romidepsin</th>\n",
       "      <th>SN-38</th>\n",
       "      <th>Temsirolimus</th>\n",
       "      <th>Trametinib</th>\n",
       "      <th>Vinblastine</th>\n",
       "      <th>Vinorelbine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22rv1</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.41803</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.23610</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.840963</td>\n",
       "      <td>-3.112784</td>\n",
       "      <td>4.203067</td>\n",
       "      <td>-4.382138</td>\n",
       "      <td>-4.917572</td>\n",
       "      <td>-4.972312</td>\n",
       "      <td>-0.974396</td>\n",
       "      <td>1.325603</td>\n",
       "      <td>-4.384381</td>\n",
       "      <td>-3.401996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2313287</td>\n",
       "      <td>0.8897</td>\n",
       "      <td>0.80360</td>\n",
       "      <td>0.99105</td>\n",
       "      <td>0.65648</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.24032</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.584971</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>1.797167</td>\n",
       "      <td>-3.486065</td>\n",
       "      <td>-6.017003</td>\n",
       "      <td>-4.132899</td>\n",
       "      <td>3.279539</td>\n",
       "      <td>-0.040150</td>\n",
       "      <td>-4.849422</td>\n",
       "      <td>-5.303091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42mgba</td>\n",
       "      <td>0.9222</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.49984</td>\n",
       "      <td>0.00897</td>\n",
       "      <td>0.95695</td>\n",
       "      <td>0.02262</td>\n",
       "      <td>0.12001</td>\n",
       "      <td>0.1765</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.240733</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>-2.233603</td>\n",
       "      <td>-3.671158</td>\n",
       "      <td>-6.415517</td>\n",
       "      <td>-4.737156</td>\n",
       "      <td>-0.074308</td>\n",
       "      <td>1.621553</td>\n",
       "      <td>-4.784344</td>\n",
       "      <td>-3.927690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5637</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.95915</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.75370</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03540</td>\n",
       "      <td>0.07694</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.321517</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>-1.829021</td>\n",
       "      <td>-0.116397</td>\n",
       "      <td>-5.559258</td>\n",
       "      <td>-4.537337</td>\n",
       "      <td>1.818908</td>\n",
       "      <td>-0.275098</td>\n",
       "      <td>-3.494500</td>\n",
       "      <td>-5.252216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>639v</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.70308</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.16765</td>\n",
       "      <td>0.05868</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.725271</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>1.391965</td>\n",
       "      <td>-1.822279</td>\n",
       "      <td>-4.955669</td>\n",
       "      <td>-6.639479</td>\n",
       "      <td>1.248546</td>\n",
       "      <td>1.437469</td>\n",
       "      <td>-3.986408</td>\n",
       "      <td>-3.138535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>wsudlcl2</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>0.76473</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.70492</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.72915</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.120747</td>\n",
       "      <td>-6.196767</td>\n",
       "      <td>-1.463565</td>\n",
       "      <td>-5.903482</td>\n",
       "      <td>-5.240336</td>\n",
       "      <td>-4.959568</td>\n",
       "      <td>-5.974346</td>\n",
       "      <td>0.641326</td>\n",
       "      <td>-7.103903</td>\n",
       "      <td>-6.196508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>yapc</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>0.98215</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.96413</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.86138</td>\n",
       "      <td>0.00357</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.932980</td>\n",
       "      <td>-5.076682</td>\n",
       "      <td>1.407784</td>\n",
       "      <td>1.214362</td>\n",
       "      <td>-3.897758</td>\n",
       "      <td>5.073883</td>\n",
       "      <td>1.195720</td>\n",
       "      <td>3.598733</td>\n",
       "      <td>-0.095525</td>\n",
       "      <td>2.307072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>yh13</td>\n",
       "      <td>0.9211</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.97220</td>\n",
       "      <td>0.59999</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.94020</td>\n",
       "      <td>0.01222</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.595530</td>\n",
       "      <td>-3.469021</td>\n",
       "      <td>-0.787238</td>\n",
       "      <td>-2.743620</td>\n",
       "      <td>-5.957548</td>\n",
       "      <td>-3.526360</td>\n",
       "      <td>-1.963500</td>\n",
       "      <td>1.153223</td>\n",
       "      <td>-5.412392</td>\n",
       "      <td>-4.207989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>ykg1</td>\n",
       "      <td>0.8617</td>\n",
       "      <td>0.92310</td>\n",
       "      <td>0.93395</td>\n",
       "      <td>0.82007</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.99590</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.262451</td>\n",
       "      <td>3.115970</td>\n",
       "      <td>0.597641</td>\n",
       "      <td>-0.763934</td>\n",
       "      <td>-5.240336</td>\n",
       "      <td>-3.225249</td>\n",
       "      <td>2.191202</td>\n",
       "      <td>0.407173</td>\n",
       "      <td>-3.494500</td>\n",
       "      <td>-2.601804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>zr7530</td>\n",
       "      <td>0.9091</td>\n",
       "      <td>0.98385</td>\n",
       "      <td>0.96925</td>\n",
       "      <td>0.88890</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.65477</td>\n",
       "      <td>0.02129</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038498</td>\n",
       "      <td>1.574590</td>\n",
       "      <td>4.366079</td>\n",
       "      <td>-0.781208</td>\n",
       "      <td>-2.472102</td>\n",
       "      <td>5.054112</td>\n",
       "      <td>1.928232</td>\n",
       "      <td>5.681084</td>\n",
       "      <td>2.268159</td>\n",
       "      <td>1.959661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>525 rows × 81048 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CELL_LINE_NAME  1:134999  1:135191;1:135218  1:135203;1:135208  \\\n",
       "0            22rv1    0.9821            1.00000            1.00000   \n",
       "1          2313287    0.8897            0.80360            0.99105   \n",
       "2           42mgba    0.9222            0.87500            1.00000   \n",
       "3             5637    1.0000            0.95915            1.00000   \n",
       "4             639v    1.0000            1.00000            1.00000   \n",
       "..             ...       ...                ...                ...   \n",
       "520       wsudlcl2    0.7586            0.76473            1.00000   \n",
       "521           yapc    0.9595            0.98215            1.00000   \n",
       "522           yh13    0.9211            1.00000            0.97220   \n",
       "523           ykg1    0.8617            0.92310            0.93395   \n",
       "524         zr7530    0.9091            0.98385            0.96925   \n",
       "\n",
       "     1:713376;1:713388;1:713400;1:713448;1:713450;1:713454  \\\n",
       "0                                              0.41803       \n",
       "1                                              0.65648       \n",
       "2                                              0.49984       \n",
       "3                                              0.75370       \n",
       "4                                              0.70308       \n",
       "..                                                 ...       \n",
       "520                                            0.70492       \n",
       "521                                            0.96413       \n",
       "522                                            0.59999       \n",
       "523                                            0.82007       \n",
       "524                                            0.88890       \n",
       "\n",
       "     1:713901;1:713921;1:714178;1:714182;1:714199;1:714254;1:714258;1:714261;1:714264;1:714277;1:714278;1:714293;1:714301;1:714491;1:714511;1:714566;1:714584  \\\n",
       "0                                              0.00000                                                                                                          \n",
       "1                                              0.00000                                                                                                          \n",
       "2                                              0.00897                                                                                                          \n",
       "3                                              0.00000                                                                                                          \n",
       "4                                              0.00000                                                                                                          \n",
       "..                                                 ...                                                                                                          \n",
       "520                                            0.00000                                                                                                          \n",
       "521                                            0.00000                                                                                                          \n",
       "522                                            0.00000                                                                                                          \n",
       "523                                            0.00000                                                                                                          \n",
       "524                                            0.00000                                                                                                          \n",
       "\n",
       "     1:715390;1:715392;1:715405;1:715415  \\\n",
       "0                                0.23610   \n",
       "1                                0.24032   \n",
       "2                                0.95695   \n",
       "3                                0.03540   \n",
       "4                                0.16765   \n",
       "..                                   ...   \n",
       "520                              0.72915   \n",
       "521                              0.86138   \n",
       "522                              0.94020   \n",
       "523                              0.99590   \n",
       "524                              0.65477   \n",
       "\n",
       "     1:804993;1:804999;1:805282;1:805284;1:805290;1:805327;1:805338;1:805341;1:805352;1:805445;1:805450;1:805467;1:805468  \\\n",
       "0                                              0.00000                                                                      \n",
       "1                                              0.00000                                                                      \n",
       "2                                              0.02262                                                                      \n",
       "3                                              0.07694                                                                      \n",
       "4                                              0.05868                                                                      \n",
       "..                                                 ...                                                                      \n",
       "520                                            0.00000                                                                      \n",
       "521                                            0.00357                                                                      \n",
       "522                                            0.01222                                                                      \n",
       "523                                            0.00000                                                                      \n",
       "524                                            0.02129                                                                      \n",
       "\n",
       "     1:805474;1:805477;1:805479  1:805484;1:805486  ...  Dactinomycin  \\\n",
       "0                       0.10000             0.0000  ...     -4.840963   \n",
       "1                       0.00000             0.0000  ...     -4.584971   \n",
       "2                       0.12001             0.1765  ...     -4.240733   \n",
       "3                       0.00000             0.0000  ...     -4.321517   \n",
       "4                       0.16667             0.3000  ...     -4.725271   \n",
       "..                          ...                ...  ...           ...   \n",
       "520                     0.00000             0.0000  ...     -5.120747   \n",
       "521                     0.00000             0.0000  ...     -1.932980   \n",
       "522                     0.00000             0.0000  ...     -3.595530   \n",
       "523                     0.00000             0.0000  ...     -5.262451   \n",
       "524                     0.00000             0.0000  ...     -0.038498   \n",
       "\n",
       "     Daporinad  Dasatinib  Rapamycin  Romidepsin     SN-38  Temsirolimus  \\\n",
       "0    -3.112784   4.203067  -4.382138   -4.917572 -4.972312     -0.974396   \n",
       "1    -3.336795   1.797167  -3.486065   -6.017003 -4.132899      3.279539   \n",
       "2    -3.336795  -2.233603  -3.671158   -6.415517 -4.737156     -0.074308   \n",
       "3    -3.336795  -1.829021  -0.116397   -5.559258 -4.537337      1.818908   \n",
       "4    -3.336795   1.391965  -1.822279   -4.955669 -6.639479      1.248546   \n",
       "..         ...        ...        ...         ...       ...           ...   \n",
       "520  -6.196767  -1.463565  -5.903482   -5.240336 -4.959568     -5.974346   \n",
       "521  -5.076682   1.407784   1.214362   -3.897758  5.073883      1.195720   \n",
       "522  -3.469021  -0.787238  -2.743620   -5.957548 -3.526360     -1.963500   \n",
       "523   3.115970   0.597641  -0.763934   -5.240336 -3.225249      2.191202   \n",
       "524   1.574590   4.366079  -0.781208   -2.472102  5.054112      1.928232   \n",
       "\n",
       "     Trametinib  Vinblastine  Vinorelbine  \n",
       "0      1.325603    -4.384381    -3.401996  \n",
       "1     -0.040150    -4.849422    -5.303091  \n",
       "2      1.621553    -4.784344    -3.927690  \n",
       "3     -0.275098    -3.494500    -5.252216  \n",
       "4      1.437469    -3.986408    -3.138535  \n",
       "..          ...          ...          ...  \n",
       "520    0.641326    -7.103903    -6.196508  \n",
       "521    3.598733    -0.095525     2.307072  \n",
       "522    1.153223    -5.412392    -4.207989  \n",
       "523    0.407173    -3.494500    -2.601804  \n",
       "524    5.681084     2.268159     1.959661  \n",
       "\n",
       "[525 rows x 81048 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna_meth = pd.read_csv('new_clean_data/consistent_dna_meth(3omics).csv')\n",
    "dna_meth.drop(columns='Unnamed: 0', inplace=True)\n",
    "dna_meth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CELL_LINE_NAME</th>\n",
       "      <th>ENSG00000000003.10</th>\n",
       "      <th>ENSG00000000005.5</th>\n",
       "      <th>ENSG00000000419.8</th>\n",
       "      <th>ENSG00000000457.9</th>\n",
       "      <th>ENSG00000000460.12</th>\n",
       "      <th>ENSG00000000938.8</th>\n",
       "      <th>ENSG00000000971.11</th>\n",
       "      <th>ENSG00000001036.9</th>\n",
       "      <th>ENSG00000001084.6</th>\n",
       "      <th>...</th>\n",
       "      <th>Dactinomycin</th>\n",
       "      <th>Daporinad</th>\n",
       "      <th>Dasatinib</th>\n",
       "      <th>Rapamycin</th>\n",
       "      <th>Romidepsin</th>\n",
       "      <th>SN-38</th>\n",
       "      <th>Temsirolimus</th>\n",
       "      <th>Trametinib</th>\n",
       "      <th>Vinblastine</th>\n",
       "      <th>Vinorelbine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22rv1</td>\n",
       "      <td>5.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.38</td>\n",
       "      <td>9.76</td>\n",
       "      <td>24.51</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>54.86</td>\n",
       "      <td>118.50</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.840963</td>\n",
       "      <td>-3.112784</td>\n",
       "      <td>4.203067</td>\n",
       "      <td>-4.382138</td>\n",
       "      <td>-4.917572</td>\n",
       "      <td>-4.972312</td>\n",
       "      <td>-0.974396</td>\n",
       "      <td>1.325603</td>\n",
       "      <td>-4.384381</td>\n",
       "      <td>-3.401996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2313287</td>\n",
       "      <td>7.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.99</td>\n",
       "      <td>16.76</td>\n",
       "      <td>13.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>170.91</td>\n",
       "      <td>93.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.584971</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>1.797167</td>\n",
       "      <td>-3.486065</td>\n",
       "      <td>-6.017003</td>\n",
       "      <td>-4.132899</td>\n",
       "      <td>3.279539</td>\n",
       "      <td>-0.040150</td>\n",
       "      <td>-4.849422</td>\n",
       "      <td>-5.303091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42mgba</td>\n",
       "      <td>23.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.28</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.27</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.45</td>\n",
       "      <td>53.29</td>\n",
       "      <td>8.36</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.240733</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>-2.233603</td>\n",
       "      <td>-3.671158</td>\n",
       "      <td>-6.415517</td>\n",
       "      <td>-4.737156</td>\n",
       "      <td>-0.074308</td>\n",
       "      <td>1.621553</td>\n",
       "      <td>-4.784344</td>\n",
       "      <td>-3.927690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5637</td>\n",
       "      <td>57.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.95</td>\n",
       "      <td>3.11</td>\n",
       "      <td>31.61</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.98</td>\n",
       "      <td>71.97</td>\n",
       "      <td>9.76</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.321517</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>-1.829021</td>\n",
       "      <td>-0.116397</td>\n",
       "      <td>-5.559258</td>\n",
       "      <td>-4.537337</td>\n",
       "      <td>1.818908</td>\n",
       "      <td>-0.275098</td>\n",
       "      <td>-3.494500</td>\n",
       "      <td>-5.252216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>639v</td>\n",
       "      <td>32.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125.99</td>\n",
       "      <td>2.69</td>\n",
       "      <td>10.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.89</td>\n",
       "      <td>82.50</td>\n",
       "      <td>24.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.725271</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>1.391965</td>\n",
       "      <td>-1.822279</td>\n",
       "      <td>-4.955669</td>\n",
       "      <td>-6.639479</td>\n",
       "      <td>1.248546</td>\n",
       "      <td>1.437469</td>\n",
       "      <td>-3.986408</td>\n",
       "      <td>-3.138535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>wsudlcl2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.71</td>\n",
       "      <td>5.60</td>\n",
       "      <td>20.21</td>\n",
       "      <td>30.57</td>\n",
       "      <td>0.46</td>\n",
       "      <td>11.78</td>\n",
       "      <td>8.32</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.120747</td>\n",
       "      <td>-6.196767</td>\n",
       "      <td>-1.463565</td>\n",
       "      <td>-5.903482</td>\n",
       "      <td>-5.240336</td>\n",
       "      <td>-4.959568</td>\n",
       "      <td>-5.974346</td>\n",
       "      <td>0.641326</td>\n",
       "      <td>-7.103903</td>\n",
       "      <td>-6.196508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>yapc</td>\n",
       "      <td>50.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103.70</td>\n",
       "      <td>7.76</td>\n",
       "      <td>12.84</td>\n",
       "      <td>0.17</td>\n",
       "      <td>13.58</td>\n",
       "      <td>71.13</td>\n",
       "      <td>29.67</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.932980</td>\n",
       "      <td>-5.076682</td>\n",
       "      <td>1.407784</td>\n",
       "      <td>1.214362</td>\n",
       "      <td>-3.897758</td>\n",
       "      <td>5.073883</td>\n",
       "      <td>1.195720</td>\n",
       "      <td>3.598733</td>\n",
       "      <td>-0.095525</td>\n",
       "      <td>2.307072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>yh13</td>\n",
       "      <td>28.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.09</td>\n",
       "      <td>4.94</td>\n",
       "      <td>13.35</td>\n",
       "      <td>0.22</td>\n",
       "      <td>139.44</td>\n",
       "      <td>118.19</td>\n",
       "      <td>13.53</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.595530</td>\n",
       "      <td>-3.469021</td>\n",
       "      <td>-0.787238</td>\n",
       "      <td>-2.743620</td>\n",
       "      <td>-5.957548</td>\n",
       "      <td>-3.526360</td>\n",
       "      <td>-1.963500</td>\n",
       "      <td>1.153223</td>\n",
       "      <td>-5.412392</td>\n",
       "      <td>-4.207989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>ykg1</td>\n",
       "      <td>61.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>109.13</td>\n",
       "      <td>5.91</td>\n",
       "      <td>17.40</td>\n",
       "      <td>0.13</td>\n",
       "      <td>53.25</td>\n",
       "      <td>92.96</td>\n",
       "      <td>23.09</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.262451</td>\n",
       "      <td>3.115970</td>\n",
       "      <td>0.597641</td>\n",
       "      <td>-0.763934</td>\n",
       "      <td>-5.240336</td>\n",
       "      <td>-3.225249</td>\n",
       "      <td>2.191202</td>\n",
       "      <td>0.407173</td>\n",
       "      <td>-3.494500</td>\n",
       "      <td>-2.601804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>zr7530</td>\n",
       "      <td>16.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.20</td>\n",
       "      <td>13.76</td>\n",
       "      <td>11.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>24.06</td>\n",
       "      <td>47.14</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038498</td>\n",
       "      <td>1.574590</td>\n",
       "      <td>4.366079</td>\n",
       "      <td>-0.781208</td>\n",
       "      <td>-2.472102</td>\n",
       "      <td>5.054112</td>\n",
       "      <td>1.928232</td>\n",
       "      <td>5.681084</td>\n",
       "      <td>2.268159</td>\n",
       "      <td>1.959661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>525 rows × 57831 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CELL_LINE_NAME  ENSG00000000003.10  ENSG00000000005.5  ENSG00000000419.8  \\\n",
       "0            22rv1                5.28                0.0              73.38   \n",
       "1          2313287                7.01                0.0             108.99   \n",
       "2           42mgba               23.09                0.0              99.28   \n",
       "3             5637               57.94                0.0              98.95   \n",
       "4             639v               32.02                0.0             125.99   \n",
       "..             ...                 ...                ...                ...   \n",
       "520       wsudlcl2                0.74                0.0              41.71   \n",
       "521           yapc               50.29                0.0             103.70   \n",
       "522           yh13               28.92                0.0              64.09   \n",
       "523           ykg1               61.08                0.0             109.13   \n",
       "524         zr7530               16.81                0.0              56.20   \n",
       "\n",
       "     ENSG00000000457.9  ENSG00000000460.12  ENSG00000000938.8  \\\n",
       "0                 9.76               24.51               0.01   \n",
       "1                16.76               13.32               0.00   \n",
       "2                 2.73                9.27               0.02   \n",
       "3                 3.11               31.61               0.14   \n",
       "4                 2.69               10.45               0.00   \n",
       "..                 ...                 ...                ...   \n",
       "520               5.60               20.21              30.57   \n",
       "521               7.76               12.84               0.17   \n",
       "522               4.94               13.35               0.22   \n",
       "523               5.91               17.40               0.13   \n",
       "524              13.76               11.53               0.00   \n",
       "\n",
       "     ENSG00000000971.11  ENSG00000001036.9  ENSG00000001084.6  ...  \\\n",
       "0                  0.08              54.86             118.50  ...   \n",
       "1                  0.23             170.91              93.00  ...   \n",
       "2                  0.45              53.29               8.36  ...   \n",
       "3                  1.98              71.97               9.76  ...   \n",
       "4                 12.89              82.50              24.13  ...   \n",
       "..                  ...                ...                ...  ...   \n",
       "520                0.46              11.78               8.32  ...   \n",
       "521               13.58              71.13              29.67  ...   \n",
       "522              139.44             118.19              13.53  ...   \n",
       "523               53.25              92.96              23.09  ...   \n",
       "524                0.03              24.06              47.14  ...   \n",
       "\n",
       "     Dactinomycin  Daporinad  Dasatinib  Rapamycin  Romidepsin     SN-38  \\\n",
       "0       -4.840963  -3.112784   4.203067  -4.382138   -4.917572 -4.972312   \n",
       "1       -4.584971  -3.336795   1.797167  -3.486065   -6.017003 -4.132899   \n",
       "2       -4.240733  -3.336795  -2.233603  -3.671158   -6.415517 -4.737156   \n",
       "3       -4.321517  -3.336795  -1.829021  -0.116397   -5.559258 -4.537337   \n",
       "4       -4.725271  -3.336795   1.391965  -1.822279   -4.955669 -6.639479   \n",
       "..            ...        ...        ...        ...         ...       ...   \n",
       "520     -5.120747  -6.196767  -1.463565  -5.903482   -5.240336 -4.959568   \n",
       "521     -1.932980  -5.076682   1.407784   1.214362   -3.897758  5.073883   \n",
       "522     -3.595530  -3.469021  -0.787238  -2.743620   -5.957548 -3.526360   \n",
       "523     -5.262451   3.115970   0.597641  -0.763934   -5.240336 -3.225249   \n",
       "524     -0.038498   1.574590   4.366079  -0.781208   -2.472102  5.054112   \n",
       "\n",
       "     Temsirolimus  Trametinib  Vinblastine  Vinorelbine  \n",
       "0       -0.974396    1.325603    -4.384381    -3.401996  \n",
       "1        3.279539   -0.040150    -4.849422    -5.303091  \n",
       "2       -0.074308    1.621553    -4.784344    -3.927690  \n",
       "3        1.818908   -0.275098    -3.494500    -5.252216  \n",
       "4        1.248546    1.437469    -3.986408    -3.138535  \n",
       "..            ...         ...          ...          ...  \n",
       "520     -5.974346    0.641326    -7.103903    -6.196508  \n",
       "521      1.195720    3.598733    -0.095525     2.307072  \n",
       "522     -1.963500    1.153223    -5.412392    -4.207989  \n",
       "523      2.191202    0.407173    -3.494500    -2.601804  \n",
       "524      1.928232    5.681084     2.268159     1.959661  \n",
       "\n",
       "[525 rows x 57831 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_exp = pd.read_csv('new_clean_data/consistent_gene_exp(3omics).csv')\n",
    "gene_exp.drop(columns='Unnamed: 0', inplace=True)\n",
    "gene_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CELL_LINE_NAME</th>\n",
       "      <th>14-3-3_beta</th>\n",
       "      <th>14-3-3_epsilon_Caution</th>\n",
       "      <th>14-3-3_zeta</th>\n",
       "      <th>4E-BP1</th>\n",
       "      <th>4E-BP1_pS65</th>\n",
       "      <th>4E-BP1_pT37_T46</th>\n",
       "      <th>4E-BP1_pT70</th>\n",
       "      <th>53BP1</th>\n",
       "      <th>A-Raf_pS299_Caution</th>\n",
       "      <th>...</th>\n",
       "      <th>Dactinomycin</th>\n",
       "      <th>Daporinad</th>\n",
       "      <th>Dasatinib</th>\n",
       "      <th>Rapamycin</th>\n",
       "      <th>Romidepsin</th>\n",
       "      <th>SN-38</th>\n",
       "      <th>Temsirolimus</th>\n",
       "      <th>Trametinib</th>\n",
       "      <th>Vinblastine</th>\n",
       "      <th>Vinorelbine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22rv1</td>\n",
       "      <td>0.108839</td>\n",
       "      <td>0.289584</td>\n",
       "      <td>0.045012</td>\n",
       "      <td>-0.396465</td>\n",
       "      <td>-0.390874</td>\n",
       "      <td>-0.753768</td>\n",
       "      <td>-0.078452</td>\n",
       "      <td>0.407514</td>\n",
       "      <td>-0.079914</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.840963</td>\n",
       "      <td>-3.112784</td>\n",
       "      <td>4.203067</td>\n",
       "      <td>-4.382138</td>\n",
       "      <td>-4.917572</td>\n",
       "      <td>-4.972312</td>\n",
       "      <td>-0.974396</td>\n",
       "      <td>1.325603</td>\n",
       "      <td>-4.384381</td>\n",
       "      <td>-3.401996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2313287</td>\n",
       "      <td>0.133986</td>\n",
       "      <td>0.024192</td>\n",
       "      <td>0.676852</td>\n",
       "      <td>-0.471224</td>\n",
       "      <td>-0.684351</td>\n",
       "      <td>-0.904702</td>\n",
       "      <td>-0.123291</td>\n",
       "      <td>-3.971552</td>\n",
       "      <td>0.037784</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.584971</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>1.797167</td>\n",
       "      <td>-3.486065</td>\n",
       "      <td>-6.017003</td>\n",
       "      <td>-4.132899</td>\n",
       "      <td>3.279539</td>\n",
       "      <td>-0.040150</td>\n",
       "      <td>-4.849422</td>\n",
       "      <td>-5.303091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42mgba</td>\n",
       "      <td>0.426952</td>\n",
       "      <td>0.043067</td>\n",
       "      <td>0.223144</td>\n",
       "      <td>0.189454</td>\n",
       "      <td>0.346910</td>\n",
       "      <td>0.398910</td>\n",
       "      <td>0.274279</td>\n",
       "      <td>-0.763783</td>\n",
       "      <td>-0.178467</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.240733</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>-2.233603</td>\n",
       "      <td>-3.671158</td>\n",
       "      <td>-6.415517</td>\n",
       "      <td>-4.737156</td>\n",
       "      <td>-0.074308</td>\n",
       "      <td>1.621553</td>\n",
       "      <td>-4.784344</td>\n",
       "      <td>-3.927690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5637</td>\n",
       "      <td>0.098320</td>\n",
       "      <td>0.468521</td>\n",
       "      <td>-0.472370</td>\n",
       "      <td>-0.124265</td>\n",
       "      <td>0.005014</td>\n",
       "      <td>-0.280498</td>\n",
       "      <td>-0.241964</td>\n",
       "      <td>0.662198</td>\n",
       "      <td>1.225230</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.321517</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>-1.829021</td>\n",
       "      <td>-0.116397</td>\n",
       "      <td>-5.559258</td>\n",
       "      <td>-4.537337</td>\n",
       "      <td>1.818908</td>\n",
       "      <td>-0.275098</td>\n",
       "      <td>-3.494500</td>\n",
       "      <td>-5.252216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>639v</td>\n",
       "      <td>-0.086995</td>\n",
       "      <td>-0.041387</td>\n",
       "      <td>-0.117372</td>\n",
       "      <td>-0.517712</td>\n",
       "      <td>-0.052923</td>\n",
       "      <td>-0.166216</td>\n",
       "      <td>-0.027035</td>\n",
       "      <td>0.267021</td>\n",
       "      <td>-0.275317</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.725271</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>1.391965</td>\n",
       "      <td>-1.822279</td>\n",
       "      <td>-4.955669</td>\n",
       "      <td>-6.639479</td>\n",
       "      <td>1.248546</td>\n",
       "      <td>1.437469</td>\n",
       "      <td>-3.986408</td>\n",
       "      <td>-3.138535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>wsudlcl2</td>\n",
       "      <td>0.089328</td>\n",
       "      <td>0.209751</td>\n",
       "      <td>0.115950</td>\n",
       "      <td>0.217414</td>\n",
       "      <td>-0.196821</td>\n",
       "      <td>-0.417688</td>\n",
       "      <td>0.248472</td>\n",
       "      <td>-0.718635</td>\n",
       "      <td>0.078394</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.120747</td>\n",
       "      <td>-6.196767</td>\n",
       "      <td>-1.463565</td>\n",
       "      <td>-5.903482</td>\n",
       "      <td>-5.240336</td>\n",
       "      <td>-4.959568</td>\n",
       "      <td>-5.974346</td>\n",
       "      <td>0.641326</td>\n",
       "      <td>-7.103903</td>\n",
       "      <td>-6.196508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>yapc</td>\n",
       "      <td>0.102917</td>\n",
       "      <td>-0.167794</td>\n",
       "      <td>0.426968</td>\n",
       "      <td>-0.306847</td>\n",
       "      <td>-0.333987</td>\n",
       "      <td>-0.697848</td>\n",
       "      <td>-0.420424</td>\n",
       "      <td>0.145249</td>\n",
       "      <td>0.046873</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.932980</td>\n",
       "      <td>-5.076682</td>\n",
       "      <td>1.407784</td>\n",
       "      <td>1.214362</td>\n",
       "      <td>-3.897758</td>\n",
       "      <td>5.073883</td>\n",
       "      <td>1.195720</td>\n",
       "      <td>3.598733</td>\n",
       "      <td>-0.095525</td>\n",
       "      <td>2.307072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>yh13</td>\n",
       "      <td>-0.272537</td>\n",
       "      <td>-0.165541</td>\n",
       "      <td>-0.597877</td>\n",
       "      <td>-0.608083</td>\n",
       "      <td>-0.088654</td>\n",
       "      <td>-0.550212</td>\n",
       "      <td>-0.011799</td>\n",
       "      <td>0.236974</td>\n",
       "      <td>-0.441684</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.595530</td>\n",
       "      <td>-3.469021</td>\n",
       "      <td>-0.787238</td>\n",
       "      <td>-2.743620</td>\n",
       "      <td>-5.957548</td>\n",
       "      <td>-3.526360</td>\n",
       "      <td>-1.963500</td>\n",
       "      <td>1.153223</td>\n",
       "      <td>-5.412392</td>\n",
       "      <td>-4.207989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>ykg1</td>\n",
       "      <td>-0.292245</td>\n",
       "      <td>0.105827</td>\n",
       "      <td>-0.336982</td>\n",
       "      <td>-0.207507</td>\n",
       "      <td>0.264828</td>\n",
       "      <td>0.272116</td>\n",
       "      <td>0.285140</td>\n",
       "      <td>0.482773</td>\n",
       "      <td>-0.261400</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.262451</td>\n",
       "      <td>3.115970</td>\n",
       "      <td>0.597641</td>\n",
       "      <td>-0.763934</td>\n",
       "      <td>-5.240336</td>\n",
       "      <td>-3.225249</td>\n",
       "      <td>2.191202</td>\n",
       "      <td>0.407173</td>\n",
       "      <td>-3.494500</td>\n",
       "      <td>-2.601804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>zr7530</td>\n",
       "      <td>-0.040076</td>\n",
       "      <td>-0.265286</td>\n",
       "      <td>1.017082</td>\n",
       "      <td>-0.666393</td>\n",
       "      <td>-0.367123</td>\n",
       "      <td>-0.784569</td>\n",
       "      <td>-0.575248</td>\n",
       "      <td>-0.494944</td>\n",
       "      <td>0.066445</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038498</td>\n",
       "      <td>1.574590</td>\n",
       "      <td>4.366079</td>\n",
       "      <td>-0.781208</td>\n",
       "      <td>-2.472102</td>\n",
       "      <td>5.054112</td>\n",
       "      <td>1.928232</td>\n",
       "      <td>5.681084</td>\n",
       "      <td>2.268159</td>\n",
       "      <td>1.959661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>525 rows × 225 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CELL_LINE_NAME  14-3-3_beta  14-3-3_epsilon_Caution  14-3-3_zeta  \\\n",
       "0            22rv1     0.108839                0.289584     0.045012   \n",
       "1          2313287     0.133986                0.024192     0.676852   \n",
       "2           42mgba     0.426952                0.043067     0.223144   \n",
       "3             5637     0.098320                0.468521    -0.472370   \n",
       "4             639v    -0.086995               -0.041387    -0.117372   \n",
       "..             ...          ...                     ...          ...   \n",
       "520       wsudlcl2     0.089328                0.209751     0.115950   \n",
       "521           yapc     0.102917               -0.167794     0.426968   \n",
       "522           yh13    -0.272537               -0.165541    -0.597877   \n",
       "523           ykg1    -0.292245                0.105827    -0.336982   \n",
       "524         zr7530    -0.040076               -0.265286     1.017082   \n",
       "\n",
       "       4E-BP1  4E-BP1_pS65  4E-BP1_pT37_T46  4E-BP1_pT70     53BP1  \\\n",
       "0   -0.396465    -0.390874        -0.753768    -0.078452  0.407514   \n",
       "1   -0.471224    -0.684351        -0.904702    -0.123291 -3.971552   \n",
       "2    0.189454     0.346910         0.398910     0.274279 -0.763783   \n",
       "3   -0.124265     0.005014        -0.280498    -0.241964  0.662198   \n",
       "4   -0.517712    -0.052923        -0.166216    -0.027035  0.267021   \n",
       "..        ...          ...              ...          ...       ...   \n",
       "520  0.217414    -0.196821        -0.417688     0.248472 -0.718635   \n",
       "521 -0.306847    -0.333987        -0.697848    -0.420424  0.145249   \n",
       "522 -0.608083    -0.088654        -0.550212    -0.011799  0.236974   \n",
       "523 -0.207507     0.264828         0.272116     0.285140  0.482773   \n",
       "524 -0.666393    -0.367123        -0.784569    -0.575248 -0.494944   \n",
       "\n",
       "     A-Raf_pS299_Caution  ...  Dactinomycin  Daporinad  Dasatinib  Rapamycin  \\\n",
       "0              -0.079914  ...     -4.840963  -3.112784   4.203067  -4.382138   \n",
       "1               0.037784  ...     -4.584971  -3.336795   1.797167  -3.486065   \n",
       "2              -0.178467  ...     -4.240733  -3.336795  -2.233603  -3.671158   \n",
       "3               1.225230  ...     -4.321517  -3.336795  -1.829021  -0.116397   \n",
       "4              -0.275317  ...     -4.725271  -3.336795   1.391965  -1.822279   \n",
       "..                   ...  ...           ...        ...        ...        ...   \n",
       "520             0.078394  ...     -5.120747  -6.196767  -1.463565  -5.903482   \n",
       "521             0.046873  ...     -1.932980  -5.076682   1.407784   1.214362   \n",
       "522            -0.441684  ...     -3.595530  -3.469021  -0.787238  -2.743620   \n",
       "523            -0.261400  ...     -5.262451   3.115970   0.597641  -0.763934   \n",
       "524             0.066445  ...     -0.038498   1.574590   4.366079  -0.781208   \n",
       "\n",
       "     Romidepsin     SN-38  Temsirolimus  Trametinib  Vinblastine  Vinorelbine  \n",
       "0     -4.917572 -4.972312     -0.974396    1.325603    -4.384381    -3.401996  \n",
       "1     -6.017003 -4.132899      3.279539   -0.040150    -4.849422    -5.303091  \n",
       "2     -6.415517 -4.737156     -0.074308    1.621553    -4.784344    -3.927690  \n",
       "3     -5.559258 -4.537337      1.818908   -0.275098    -3.494500    -5.252216  \n",
       "4     -4.955669 -6.639479      1.248546    1.437469    -3.986408    -3.138535  \n",
       "..          ...       ...           ...         ...          ...          ...  \n",
       "520   -5.240336 -4.959568     -5.974346    0.641326    -7.103903    -6.196508  \n",
       "521   -3.897758  5.073883      1.195720    3.598733    -0.095525     2.307072  \n",
       "522   -5.957548 -3.526360     -1.963500    1.153223    -5.412392    -4.207989  \n",
       "523   -5.240336 -3.225249      2.191202    0.407173    -3.494500    -2.601804  \n",
       "524   -2.472102  5.054112      1.928232    5.681084     2.268159     1.959661  \n",
       "\n",
       "[525 rows x 225 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rppa = pd.read_csv('new_clean_data/consistent_rppa(3omics).csv')\n",
    "rppa.drop(columns='Unnamed: 0', inplace=True)\n",
    "rppa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns\n",
    "cell_line_cols = ['CELL_LINE_NAME']\n",
    "drug_resp_cols = ['Dactinomycin', 'Daporinad', 'Dasatinib',\t'Rapamycin', 'Romidepsin', 'SN-38',\t'Temsirolimus',\t'Trametinib', 'Vinblastine', 'Vinorelbine']\n",
    "dna_meth_cols = dna_meth.drop(columns=['CELL_LINE_NAME','Dactinomycin', 'Daporinad', 'Dasatinib',\t'Rapamycin', 'Romidepsin', 'SN-38',\t'Temsirolimus',\t'Trametinib', 'Vinblastine', 'Vinorelbine']).columns\n",
    "gene_exp_cols = gene_exp.drop(columns=['CELL_LINE_NAME','Dactinomycin', 'Daporinad', 'Dasatinib',\t'Rapamycin', 'Romidepsin', 'SN-38',\t'Temsirolimus',\t'Trametinib', 'Vinblastine', 'Vinorelbine']).columns\n",
    "rppa_cols = rppa.drop(columns=['CELL_LINE_NAME','Dactinomycin', 'Daporinad', 'Dasatinib',\t'Rapamycin', 'Romidepsin', 'SN-38',\t'Temsirolimus',\t'Trametinib', 'Vinblastine', 'Vinorelbine']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drug response labels to tensor\n",
    "drug_resp_dna = np.stack([dna_meth[col].values for col in drug_resp_cols], 1)\n",
    "drug_resp_dna = torch.tensor(drug_resp_dna, dtype=torch.float)\n",
    "drug_resp_gene = np.stack([gene_exp[col].values for col in drug_resp_cols], 1)\n",
    "drug_resp_gene = torch.tensor(drug_resp_gene, dtype=torch.float)\n",
    "drug_resp_rppa = np.stack([rppa[col].values for col in drug_resp_cols], 1)\n",
    "drug_resp_rppa = torch.tensor(drug_resp_rppa, dtype=torch.float)\n",
    "drug_resp = drug_resp_dna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dna_meth values to tensor\n",
    "dna_meth_values = np.stack([dna_meth[col].values for col in dna_meth_cols], 1)\n",
    "dna_meth_values = torch.tensor(dna_meth_values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_exp values to tensor and scale from 0 to 1\n",
    "gene_exp_values = np.stack([gene_exp[col].values for col in gene_exp_cols], 1)\n",
    "scaler = MinMaxScaler()\n",
    "gene_exp_values_t = gene_exp_values.T\n",
    "scaled_gene_exp_values_t = scaler.fit_transform(gene_exp_values_t)\n",
    "scaled_gene_exp_values = scaled_gene_exp_values_t.T\n",
    "scaled_gene_exp_values = torch.tensor(scaled_gene_exp_values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rppa values to tensor and scale from 0 to 1\n",
    "rppa_values = np.stack([rppa[col].values for col in rppa_cols], 1)\n",
    "scaler = MinMaxScaler()\n",
    "rppa_values_t = rppa_values.T\n",
    "scaled_rppa_values_t = scaler.fit_transform(rppa_values_t)\n",
    "scaled_rppa_values = scaled_rppa_values_t.T\n",
    "scaled_rppa_values = torch.tensor(scaled_rppa_values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "full_dataset = torch.utils.data.TensorDataset(dna_meth_values, scaled_gene_exp_values, scaled_rppa_values, drug_resp)\n",
    "full_loader = DataLoader(full_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE model with Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_with_regressor(nn.Module):\n",
    "    def __init__(self, input_size_dna, input_size_gene,input_size_rppa, level_2, level_3, latent_dim, shared_size, independent_size):\n",
    "        super(VAE_with_regressor, self).__init__()\n",
    "\n",
    "        self.input_size_dna = input_size_dna\n",
    "        self.input_size_gene = input_size_gene\n",
    "        self.input_size_rppa = input_size_rppa\n",
    "        self.level_2 = level_2\n",
    "        self.level_3 = level_3\n",
    "        self.latent_dim = latent_dim\n",
    "        self.shared_size = shared_size\n",
    "        self.independent_size = independent_size\n",
    "\n",
    "        # Encoder DNA\n",
    "        self.enc_fc1_dna = nn.Sequential(\n",
    "                        nn.Linear(input_size_dna, level_2),\n",
    "                        nn.BatchNorm1d(level_2),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.enc_fc2_dna = nn.Sequential(\n",
    "                        nn.Linear(level_2, level_3),\n",
    "                        nn.BatchNorm1d(level_3),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.enc_fc3_dna_mean = nn.Sequential(\n",
    "                    nn.Linear(level_3, latent_dim),\n",
    "                    nn.BatchNorm1d(latent_dim))\n",
    "        \n",
    "        self.enc_fc3_dna_log_var = nn.Sequential(\n",
    "                    nn.Linear(level_3, latent_dim),\n",
    "                    nn.BatchNorm1d(latent_dim))\n",
    "        \n",
    "        # Decoder DNA\n",
    "        self.dec_fc3_dna = nn.Sequential(\n",
    "                        nn.Linear(latent_dim, level_3),\n",
    "                        nn.BatchNorm1d(level_3),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.dec_fc2_dna = nn.Sequential(\n",
    "                        nn.Linear(level_3, level_2),\n",
    "                        nn.BatchNorm1d(level_2),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.dec_fc1_dna = nn.Sequential(\n",
    "                    nn.Linear(level_2, input_size_dna),\n",
    "                    nn.BatchNorm1d(input_size_dna),\n",
    "                    nn.Sigmoid())\n",
    "        \n",
    "        # Encoder Gene\n",
    "        self.enc_fc1_gene = nn.Sequential(\n",
    "                        nn.Linear(input_size_gene, level_2),\n",
    "                        nn.BatchNorm1d(level_2),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.enc_fc2_gene = nn.Sequential(\n",
    "                        nn.Linear(level_2, level_3),\n",
    "                        nn.BatchNorm1d(level_3),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.enc_fc3_gene_mean = nn.Sequential(\n",
    "                    nn.Linear(level_3, latent_dim),\n",
    "                    nn.BatchNorm1d(latent_dim))\n",
    "        \n",
    "        self.enc_fc3_gene_log_var = nn.Sequential(\n",
    "                    nn.Linear(level_3, latent_dim),\n",
    "                    nn.BatchNorm1d(latent_dim))\n",
    "        \n",
    "        # Decoder Gene\n",
    "        self.dec_fc3_gene = nn.Sequential(\n",
    "                        nn.Linear(latent_dim, level_3),\n",
    "                        nn.BatchNorm1d(level_3),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.dec_fc2_gene = nn.Sequential(\n",
    "                        nn.Linear(level_3, level_2),\n",
    "                        nn.BatchNorm1d(level_2),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.dec_fc1_gene = nn.Sequential(\n",
    "                    nn.Linear(level_2, input_size_gene),\n",
    "                    nn.BatchNorm1d(input_size_gene),\n",
    "                    nn.Sigmoid())\n",
    "        \n",
    "         # Encoder RPPA\n",
    "        self.enc_fc1_rppa = nn.Sequential(\n",
    "                        nn.Linear(input_size_rppa, latent_dim),\n",
    "                        nn.BatchNorm1d(latent_dim),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        # self.enc_fc2_rppa = nn.Sequential(\n",
    "        #                 nn.Linear(level_2, level_3),\n",
    "        #                 nn.BatchNorm1d(level_3),\n",
    "        #                 nn.ReLU())\n",
    "        \n",
    "        self.enc_fc2_rppa_mean = nn.Sequential(\n",
    "                    nn.Linear(latent_dim, latent_dim),\n",
    "                    nn.BatchNorm1d(latent_dim))\n",
    "        \n",
    "        self.enc_fc2_rppa_log_var = nn.Sequential(\n",
    "                    nn.Linear(latent_dim, latent_dim),\n",
    "                    nn.BatchNorm1d(latent_dim))\n",
    "        \n",
    "        # Decoder RPPA\n",
    "        self.dec_fc2_rppa = nn.Sequential(\n",
    "                        nn.Linear(latent_dim, latent_dim),\n",
    "                        nn.BatchNorm1d(latent_dim),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        # self.dec_fc2_rppa = nn.Sequential(\n",
    "        #                 nn.Linear(level_3, level_2),\n",
    "        #                 nn.BatchNorm1d(level_2),\n",
    "        #                 nn.ReLU())\n",
    "        \n",
    "        self.dec_fc1_rppa = nn.Sequential(\n",
    "                    nn.Linear(latent_dim, input_size_rppa),\n",
    "                    nn.BatchNorm1d(input_size_rppa),\n",
    "                    nn.Sigmoid())\n",
    "        \n",
    "        # attention layer\n",
    "        self.attention = MultiheadAttention(embed_dim=(independent_size * 3 + shared_size), num_heads=int((independent_size * 3 + shared_size)))\n",
    "        \n",
    "        # Regression fc layers\n",
    "        self.r_fc1 = nn.Sequential(\n",
    "                    nn.Linear(independent_size * 3 + shared_size, 64),\n",
    "                    nn.BatchNorm1d(64),\n",
    "                    nn.ReLU())\n",
    "        self.r_fc2 = nn.Sequential(\n",
    "                    nn.Linear(64, 32),\n",
    "                    nn.BatchNorm1d(32),\n",
    "                    nn.ReLU())\n",
    "        \n",
    "        self.r_fc3 = nn.Sequential(\n",
    "                    nn.Linear(32, 10),\n",
    "                    nn.BatchNorm1d(10))\n",
    "        \n",
    "    def encode(self, x):\n",
    "        dna_l2_layer = self.enc_fc1_dna(x[0])\n",
    "        dna_l3_layer = self.enc_fc2_dna(dna_l2_layer)\n",
    "        mu_dna = self.enc_fc3_dna_mean(dna_l3_layer)\n",
    "        logvar_dna = self.enc_fc3_dna_log_var(dna_l3_layer)\n",
    "\n",
    "        gene_l2_layer = self.enc_fc1_gene(x[1])\n",
    "        gene_l3_layer = self.enc_fc2_gene(gene_l2_layer)\n",
    "        mu_gene = self.enc_fc3_gene_mean(gene_l3_layer)\n",
    "        logvar_gene = self.enc_fc3_gene_log_var(gene_l3_layer)\n",
    "\n",
    "        rppa_l2_layer = self.enc_fc1_rppa(x[2])\n",
    "        mu_rppa = self.enc_fc2_rppa_mean(rppa_l2_layer)\n",
    "        logvar_rppa = self.enc_fc2_rppa_log_var(rppa_l2_layer)\n",
    "\n",
    "        u_dna, s_dna = torch.split(mu_dna, [self.independent_size, self.shared_size], -1)\n",
    "        u_gene, s_gene = torch.split(mu_gene, [self.independent_size, self.shared_size], -1)\n",
    "        u_rppa, s_rppa = torch.split(mu_rppa, [self.independent_size, self.shared_size], -1)\n",
    "        shared_mean = torch.mean(torch.stack([s_dna, s_gene, s_rppa]), dim=0)\n",
    "        for_pred = torch.cat((u_dna, shared_mean, u_gene, u_rppa), dim=1)\n",
    "\n",
    "        mu_for_pred = for_pred.unsqueeze(0).transpose(0, 1)\n",
    "        attended_mean, _ = self.attention(mu_for_pred,mu_for_pred,mu_for_pred)\n",
    "        attended_mean = attended_mean.transpose(0, 1).squeeze(0)\n",
    "        # attended_mean = for_pred\n",
    "\n",
    "        return mu_dna, logvar_dna, mu_gene, logvar_gene, attended_mean, u_dna, s_dna, u_gene, s_gene, u_rppa, s_rppa, mu_rppa, logvar_rppa\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def decode_dna(self, z):\n",
    "        l3_dna_layer = self.dec_fc3_dna(z)\n",
    "        l2_dna_layer = self.dec_fc2_gene(l3_dna_layer)\n",
    "        recon_dna = self.dec_fc1_dna(l2_dna_layer)\n",
    "        return recon_dna\n",
    "    \n",
    "    def decode_gene(self,z):\n",
    "        l3_gene_layer = self.dec_fc3_gene(z)\n",
    "        l2_gene_layer = self.dec_fc2_gene(l3_gene_layer)\n",
    "        recon_gene = self. dec_fc1_gene(l2_gene_layer)\n",
    "        return recon_gene\n",
    "    \n",
    "    def decode_rppa(self,z):\n",
    "        l2_rppa_layer = self.dec_fc2_rppa(z)\n",
    "        recon_rppa = self. dec_fc1_rppa(l2_rppa_layer)\n",
    "        return recon_rppa\n",
    "    \n",
    "    def regressor(self, mean):\n",
    "        level_1_layer = self.r_fc1(mean)\n",
    "        level_2_layer = self.r_fc2(level_1_layer)\n",
    "        output_layer = self.r_fc3(level_2_layer)\n",
    "        return output_layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu_dna, logvar_dna, mu_gene, logvar_gene, attended_mean, u_dna, s_dna, u_gene, s_gene, u_rppa, s_rppa, mu_rppa, logvar_rppa = self.encode(x)\n",
    "        z_dna = self.reparameterize(mu_dna, logvar_dna)\n",
    "        z_gene = self.reparameterize(mu_gene, logvar_gene)\n",
    "        z_rppa = self.reparameterize(mu_rppa, logvar_rppa)\n",
    "        recon_dna = self.decode_dna(z_dna)\n",
    "        recon_gene = self.decode_gene(z_gene)\n",
    "        recon_rppa = self.decode_rppa(z_rppa)\n",
    "\n",
    "        \n",
    "        y_pred = self.regressor(attended_mean)\n",
    "\n",
    "        return recon_dna, recon_gene, recon_rppa, mu_dna, logvar_dna, mu_gene, logvar_gene, mu_rppa, logvar_rppa, u_dna, s_dna, u_gene, s_gene, u_rppa, s_rppa, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function(x_hat, x, mean, log_var): # recon loss and kld loss\n",
    "        bce = torch.nn.functional.binary_cross_entropy(x_hat, x, reduction = 'sum')\n",
    "        kld = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "        loss = kld + bce\n",
    "        return loss, kld, bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss_function(y_pred, y):\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "    loss = torch.sqrt(loss_fn(y_pred, y))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_function(y_pred, y):\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_function_mean(y_pred, y):\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_function_3omics(y1, y2, y3):\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "    loss1 = loss_fn(y1, y2)\n",
    "    loss2 = loss_fn(y1, y3)\n",
    "    loss3 = loss_fn(y2, y3)\n",
    "    combined_loss = (loss1 + loss2 + loss3) / 3\n",
    "    return combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndependenceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IndependenceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, batch1, batch2, batch3):\n",
    "        # Compute the covariance matrices for each pair of batches\n",
    "        cov_matrix1 = torch.matmul(batch1.unsqueeze(2), batch2.unsqueeze(1))\n",
    "        cov_matrix2 = torch.matmul(batch1.unsqueeze(2), batch3.unsqueeze(1))\n",
    "        cov_matrix3 = torch.matmul(batch2.unsqueeze(2), batch3.unsqueeze(1))\n",
    "\n",
    "        # Compute the Frobenius norm of each covariance matrix\n",
    "        loss1 = torch.norm(cov_matrix1, dim=(1, 2), p='fro')\n",
    "        loss2 = torch.norm(cov_matrix2, dim=(1, 2), p='fro')\n",
    "        loss3 = torch.norm(cov_matrix3, dim=(1, 2), p='fro')\n",
    "\n",
    "        # Calculate the mean of the individual losses\n",
    "        loss_mean = (loss1 + loss2 + loss3) / 3.0\n",
    "        loss_mean = loss_mean.mean()\n",
    "\n",
    "        return loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "independence_loss = IndependenceLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y_true, y_pred):\n",
    "    y_true_mean = torch.mean(y_true)\n",
    "    SS_res = torch.sum(torch.square(y_true - y_pred))\n",
    "    SS_tot = torch.sum(torch.square(y_true - y_true_mean))\n",
    "    r_squared = 1 - SS_res / (SS_tot + torch.finfo(torch.float32).eps)\n",
    "    return r_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "lr = 0.001\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda')\n",
    "\n",
    "k = 10\n",
    "splits = KFold(n_splits=k, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "input_size_dna = 81037\n",
    "input_size_gene = 57820\n",
    "input_size_rppa = 214\n",
    "level_2 = 2048\n",
    "level_3 = 1024\n",
    "latent_dim = 128 # target latent size\n",
    "\n",
    "shared_size = 10\n",
    "independent_size = 118\n",
    "\n",
    "model = VAE_with_regressor(input_size_dna, input_size_gene,input_size_rppa, level_2, level_3, latent_dim, shared_size, independent_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (model, dataloader, epoch, optimizer, w_recon_loss, w_kl_loss, w_reg_loss, w_shared_loss, w_ind_loss):\n",
    "    model.train()\n",
    "    train_recon = 0\n",
    "    train_kl  = 0\n",
    "    train_reg = 0\n",
    "    train_shared = 0\n",
    "    train_ind = 0\n",
    "    batch_num = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        batch_num = batch_num + 1\n",
    "        # get values\n",
    "        dna_meth_values, gene_exp_values, rppa_values, drug_resp = batch\n",
    "        dna_meth_values = dna_meth_values.to(device)\n",
    "        gene_exp_values = gene_exp_values.to(device)\n",
    "        rppa_values = rppa_values.to(device)\n",
    "        drug_resp = drug_resp.to(device)\n",
    "\n",
    "        dna_plus_gene = [dna_meth_values, gene_exp_values, rppa_values]\n",
    "\n",
    "        \n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        # apply model\n",
    "        recon_dna, recon_gene, recon_rppa, mu_dna, logvar_dna, mu_gene, logvar_gene, mu_rppa, logvar_rppa, u_dna, s_dna, u_gene, s_gene, u_rppa, s_rppa, y_pred = model(dna_plus_gene)\n",
    "\n",
    "        # losses\n",
    "        dna_vae_loss, dna_kld, dna_bce = vae_loss_function(recon_dna, dna_meth_values, mu_dna, logvar_dna)\n",
    "        gene_vae_loss, gene_kld, gene_bce = vae_loss_function(recon_gene, gene_exp_values, mu_gene, logvar_gene)\n",
    "        rppa_vae_loss, rppa_kld, rppa_bce = vae_loss_function(recon_rppa, rppa_values, mu_rppa, logvar_rppa)\n",
    "\n",
    "        shared_loss = mse_loss_function_3omics(s_dna, s_gene, s_rppa)\n",
    "        ind_loss = independence_loss(u_dna, u_gene, u_rppa)\n",
    "\n",
    "        reg_loss = mse_loss_function_mean(y_pred, drug_resp)\n",
    "\n",
    "        recon_loss = (1/3) * (dna_bce + gene_bce + rppa_bce)\n",
    "        kld_loss = (1/3) * (dna_kld + gene_kld + rppa_kld)\n",
    "\n",
    "        loss = w_recon_loss * recon_loss + w_kl_loss * kld_loss + w_reg_loss * reg_loss + w_shared_loss * shared_loss + w_ind_loss * ind_loss\n",
    "\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            train_recon += recon_loss.item()\n",
    "            train_kl += kld_loss.item()\n",
    "            train_reg += reg_loss.item()\n",
    "            train_shared += shared_loss.item()\n",
    "            train_ind += ind_loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_recon_ave = train_recon/ len(dataloader.sampler)\n",
    "    train_kl_ave = train_kl/ len(dataloader.sampler)\n",
    "    train_reg_ave = train_reg/ batch_num\n",
    "    train_shared_ave = train_shared/ batch_num\n",
    "    train_ind_ave = train_ind/batch_num\n",
    "\n",
    "    print('=====> Epoch {} \\n' \n",
    "          'Average Recon Loss: {:.3f} \\n'\n",
    "          'Average KL Loss: {:.3f} \\n'\n",
    "          'Average Regression Loss: {:.3f} \\n'\n",
    "          'Average Shared Loss: {:.3f} \\n'\n",
    "          'Average Independence Loss: {:.3f} \\n'.format(epoch, train_recon_ave, train_kl_ave, train_reg_ave, train_shared_ave, train_ind_ave))\n",
    "    return train_recon_ave, train_kl_ave, train_reg_ave, train_shared_ave, train_ind_ave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model):\n",
    "    model.eval()\n",
    "\n",
    "    test_recon_dna = 0\n",
    "    test_recon_gene = 0\n",
    "    test_recon_rppa = 0\n",
    "    test_kl_dna = 0\n",
    "    test_kl_gene = 0\n",
    "    test_kl_rppa = 0\n",
    "\n",
    "    test_reg = 0\n",
    "    test_rmse = 0\n",
    "    test_shared = 0\n",
    "    test_shared_rmse = 0\n",
    "    test_r2 = 0\n",
    "    test_ind_loss = 0\n",
    "    \n",
    "\n",
    "    batch_num = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            batch_num = batch_num + 1\n",
    "            \n",
    "            # get values\n",
    "            dna_meth_values, gene_exp_values, rppa_values, drug_resp = batch\n",
    "            dna_meth_values = dna_meth_values.to(device)\n",
    "            gene_exp_values = gene_exp_values.to(device)\n",
    "            rppa_values = rppa_values.to(device)\n",
    "            drug_resp = drug_resp.to(device)\n",
    "\n",
    "            dna_plus_gene = [dna_meth_values, gene_exp_values, rppa_values]\n",
    "\n",
    "            recon_dna, recon_gene, recon_rppa, mu_dna, logvar_dna, mu_gene, logvar_gene, mu_rppa, logvar_rppa, u_dna, s_dna, u_gene, s_gene, u_rppa, s_rppa, y_pred = model(dna_plus_gene)\n",
    "\n",
    "             # losses\n",
    "            dna_vae_loss, dna_kld, dna_bce = vae_loss_function(recon_dna, dna_meth_values, mu_dna, logvar_dna)\n",
    "            gene_vae_loss, gene_kld, gene_bce = vae_loss_function(recon_gene, gene_exp_values, mu_gene, logvar_gene)\n",
    "            rppa_vae_loss, rppa_kld, rppa_bce = vae_loss_function(recon_rppa, rppa_values, mu_rppa, logvar_rppa)\n",
    "\n",
    "            shared_loss = mse_loss_function_3omics(s_dna, s_gene, s_rppa)\n",
    "            ind_loss = independence_loss(u_dna, u_gene, u_rppa)\n",
    "\n",
    "            reg_loss = mse_loss_function_mean(y_pred, drug_resp)\n",
    "            r2_value = r_squared(drug_resp, y_pred)\n",
    "\n",
    "            test_recon_dna += dna_bce.item()\n",
    "            test_recon_gene += gene_bce.item()\n",
    "            test_recon_rppa += rppa_bce.item()\n",
    "            test_kl_dna += dna_kld.item()\n",
    "            test_kl_gene += gene_kld.item()\n",
    "            test_kl_rppa += rppa_kld.item()\n",
    "\n",
    "            test_reg += reg_loss.item()\n",
    "            test_shared += shared_loss.item()\n",
    "            test_r2 += r2_value.item()\n",
    "            test_ind_loss += ind_loss.item()\n",
    "\n",
    "        # print loss\n",
    "        test_recon_dna_ave = test_recon_dna/len(dataloader.sampler)\n",
    "        test_recon_gene_ave = test_recon_gene/len(dataloader.sampler)\n",
    "        test_recon_rppa_ave = test_recon_rppa/len(dataloader.sampler)\n",
    "        test_kl_dna_ave = test_kl_dna/ len(dataloader.sampler)\n",
    "        test_kl_gene_ave = test_kl_gene/ len(dataloader.sampler)\n",
    "        test_kld_rppa_ave = test_kl_rppa/ len(dataloader.sampler)\n",
    "\n",
    "        test_reg_ave = test_reg/ (len(dataloader.sampler) * 10)\n",
    "        test_rmse_ave = math.sqrt(test_reg_ave)\n",
    "        test_shared_ave = test_shared/ (len(dataloader.sampler) * 10)\n",
    "        test_shared_rmse_ave = math.sqrt(test_shared_ave)\n",
    "        test_r2_ave = test_r2/ batch_num\n",
    "        test_ind_ave = test_ind_loss/ batch_num\n",
    "\n",
    "        print('Average DNA Recon Loss: {:.3f} \\n'\n",
    "              'Average Gene Recon Loss: {:.3f} \\n'\n",
    "              'Average RPPA Recon Loss: {:.3f} \\n'\n",
    "          'Average DNA KL Loss: {:.3f} \\n'\n",
    "          'Average Gene KL Loss: {:.3f} \\n'\n",
    "          'Average RPPA KL Loss: {:.3f} \\n'\n",
    "          'Average Regressor Loss: {:.3f} \\n'\n",
    "          'Average RMSE Loss: {:.3f} \\n'\n",
    "          'Average Shared MSE Loss: {:.3f} \\n'\n",
    "          'Average Shared RMSE Loss: {:.3f} \\n'\n",
    "          'Average R2: {:.3f} \\n'\n",
    "          'Average Indepence Loss: {:.3f} \\n'.format(test_recon_dna_ave, test_recon_gene_ave, test_recon_rppa_ave, test_kl_dna_ave, test_kl_gene_ave, test_kld_rppa_ave, test_reg_ave, test_rmse_ave, test_shared_ave, test_shared_rmse_ave, test_r2_ave, test_ind_ave))\n",
    "        \n",
    "        return test_recon_dna_ave, test_recon_gene_ave, test_recon_rppa_ave, test_kl_dna_ave, test_kl_gene_ave, test_kld_rppa_ave, test_reg_ave, test_rmse_ave, test_shared_ave, test_shared_rmse_ave, test_r2_ave, test_ind_ave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Training and K fold cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 34381.868 \n",
      "Average KL Loss: 126.511 \n",
      "Average Regression Loss: 13.492 \n",
      "Average Shared Loss: 1.977 \n",
      "Average Independence Loss: 87.981 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 33066.289 \n",
      "Average KL Loss: 114.497 \n",
      "Average Regression Loss: 13.425 \n",
      "Average Shared Loss: 2.031 \n",
      "Average Independence Loss: 84.530 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 32566.711 \n",
      "Average KL Loss: 109.515 \n",
      "Average Regression Loss: 13.607 \n",
      "Average Shared Loss: 2.064 \n",
      "Average Independence Loss: 84.487 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 32134.875 \n",
      "Average KL Loss: 106.043 \n",
      "Average Regression Loss: 13.591 \n",
      "Average Shared Loss: 2.050 \n",
      "Average Independence Loss: 81.831 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 31814.472 \n",
      "Average KL Loss: 101.803 \n",
      "Average Regression Loss: 13.634 \n",
      "Average Shared Loss: 2.045 \n",
      "Average Independence Loss: 84.193 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 31496.058 \n",
      "Average KL Loss: 99.148 \n",
      "Average Regression Loss: 13.424 \n",
      "Average Shared Loss: 2.016 \n",
      "Average Independence Loss: 85.008 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 31186.786 \n",
      "Average KL Loss: 97.061 \n",
      "Average Regression Loss: 13.522 \n",
      "Average Shared Loss: 2.018 \n",
      "Average Independence Loss: 85.948 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 30893.974 \n",
      "Average KL Loss: 94.639 \n",
      "Average Regression Loss: 13.588 \n",
      "Average Shared Loss: 1.957 \n",
      "Average Independence Loss: 84.862 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 30531.639 \n",
      "Average KL Loss: 92.458 \n",
      "Average Regression Loss: 13.512 \n",
      "Average Shared Loss: 1.956 \n",
      "Average Independence Loss: 88.060 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 30203.249 \n",
      "Average KL Loss: 92.559 \n",
      "Average Regression Loss: 13.407 \n",
      "Average Shared Loss: 1.949 \n",
      "Average Independence Loss: 86.195 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 29927.950 \n",
      "Average KL Loss: 90.594 \n",
      "Average Regression Loss: 13.451 \n",
      "Average Shared Loss: 1.924 \n",
      "Average Independence Loss: 86.463 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 29711.767 \n",
      "Average KL Loss: 89.016 \n",
      "Average Regression Loss: 13.495 \n",
      "Average Shared Loss: 1.928 \n",
      "Average Independence Loss: 85.367 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 29348.443 \n",
      "Average KL Loss: 87.939 \n",
      "Average Regression Loss: 13.449 \n",
      "Average Shared Loss: 1.899 \n",
      "Average Independence Loss: 84.503 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 29065.517 \n",
      "Average KL Loss: 87.523 \n",
      "Average Regression Loss: 13.508 \n",
      "Average Shared Loss: 1.913 \n",
      "Average Independence Loss: 84.057 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 28786.502 \n",
      "Average KL Loss: 84.381 \n",
      "Average Regression Loss: 13.519 \n",
      "Average Shared Loss: 1.888 \n",
      "Average Independence Loss: 85.026 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 28541.837 \n",
      "Average KL Loss: 84.568 \n",
      "Average Regression Loss: 13.565 \n",
      "Average Shared Loss: 1.874 \n",
      "Average Independence Loss: 82.672 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 28265.655 \n",
      "Average KL Loss: 84.000 \n",
      "Average Regression Loss: 13.507 \n",
      "Average Shared Loss: 1.867 \n",
      "Average Independence Loss: 80.713 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 27990.900 \n",
      "Average KL Loss: 81.355 \n",
      "Average Regression Loss: 13.516 \n",
      "Average Shared Loss: 1.860 \n",
      "Average Independence Loss: 81.994 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 27734.184 \n",
      "Average KL Loss: 80.309 \n",
      "Average Regression Loss: 13.457 \n",
      "Average Shared Loss: 1.837 \n",
      "Average Independence Loss: 82.109 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 27567.012 \n",
      "Average KL Loss: 78.621 \n",
      "Average Regression Loss: 13.469 \n",
      "Average Shared Loss: 1.820 \n",
      "Average Independence Loss: 81.453 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 27251.854 \n",
      "Average KL Loss: 79.357 \n",
      "Average Regression Loss: 13.520 \n",
      "Average Shared Loss: 1.837 \n",
      "Average Independence Loss: 78.679 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 27003.081 \n",
      "Average KL Loss: 78.047 \n",
      "Average Regression Loss: 13.487 \n",
      "Average Shared Loss: 1.825 \n",
      "Average Independence Loss: 80.218 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 26821.111 \n",
      "Average KL Loss: 77.437 \n",
      "Average Regression Loss: 13.422 \n",
      "Average Shared Loss: 1.814 \n",
      "Average Independence Loss: 77.226 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 26505.669 \n",
      "Average KL Loss: 76.436 \n",
      "Average Regression Loss: 13.423 \n",
      "Average Shared Loss: 1.798 \n",
      "Average Independence Loss: 75.994 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 26363.128 \n",
      "Average KL Loss: 75.817 \n",
      "Average Regression Loss: 13.374 \n",
      "Average Shared Loss: 1.814 \n",
      "Average Independence Loss: 76.740 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 26099.132 \n",
      "Average KL Loss: 75.080 \n",
      "Average Regression Loss: 13.378 \n",
      "Average Shared Loss: 1.821 \n",
      "Average Independence Loss: 76.256 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 25812.319 \n",
      "Average KL Loss: 74.895 \n",
      "Average Regression Loss: 13.405 \n",
      "Average Shared Loss: 1.789 \n",
      "Average Independence Loss: 75.332 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 25619.433 \n",
      "Average KL Loss: 74.732 \n",
      "Average Regression Loss: 13.406 \n",
      "Average Shared Loss: 1.796 \n",
      "Average Independence Loss: 74.464 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 25408.255 \n",
      "Average KL Loss: 74.344 \n",
      "Average Regression Loss: 13.337 \n",
      "Average Shared Loss: 1.796 \n",
      "Average Independence Loss: 73.285 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 25146.428 \n",
      "Average KL Loss: 74.236 \n",
      "Average Regression Loss: 13.374 \n",
      "Average Shared Loss: 1.805 \n",
      "Average Independence Loss: 72.787 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 24918.420 \n",
      "Average KL Loss: 73.817 \n",
      "Average Regression Loss: 13.432 \n",
      "Average Shared Loss: 1.781 \n",
      "Average Independence Loss: 71.673 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 24744.606 \n",
      "Average KL Loss: 72.586 \n",
      "Average Regression Loss: 13.401 \n",
      "Average Shared Loss: 1.770 \n",
      "Average Independence Loss: 71.518 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 24514.664 \n",
      "Average KL Loss: 72.506 \n",
      "Average Regression Loss: 13.377 \n",
      "Average Shared Loss: 1.784 \n",
      "Average Independence Loss: 71.694 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 24305.020 \n",
      "Average KL Loss: 71.968 \n",
      "Average Regression Loss: 13.416 \n",
      "Average Shared Loss: 1.771 \n",
      "Average Independence Loss: 71.170 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 24105.694 \n",
      "Average KL Loss: 71.991 \n",
      "Average Regression Loss: 13.402 \n",
      "Average Shared Loss: 1.760 \n",
      "Average Independence Loss: 70.070 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 23917.839 \n",
      "Average KL Loss: 71.441 \n",
      "Average Regression Loss: 13.429 \n",
      "Average Shared Loss: 1.765 \n",
      "Average Independence Loss: 70.023 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 23686.757 \n",
      "Average KL Loss: 71.203 \n",
      "Average Regression Loss: 13.412 \n",
      "Average Shared Loss: 1.771 \n",
      "Average Independence Loss: 67.609 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 23492.744 \n",
      "Average KL Loss: 70.890 \n",
      "Average Regression Loss: 13.452 \n",
      "Average Shared Loss: 1.770 \n",
      "Average Independence Loss: 67.190 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 23288.394 \n",
      "Average KL Loss: 70.562 \n",
      "Average Regression Loss: 13.384 \n",
      "Average Shared Loss: 1.773 \n",
      "Average Independence Loss: 67.456 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 23146.717 \n",
      "Average KL Loss: 69.865 \n",
      "Average Regression Loss: 13.455 \n",
      "Average Shared Loss: 1.770 \n",
      "Average Independence Loss: 67.386 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 22910.534 \n",
      "Average KL Loss: 69.414 \n",
      "Average Regression Loss: 13.444 \n",
      "Average Shared Loss: 1.770 \n",
      "Average Independence Loss: 66.999 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 22677.256 \n",
      "Average KL Loss: 69.488 \n",
      "Average Regression Loss: 13.446 \n",
      "Average Shared Loss: 1.760 \n",
      "Average Independence Loss: 66.156 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 22460.752 \n",
      "Average KL Loss: 69.353 \n",
      "Average Regression Loss: 13.466 \n",
      "Average Shared Loss: 1.761 \n",
      "Average Independence Loss: 64.399 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 22328.786 \n",
      "Average KL Loss: 68.749 \n",
      "Average Regression Loss: 13.489 \n",
      "Average Shared Loss: 1.755 \n",
      "Average Independence Loss: 64.070 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 22165.920 \n",
      "Average KL Loss: 68.636 \n",
      "Average Regression Loss: 13.454 \n",
      "Average Shared Loss: 1.771 \n",
      "Average Independence Loss: 63.409 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 21970.607 \n",
      "Average KL Loss: 68.577 \n",
      "Average Regression Loss: 13.430 \n",
      "Average Shared Loss: 1.763 \n",
      "Average Independence Loss: 62.422 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 21770.799 \n",
      "Average KL Loss: 68.285 \n",
      "Average Regression Loss: 13.439 \n",
      "Average Shared Loss: 1.765 \n",
      "Average Independence Loss: 62.550 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 21562.865 \n",
      "Average KL Loss: 68.100 \n",
      "Average Regression Loss: 13.426 \n",
      "Average Shared Loss: 1.767 \n",
      "Average Independence Loss: 61.955 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 21393.205 \n",
      "Average KL Loss: 67.904 \n",
      "Average Regression Loss: 13.457 \n",
      "Average Shared Loss: 1.769 \n",
      "Average Independence Loss: 61.268 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 21273.558 \n",
      "Average KL Loss: 67.692 \n",
      "Average Regression Loss: 13.442 \n",
      "Average Shared Loss: 1.778 \n",
      "Average Independence Loss: 59.405 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 21131.089 \n",
      "Average KL Loss: 67.562 \n",
      "Average Regression Loss: 12.309 \n",
      "Average Shared Loss: 1.761 \n",
      "Average Independence Loss: 58.694 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 21412.657 \n",
      "Average KL Loss: 67.713 \n",
      "Average Regression Loss: 11.366 \n",
      "Average Shared Loss: 1.754 \n",
      "Average Independence Loss: 56.481 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 21443.517 \n",
      "Average KL Loss: 67.992 \n",
      "Average Regression Loss: 10.825 \n",
      "Average Shared Loss: 1.750 \n",
      "Average Independence Loss: 52.882 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 21496.664 \n",
      "Average KL Loss: 68.279 \n",
      "Average Regression Loss: 10.491 \n",
      "Average Shared Loss: 1.744 \n",
      "Average Independence Loss: 51.819 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 21473.566 \n",
      "Average KL Loss: 68.420 \n",
      "Average Regression Loss: 10.247 \n",
      "Average Shared Loss: 1.739 \n",
      "Average Independence Loss: 49.410 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 21454.186 \n",
      "Average KL Loss: 68.564 \n",
      "Average Regression Loss: 10.018 \n",
      "Average Shared Loss: 1.735 \n",
      "Average Independence Loss: 49.132 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 21427.502 \n",
      "Average KL Loss: 68.618 \n",
      "Average Regression Loss: 9.788 \n",
      "Average Shared Loss: 1.716 \n",
      "Average Independence Loss: 47.124 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 21455.393 \n",
      "Average KL Loss: 68.808 \n",
      "Average Regression Loss: 9.524 \n",
      "Average Shared Loss: 1.699 \n",
      "Average Independence Loss: 47.538 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 21443.677 \n",
      "Average KL Loss: 68.787 \n",
      "Average Regression Loss: 9.413 \n",
      "Average Shared Loss: 1.693 \n",
      "Average Independence Loss: 45.803 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 21450.949 \n",
      "Average KL Loss: 68.983 \n",
      "Average Regression Loss: 9.202 \n",
      "Average Shared Loss: 1.684 \n",
      "Average Independence Loss: 45.807 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 21438.531 \n",
      "Average KL Loss: 69.020 \n",
      "Average Regression Loss: 9.018 \n",
      "Average Shared Loss: 1.666 \n",
      "Average Independence Loss: 45.695 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 21424.708 \n",
      "Average KL Loss: 68.998 \n",
      "Average Regression Loss: 8.840 \n",
      "Average Shared Loss: 1.662 \n",
      "Average Independence Loss: 44.365 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 21450.334 \n",
      "Average KL Loss: 69.142 \n",
      "Average Regression Loss: 8.634 \n",
      "Average Shared Loss: 1.655 \n",
      "Average Independence Loss: 44.445 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 21492.557 \n",
      "Average KL Loss: 69.103 \n",
      "Average Regression Loss: 8.652 \n",
      "Average Shared Loss: 1.652 \n",
      "Average Independence Loss: 42.826 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 21433.774 \n",
      "Average KL Loss: 69.138 \n",
      "Average Regression Loss: 8.551 \n",
      "Average Shared Loss: 1.641 \n",
      "Average Independence Loss: 42.975 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 21436.849 \n",
      "Average KL Loss: 69.003 \n",
      "Average Regression Loss: 8.348 \n",
      "Average Shared Loss: 1.633 \n",
      "Average Independence Loss: 41.058 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 21478.395 \n",
      "Average KL Loss: 69.064 \n",
      "Average Regression Loss: 8.153 \n",
      "Average Shared Loss: 1.640 \n",
      "Average Independence Loss: 41.351 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 21440.091 \n",
      "Average KL Loss: 68.998 \n",
      "Average Regression Loss: 8.067 \n",
      "Average Shared Loss: 1.637 \n",
      "Average Independence Loss: 40.344 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 21462.084 \n",
      "Average KL Loss: 69.010 \n",
      "Average Regression Loss: 7.911 \n",
      "Average Shared Loss: 1.617 \n",
      "Average Independence Loss: 39.841 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 21445.280 \n",
      "Average KL Loss: 69.101 \n",
      "Average Regression Loss: 7.778 \n",
      "Average Shared Loss: 1.621 \n",
      "Average Independence Loss: 40.117 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 21451.519 \n",
      "Average KL Loss: 69.119 \n",
      "Average Regression Loss: 7.738 \n",
      "Average Shared Loss: 1.612 \n",
      "Average Independence Loss: 39.967 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 21483.615 \n",
      "Average KL Loss: 69.060 \n",
      "Average Regression Loss: 7.699 \n",
      "Average Shared Loss: 1.612 \n",
      "Average Independence Loss: 38.907 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 21441.652 \n",
      "Average KL Loss: 69.135 \n",
      "Average Regression Loss: 7.562 \n",
      "Average Shared Loss: 1.612 \n",
      "Average Independence Loss: 38.583 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 21452.619 \n",
      "Average KL Loss: 69.113 \n",
      "Average Regression Loss: 7.442 \n",
      "Average Shared Loss: 1.608 \n",
      "Average Independence Loss: 38.182 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 21481.494 \n",
      "Average KL Loss: 69.041 \n",
      "Average Regression Loss: 7.334 \n",
      "Average Shared Loss: 1.602 \n",
      "Average Independence Loss: 37.074 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 21458.245 \n",
      "Average KL Loss: 69.066 \n",
      "Average Regression Loss: 7.315 \n",
      "Average Shared Loss: 1.594 \n",
      "Average Independence Loss: 36.718 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 21470.947 \n",
      "Average KL Loss: 68.992 \n",
      "Average Regression Loss: 7.134 \n",
      "Average Shared Loss: 1.593 \n",
      "Average Independence Loss: 36.760 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 21451.043 \n",
      "Average KL Loss: 68.870 \n",
      "Average Regression Loss: 7.106 \n",
      "Average Shared Loss: 1.585 \n",
      "Average Independence Loss: 35.657 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 21454.713 \n",
      "Average KL Loss: 69.094 \n",
      "Average Regression Loss: 7.023 \n",
      "Average Shared Loss: 1.591 \n",
      "Average Independence Loss: 36.384 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 21413.857 \n",
      "Average KL Loss: 68.977 \n",
      "Average Regression Loss: 6.969 \n",
      "Average Shared Loss: 1.588 \n",
      "Average Independence Loss: 35.433 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 21442.297 \n",
      "Average KL Loss: 69.016 \n",
      "Average Regression Loss: 6.805 \n",
      "Average Shared Loss: 1.589 \n",
      "Average Independence Loss: 34.322 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 21436.844 \n",
      "Average KL Loss: 69.033 \n",
      "Average Regression Loss: 6.800 \n",
      "Average Shared Loss: 1.600 \n",
      "Average Independence Loss: 34.869 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 21420.866 \n",
      "Average KL Loss: 69.020 \n",
      "Average Regression Loss: 6.678 \n",
      "Average Shared Loss: 1.577 \n",
      "Average Independence Loss: 34.302 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 21445.077 \n",
      "Average KL Loss: 69.093 \n",
      "Average Regression Loss: 6.633 \n",
      "Average Shared Loss: 1.587 \n",
      "Average Independence Loss: 33.577 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 21457.763 \n",
      "Average KL Loss: 68.965 \n",
      "Average Regression Loss: 6.477 \n",
      "Average Shared Loss: 1.580 \n",
      "Average Independence Loss: 32.781 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 21484.850 \n",
      "Average KL Loss: 69.062 \n",
      "Average Regression Loss: 6.494 \n",
      "Average Shared Loss: 1.577 \n",
      "Average Independence Loss: 33.005 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 21397.724 \n",
      "Average KL Loss: 69.200 \n",
      "Average Regression Loss: 6.457 \n",
      "Average Shared Loss: 1.575 \n",
      "Average Independence Loss: 33.256 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 21450.707 \n",
      "Average KL Loss: 69.219 \n",
      "Average Regression Loss: 6.386 \n",
      "Average Shared Loss: 1.566 \n",
      "Average Independence Loss: 32.131 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 21502.673 \n",
      "Average KL Loss: 69.218 \n",
      "Average Regression Loss: 6.496 \n",
      "Average Shared Loss: 1.578 \n",
      "Average Independence Loss: 32.573 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 21437.382 \n",
      "Average KL Loss: 69.174 \n",
      "Average Regression Loss: 6.238 \n",
      "Average Shared Loss: 1.581 \n",
      "Average Independence Loss: 30.714 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 21434.942 \n",
      "Average KL Loss: 69.185 \n",
      "Average Regression Loss: 6.187 \n",
      "Average Shared Loss: 1.576 \n",
      "Average Independence Loss: 31.457 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 21451.056 \n",
      "Average KL Loss: 69.163 \n",
      "Average Regression Loss: 6.041 \n",
      "Average Shared Loss: 1.568 \n",
      "Average Independence Loss: 31.045 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 21470.394 \n",
      "Average KL Loss: 69.200 \n",
      "Average Regression Loss: 5.992 \n",
      "Average Shared Loss: 1.569 \n",
      "Average Independence Loss: 30.600 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 21418.499 \n",
      "Average KL Loss: 69.201 \n",
      "Average Regression Loss: 5.876 \n",
      "Average Shared Loss: 1.564 \n",
      "Average Independence Loss: 31.527 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 21453.939 \n",
      "Average KL Loss: 69.233 \n",
      "Average Regression Loss: 5.870 \n",
      "Average Shared Loss: 1.553 \n",
      "Average Independence Loss: 30.970 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 21470.339 \n",
      "Average KL Loss: 69.237 \n",
      "Average Regression Loss: 5.953 \n",
      "Average Shared Loss: 1.574 \n",
      "Average Independence Loss: 29.328 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 21410.541 \n",
      "Average KL Loss: 69.184 \n",
      "Average Regression Loss: 5.667 \n",
      "Average Shared Loss: 1.560 \n",
      "Average Independence Loss: 29.715 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 21477.140 \n",
      "Average KL Loss: 69.223 \n",
      "Average Regression Loss: 5.789 \n",
      "Average Shared Loss: 1.556 \n",
      "Average Independence Loss: 30.354 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 21450.269 \n",
      "Average KL Loss: 69.208 \n",
      "Average Regression Loss: 5.606 \n",
      "Average Shared Loss: 1.575 \n",
      "Average Independence Loss: 30.584 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 21504.448 \n",
      "Average KL Loss: 69.140 \n",
      "Average Regression Loss: 5.646 \n",
      "Average Shared Loss: 1.564 \n",
      "Average Independence Loss: 29.104 \n",
      "\n",
      "Average DNA Recon Loss: 42920.584 \n",
      "Average Gene Recon Loss: 24930.483 \n",
      "Average RPPA Recon Loss: 148.486 \n",
      "Average DNA KL Loss: 94.926 \n",
      "Average Gene KL Loss: 70.225 \n",
      "Average RPPA KL Loss: 6.044 \n",
      "Average Regressor Loss: 0.032 \n",
      "Average RMSE Loss: 0.178 \n",
      "Average Shared MSE Loss: 0.005 \n",
      "Average Shared RMSE Loss: 0.072 \n",
      "Average R2: 0.006 \n",
      "Average Indepence Loss: 25.438 \n",
      "\n",
      "Fold 2\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 21348.262 \n",
      "Average KL Loss: 68.658 \n",
      "Average Regression Loss: 6.199 \n",
      "Average Shared Loss: 1.618 \n",
      "Average Independence Loss: 33.189 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 21101.138 \n",
      "Average KL Loss: 67.349 \n",
      "Average Regression Loss: 6.509 \n",
      "Average Shared Loss: 1.697 \n",
      "Average Independence Loss: 38.518 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 20863.118 \n",
      "Average KL Loss: 66.401 \n",
      "Average Regression Loss: 6.674 \n",
      "Average Shared Loss: 1.708 \n",
      "Average Independence Loss: 40.817 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 20532.760 \n",
      "Average KL Loss: 65.946 \n",
      "Average Regression Loss: 6.578 \n",
      "Average Shared Loss: 1.682 \n",
      "Average Independence Loss: 42.794 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 20277.416 \n",
      "Average KL Loss: 65.743 \n",
      "Average Regression Loss: 6.706 \n",
      "Average Shared Loss: 1.708 \n",
      "Average Independence Loss: 44.393 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 20007.784 \n",
      "Average KL Loss: 65.609 \n",
      "Average Regression Loss: 6.546 \n",
      "Average Shared Loss: 1.714 \n",
      "Average Independence Loss: 46.086 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 19785.806 \n",
      "Average KL Loss: 65.336 \n",
      "Average Regression Loss: 6.542 \n",
      "Average Shared Loss: 1.709 \n",
      "Average Independence Loss: 44.154 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 19526.024 \n",
      "Average KL Loss: 65.273 \n",
      "Average Regression Loss: 6.558 \n",
      "Average Shared Loss: 1.712 \n",
      "Average Independence Loss: 44.606 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 19294.280 \n",
      "Average KL Loss: 65.015 \n",
      "Average Regression Loss: 6.616 \n",
      "Average Shared Loss: 1.717 \n",
      "Average Independence Loss: 44.647 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 19063.783 \n",
      "Average KL Loss: 65.204 \n",
      "Average Regression Loss: 6.578 \n",
      "Average Shared Loss: 1.718 \n",
      "Average Independence Loss: 45.002 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 18879.013 \n",
      "Average KL Loss: 65.076 \n",
      "Average Regression Loss: 6.640 \n",
      "Average Shared Loss: 1.721 \n",
      "Average Independence Loss: 44.164 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 18665.759 \n",
      "Average KL Loss: 64.976 \n",
      "Average Regression Loss: 6.536 \n",
      "Average Shared Loss: 1.727 \n",
      "Average Independence Loss: 43.092 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 18466.822 \n",
      "Average KL Loss: 65.018 \n",
      "Average Regression Loss: 6.519 \n",
      "Average Shared Loss: 1.727 \n",
      "Average Independence Loss: 42.641 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 18267.752 \n",
      "Average KL Loss: 65.133 \n",
      "Average Regression Loss: 6.545 \n",
      "Average Shared Loss: 1.732 \n",
      "Average Independence Loss: 43.089 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 18110.409 \n",
      "Average KL Loss: 65.294 \n",
      "Average Regression Loss: 6.575 \n",
      "Average Shared Loss: 1.743 \n",
      "Average Independence Loss: 42.575 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 17944.422 \n",
      "Average KL Loss: 65.323 \n",
      "Average Regression Loss: 6.601 \n",
      "Average Shared Loss: 1.751 \n",
      "Average Independence Loss: 42.231 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 17757.372 \n",
      "Average KL Loss: 65.245 \n",
      "Average Regression Loss: 6.601 \n",
      "Average Shared Loss: 1.751 \n",
      "Average Independence Loss: 40.879 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 17589.878 \n",
      "Average KL Loss: 65.445 \n",
      "Average Regression Loss: 6.553 \n",
      "Average Shared Loss: 1.761 \n",
      "Average Independence Loss: 40.992 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 17430.415 \n",
      "Average KL Loss: 65.547 \n",
      "Average Regression Loss: 6.634 \n",
      "Average Shared Loss: 1.765 \n",
      "Average Independence Loss: 40.941 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 17197.149 \n",
      "Average KL Loss: 65.630 \n",
      "Average Regression Loss: 6.599 \n",
      "Average Shared Loss: 1.765 \n",
      "Average Independence Loss: 40.569 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 17094.074 \n",
      "Average KL Loss: 65.588 \n",
      "Average Regression Loss: 6.535 \n",
      "Average Shared Loss: 1.765 \n",
      "Average Independence Loss: 39.130 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 16897.006 \n",
      "Average KL Loss: 65.587 \n",
      "Average Regression Loss: 6.500 \n",
      "Average Shared Loss: 1.782 \n",
      "Average Independence Loss: 38.915 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 16730.082 \n",
      "Average KL Loss: 65.925 \n",
      "Average Regression Loss: 6.649 \n",
      "Average Shared Loss: 1.780 \n",
      "Average Independence Loss: 39.658 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 16564.829 \n",
      "Average KL Loss: 65.867 \n",
      "Average Regression Loss: 6.490 \n",
      "Average Shared Loss: 1.793 \n",
      "Average Independence Loss: 38.280 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 16514.427 \n",
      "Average KL Loss: 66.156 \n",
      "Average Regression Loss: 6.617 \n",
      "Average Shared Loss: 1.797 \n",
      "Average Independence Loss: 38.344 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 16383.406 \n",
      "Average KL Loss: 66.105 \n",
      "Average Regression Loss: 6.667 \n",
      "Average Shared Loss: 1.800 \n",
      "Average Independence Loss: 37.472 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 16157.716 \n",
      "Average KL Loss: 66.326 \n",
      "Average Regression Loss: 6.566 \n",
      "Average Shared Loss: 1.792 \n",
      "Average Independence Loss: 37.901 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 16078.172 \n",
      "Average KL Loss: 66.318 \n",
      "Average Regression Loss: 6.611 \n",
      "Average Shared Loss: 1.808 \n",
      "Average Independence Loss: 37.183 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 15870.627 \n",
      "Average KL Loss: 66.730 \n",
      "Average Regression Loss: 6.574 \n",
      "Average Shared Loss: 1.813 \n",
      "Average Independence Loss: 37.515 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 15727.216 \n",
      "Average KL Loss: 66.651 \n",
      "Average Regression Loss: 6.581 \n",
      "Average Shared Loss: 1.823 \n",
      "Average Independence Loss: 36.533 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 15620.139 \n",
      "Average KL Loss: 66.943 \n",
      "Average Regression Loss: 6.654 \n",
      "Average Shared Loss: 1.821 \n",
      "Average Independence Loss: 37.158 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 15476.641 \n",
      "Average KL Loss: 66.843 \n",
      "Average Regression Loss: 6.498 \n",
      "Average Shared Loss: 1.830 \n",
      "Average Independence Loss: 35.613 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 15354.616 \n",
      "Average KL Loss: 67.143 \n",
      "Average Regression Loss: 6.586 \n",
      "Average Shared Loss: 1.826 \n",
      "Average Independence Loss: 35.708 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 15260.287 \n",
      "Average KL Loss: 67.402 \n",
      "Average Regression Loss: 6.585 \n",
      "Average Shared Loss: 1.823 \n",
      "Average Independence Loss: 35.751 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 15149.713 \n",
      "Average KL Loss: 67.443 \n",
      "Average Regression Loss: 6.672 \n",
      "Average Shared Loss: 1.840 \n",
      "Average Independence Loss: 34.931 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 15027.891 \n",
      "Average KL Loss: 67.418 \n",
      "Average Regression Loss: 6.606 \n",
      "Average Shared Loss: 1.841 \n",
      "Average Independence Loss: 34.360 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 14915.155 \n",
      "Average KL Loss: 67.827 \n",
      "Average Regression Loss: 6.605 \n",
      "Average Shared Loss: 1.848 \n",
      "Average Independence Loss: 35.024 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 14786.316 \n",
      "Average KL Loss: 67.811 \n",
      "Average Regression Loss: 6.706 \n",
      "Average Shared Loss: 1.839 \n",
      "Average Independence Loss: 34.209 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 14699.876 \n",
      "Average KL Loss: 68.081 \n",
      "Average Regression Loss: 6.703 \n",
      "Average Shared Loss: 1.848 \n",
      "Average Independence Loss: 34.395 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 14562.750 \n",
      "Average KL Loss: 68.520 \n",
      "Average Regression Loss: 6.683 \n",
      "Average Shared Loss: 1.858 \n",
      "Average Independence Loss: 35.250 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 14484.382 \n",
      "Average KL Loss: 68.298 \n",
      "Average Regression Loss: 6.692 \n",
      "Average Shared Loss: 1.855 \n",
      "Average Independence Loss: 33.637 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 14309.912 \n",
      "Average KL Loss: 68.370 \n",
      "Average Regression Loss: 6.612 \n",
      "Average Shared Loss: 1.871 \n",
      "Average Independence Loss: 33.089 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 14223.681 \n",
      "Average KL Loss: 68.871 \n",
      "Average Regression Loss: 6.672 \n",
      "Average Shared Loss: 1.856 \n",
      "Average Independence Loss: 33.902 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 14165.586 \n",
      "Average KL Loss: 68.943 \n",
      "Average Regression Loss: 6.599 \n",
      "Average Shared Loss: 1.870 \n",
      "Average Independence Loss: 33.376 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 13999.024 \n",
      "Average KL Loss: 69.115 \n",
      "Average Regression Loss: 6.654 \n",
      "Average Shared Loss: 1.875 \n",
      "Average Independence Loss: 33.337 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 13942.214 \n",
      "Average KL Loss: 69.125 \n",
      "Average Regression Loss: 6.665 \n",
      "Average Shared Loss: 1.858 \n",
      "Average Independence Loss: 32.849 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 13885.530 \n",
      "Average KL Loss: 69.203 \n",
      "Average Regression Loss: 6.610 \n",
      "Average Shared Loss: 1.879 \n",
      "Average Independence Loss: 32.579 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 13760.028 \n",
      "Average KL Loss: 69.211 \n",
      "Average Regression Loss: 6.647 \n",
      "Average Shared Loss: 1.878 \n",
      "Average Independence Loss: 31.950 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 13682.620 \n",
      "Average KL Loss: 69.414 \n",
      "Average Regression Loss: 6.734 \n",
      "Average Shared Loss: 1.876 \n",
      "Average Independence Loss: 31.837 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 13562.822 \n",
      "Average KL Loss: 69.799 \n",
      "Average Regression Loss: 6.625 \n",
      "Average Shared Loss: 1.876 \n",
      "Average Independence Loss: 32.602 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 13619.568 \n",
      "Average KL Loss: 69.836 \n",
      "Average Regression Loss: 6.575 \n",
      "Average Shared Loss: 1.880 \n",
      "Average Independence Loss: 31.448 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 14010.776 \n",
      "Average KL Loss: 69.827 \n",
      "Average Regression Loss: 6.136 \n",
      "Average Shared Loss: 1.877 \n",
      "Average Independence Loss: 31.020 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 14027.227 \n",
      "Average KL Loss: 69.826 \n",
      "Average Regression Loss: 5.867 \n",
      "Average Shared Loss: 1.881 \n",
      "Average Independence Loss: 30.480 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 14077.760 \n",
      "Average KL Loss: 69.724 \n",
      "Average Regression Loss: 5.617 \n",
      "Average Shared Loss: 1.884 \n",
      "Average Independence Loss: 29.611 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 14106.704 \n",
      "Average KL Loss: 69.642 \n",
      "Average Regression Loss: 5.513 \n",
      "Average Shared Loss: 1.884 \n",
      "Average Independence Loss: 27.999 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 14076.104 \n",
      "Average KL Loss: 69.836 \n",
      "Average Regression Loss: 5.334 \n",
      "Average Shared Loss: 1.884 \n",
      "Average Independence Loss: 28.801 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 14100.634 \n",
      "Average KL Loss: 69.838 \n",
      "Average Regression Loss: 5.267 \n",
      "Average Shared Loss: 1.871 \n",
      "Average Independence Loss: 27.530 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 14112.357 \n",
      "Average KL Loss: 69.656 \n",
      "Average Regression Loss: 5.145 \n",
      "Average Shared Loss: 1.882 \n",
      "Average Independence Loss: 26.555 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 14116.343 \n",
      "Average KL Loss: 69.797 \n",
      "Average Regression Loss: 4.969 \n",
      "Average Shared Loss: 1.883 \n",
      "Average Independence Loss: 26.221 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 14088.249 \n",
      "Average KL Loss: 69.757 \n",
      "Average Regression Loss: 4.877 \n",
      "Average Shared Loss: 1.871 \n",
      "Average Independence Loss: 25.622 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 14079.376 \n",
      "Average KL Loss: 69.763 \n",
      "Average Regression Loss: 4.746 \n",
      "Average Shared Loss: 1.870 \n",
      "Average Independence Loss: 24.838 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 14115.275 \n",
      "Average KL Loss: 69.584 \n",
      "Average Regression Loss: 4.728 \n",
      "Average Shared Loss: 1.873 \n",
      "Average Independence Loss: 23.913 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 14069.429 \n",
      "Average KL Loss: 69.954 \n",
      "Average Regression Loss: 4.617 \n",
      "Average Shared Loss: 1.862 \n",
      "Average Independence Loss: 24.407 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 14068.854 \n",
      "Average KL Loss: 69.841 \n",
      "Average Regression Loss: 4.587 \n",
      "Average Shared Loss: 1.877 \n",
      "Average Independence Loss: 23.558 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 14049.093 \n",
      "Average KL Loss: 69.930 \n",
      "Average Regression Loss: 4.492 \n",
      "Average Shared Loss: 1.861 \n",
      "Average Independence Loss: 22.814 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 14139.490 \n",
      "Average KL Loss: 70.046 \n",
      "Average Regression Loss: 4.313 \n",
      "Average Shared Loss: 1.849 \n",
      "Average Independence Loss: 22.842 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 14116.919 \n",
      "Average KL Loss: 69.772 \n",
      "Average Regression Loss: 4.275 \n",
      "Average Shared Loss: 1.859 \n",
      "Average Independence Loss: 21.432 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 14123.013 \n",
      "Average KL Loss: 69.735 \n",
      "Average Regression Loss: 4.233 \n",
      "Average Shared Loss: 1.861 \n",
      "Average Independence Loss: 20.741 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 14108.431 \n",
      "Average KL Loss: 69.966 \n",
      "Average Regression Loss: 4.248 \n",
      "Average Shared Loss: 1.859 \n",
      "Average Independence Loss: 21.188 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 14088.174 \n",
      "Average KL Loss: 69.922 \n",
      "Average Regression Loss: 4.072 \n",
      "Average Shared Loss: 1.857 \n",
      "Average Independence Loss: 20.535 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 14110.249 \n",
      "Average KL Loss: 69.848 \n",
      "Average Regression Loss: 4.033 \n",
      "Average Shared Loss: 1.866 \n",
      "Average Independence Loss: 20.114 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 14037.725 \n",
      "Average KL Loss: 69.907 \n",
      "Average Regression Loss: 3.953 \n",
      "Average Shared Loss: 1.864 \n",
      "Average Independence Loss: 20.285 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 14068.527 \n",
      "Average KL Loss: 70.016 \n",
      "Average Regression Loss: 3.945 \n",
      "Average Shared Loss: 1.869 \n",
      "Average Independence Loss: 20.025 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 14097.193 \n",
      "Average KL Loss: 69.999 \n",
      "Average Regression Loss: 3.917 \n",
      "Average Shared Loss: 1.864 \n",
      "Average Independence Loss: 19.910 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 14087.849 \n",
      "Average KL Loss: 70.000 \n",
      "Average Regression Loss: 3.847 \n",
      "Average Shared Loss: 1.856 \n",
      "Average Independence Loss: 19.508 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 14114.676 \n",
      "Average KL Loss: 69.894 \n",
      "Average Regression Loss: 3.811 \n",
      "Average Shared Loss: 1.863 \n",
      "Average Independence Loss: 18.383 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 14142.722 \n",
      "Average KL Loss: 70.026 \n",
      "Average Regression Loss: 3.871 \n",
      "Average Shared Loss: 1.855 \n",
      "Average Independence Loss: 18.956 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 14080.396 \n",
      "Average KL Loss: 69.919 \n",
      "Average Regression Loss: 3.682 \n",
      "Average Shared Loss: 1.855 \n",
      "Average Independence Loss: 18.296 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 14060.844 \n",
      "Average KL Loss: 69.846 \n",
      "Average Regression Loss: 3.684 \n",
      "Average Shared Loss: 1.851 \n",
      "Average Independence Loss: 17.615 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 14113.605 \n",
      "Average KL Loss: 69.689 \n",
      "Average Regression Loss: 3.568 \n",
      "Average Shared Loss: 1.857 \n",
      "Average Independence Loss: 16.977 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 14079.740 \n",
      "Average KL Loss: 69.873 \n",
      "Average Regression Loss: 3.686 \n",
      "Average Shared Loss: 1.870 \n",
      "Average Independence Loss: 17.632 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 14096.590 \n",
      "Average KL Loss: 69.864 \n",
      "Average Regression Loss: 3.435 \n",
      "Average Shared Loss: 1.852 \n",
      "Average Independence Loss: 17.266 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 14055.807 \n",
      "Average KL Loss: 69.813 \n",
      "Average Regression Loss: 3.349 \n",
      "Average Shared Loss: 1.849 \n",
      "Average Independence Loss: 16.921 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 14082.086 \n",
      "Average KL Loss: 69.807 \n",
      "Average Regression Loss: 3.314 \n",
      "Average Shared Loss: 1.851 \n",
      "Average Independence Loss: 15.955 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 14127.098 \n",
      "Average KL Loss: 69.811 \n",
      "Average Regression Loss: 3.320 \n",
      "Average Shared Loss: 1.847 \n",
      "Average Independence Loss: 16.261 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 14085.638 \n",
      "Average KL Loss: 69.800 \n",
      "Average Regression Loss: 3.184 \n",
      "Average Shared Loss: 1.842 \n",
      "Average Independence Loss: 16.170 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 14076.966 \n",
      "Average KL Loss: 69.823 \n",
      "Average Regression Loss: 3.195 \n",
      "Average Shared Loss: 1.838 \n",
      "Average Independence Loss: 15.634 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 14134.081 \n",
      "Average KL Loss: 69.790 \n",
      "Average Regression Loss: 3.235 \n",
      "Average Shared Loss: 1.848 \n",
      "Average Independence Loss: 15.346 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 14084.871 \n",
      "Average KL Loss: 69.850 \n",
      "Average Regression Loss: 3.167 \n",
      "Average Shared Loss: 1.844 \n",
      "Average Independence Loss: 15.727 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 14083.807 \n",
      "Average KL Loss: 69.761 \n",
      "Average Regression Loss: 3.147 \n",
      "Average Shared Loss: 1.842 \n",
      "Average Independence Loss: 15.369 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 14089.539 \n",
      "Average KL Loss: 69.857 \n",
      "Average Regression Loss: 3.196 \n",
      "Average Shared Loss: 1.841 \n",
      "Average Independence Loss: 15.433 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 14078.536 \n",
      "Average KL Loss: 69.716 \n",
      "Average Regression Loss: 3.177 \n",
      "Average Shared Loss: 1.847 \n",
      "Average Independence Loss: 14.738 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 14075.282 \n",
      "Average KL Loss: 69.884 \n",
      "Average Regression Loss: 3.089 \n",
      "Average Shared Loss: 1.841 \n",
      "Average Independence Loss: 14.853 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 14122.038 \n",
      "Average KL Loss: 69.648 \n",
      "Average Regression Loss: 2.960 \n",
      "Average Shared Loss: 1.843 \n",
      "Average Independence Loss: 14.267 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 14129.923 \n",
      "Average KL Loss: 69.685 \n",
      "Average Regression Loss: 3.106 \n",
      "Average Shared Loss: 1.851 \n",
      "Average Independence Loss: 13.547 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 14103.371 \n",
      "Average KL Loss: 69.791 \n",
      "Average Regression Loss: 2.983 \n",
      "Average Shared Loss: 1.841 \n",
      "Average Independence Loss: 14.220 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 14130.931 \n",
      "Average KL Loss: 69.611 \n",
      "Average Regression Loss: 2.958 \n",
      "Average Shared Loss: 1.843 \n",
      "Average Independence Loss: 13.430 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 14091.644 \n",
      "Average KL Loss: 69.902 \n",
      "Average Regression Loss: 2.890 \n",
      "Average Shared Loss: 1.836 \n",
      "Average Independence Loss: 14.474 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 14089.634 \n",
      "Average KL Loss: 69.925 \n",
      "Average Regression Loss: 2.882 \n",
      "Average Shared Loss: 1.833 \n",
      "Average Independence Loss: 14.569 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 14107.348 \n",
      "Average KL Loss: 69.713 \n",
      "Average Regression Loss: 2.791 \n",
      "Average Shared Loss: 1.852 \n",
      "Average Independence Loss: 13.635 \n",
      "\n",
      "Average DNA Recon Loss: 39665.777 \n",
      "Average Gene Recon Loss: 12320.608 \n",
      "Average RPPA Recon Loss: 147.237 \n",
      "Average DNA KL Loss: 147.428 \n",
      "Average Gene KL Loss: 12.680 \n",
      "Average RPPA KL Loss: 0.330 \n",
      "Average Regressor Loss: 0.022 \n",
      "Average RMSE Loss: 0.148 \n",
      "Average Shared MSE Loss: 0.006 \n",
      "Average Shared RMSE Loss: 0.076 \n",
      "Average R2: 0.380 \n",
      "Average Indepence Loss: 5.813 \n",
      "\n",
      "Fold 3\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 14134.664 \n",
      "Average KL Loss: 69.699 \n",
      "Average Regression Loss: 3.443 \n",
      "Average Shared Loss: 1.849 \n",
      "Average Independence Loss: 14.605 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 13940.562 \n",
      "Average KL Loss: 69.584 \n",
      "Average Regression Loss: 3.647 \n",
      "Average Shared Loss: 1.869 \n",
      "Average Independence Loss: 17.138 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 13667.699 \n",
      "Average KL Loss: 69.473 \n",
      "Average Regression Loss: 3.490 \n",
      "Average Shared Loss: 1.881 \n",
      "Average Independence Loss: 18.469 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 13539.708 \n",
      "Average KL Loss: 69.598 \n",
      "Average Regression Loss: 3.750 \n",
      "Average Shared Loss: 1.877 \n",
      "Average Independence Loss: 19.977 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 13393.439 \n",
      "Average KL Loss: 69.755 \n",
      "Average Regression Loss: 3.621 \n",
      "Average Shared Loss: 1.899 \n",
      "Average Independence Loss: 20.878 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 13279.898 \n",
      "Average KL Loss: 69.948 \n",
      "Average Regression Loss: 3.687 \n",
      "Average Shared Loss: 1.899 \n",
      "Average Independence Loss: 21.655 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 13104.143 \n",
      "Average KL Loss: 70.039 \n",
      "Average Regression Loss: 3.655 \n",
      "Average Shared Loss: 1.912 \n",
      "Average Independence Loss: 21.368 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 12960.399 \n",
      "Average KL Loss: 70.127 \n",
      "Average Regression Loss: 3.717 \n",
      "Average Shared Loss: 1.904 \n",
      "Average Independence Loss: 21.829 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 12852.576 \n",
      "Average KL Loss: 70.088 \n",
      "Average Regression Loss: 3.622 \n",
      "Average Shared Loss: 1.907 \n",
      "Average Independence Loss: 21.090 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 12779.108 \n",
      "Average KL Loss: 70.293 \n",
      "Average Regression Loss: 3.678 \n",
      "Average Shared Loss: 1.899 \n",
      "Average Independence Loss: 21.385 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 12641.923 \n",
      "Average KL Loss: 70.209 \n",
      "Average Regression Loss: 3.735 \n",
      "Average Shared Loss: 1.910 \n",
      "Average Independence Loss: 20.362 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 12562.144 \n",
      "Average KL Loss: 70.610 \n",
      "Average Regression Loss: 3.618 \n",
      "Average Shared Loss: 1.910 \n",
      "Average Independence Loss: 21.569 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 12478.333 \n",
      "Average KL Loss: 70.654 \n",
      "Average Regression Loss: 3.619 \n",
      "Average Shared Loss: 1.915 \n",
      "Average Independence Loss: 21.007 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 12396.852 \n",
      "Average KL Loss: 70.814 \n",
      "Average Regression Loss: 3.596 \n",
      "Average Shared Loss: 1.913 \n",
      "Average Independence Loss: 21.852 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 12287.616 \n",
      "Average KL Loss: 70.815 \n",
      "Average Regression Loss: 3.661 \n",
      "Average Shared Loss: 1.911 \n",
      "Average Independence Loss: 21.029 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 12202.263 \n",
      "Average KL Loss: 70.895 \n",
      "Average Regression Loss: 3.578 \n",
      "Average Shared Loss: 1.916 \n",
      "Average Independence Loss: 21.126 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 12051.037 \n",
      "Average KL Loss: 70.834 \n",
      "Average Regression Loss: 3.617 \n",
      "Average Shared Loss: 1.917 \n",
      "Average Independence Loss: 20.182 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 12110.020 \n",
      "Average KL Loss: 71.124 \n",
      "Average Regression Loss: 3.701 \n",
      "Average Shared Loss: 1.912 \n",
      "Average Independence Loss: 21.205 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 11955.659 \n",
      "Average KL Loss: 71.018 \n",
      "Average Regression Loss: 3.589 \n",
      "Average Shared Loss: 1.908 \n",
      "Average Independence Loss: 19.795 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 11913.535 \n",
      "Average KL Loss: 71.356 \n",
      "Average Regression Loss: 3.583 \n",
      "Average Shared Loss: 1.903 \n",
      "Average Independence Loss: 20.796 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 11778.115 \n",
      "Average KL Loss: 71.390 \n",
      "Average Regression Loss: 3.575 \n",
      "Average Shared Loss: 1.897 \n",
      "Average Independence Loss: 20.861 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 11784.666 \n",
      "Average KL Loss: 71.568 \n",
      "Average Regression Loss: 3.883 \n",
      "Average Shared Loss: 1.904 \n",
      "Average Independence Loss: 20.925 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 11653.457 \n",
      "Average KL Loss: 71.275 \n",
      "Average Regression Loss: 3.695 \n",
      "Average Shared Loss: 1.913 \n",
      "Average Independence Loss: 19.271 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 11609.975 \n",
      "Average KL Loss: 71.390 \n",
      "Average Regression Loss: 3.652 \n",
      "Average Shared Loss: 1.912 \n",
      "Average Independence Loss: 19.354 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 11522.034 \n",
      "Average KL Loss: 71.609 \n",
      "Average Regression Loss: 3.600 \n",
      "Average Shared Loss: 1.903 \n",
      "Average Independence Loss: 20.039 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 11458.888 \n",
      "Average KL Loss: 71.490 \n",
      "Average Regression Loss: 3.610 \n",
      "Average Shared Loss: 1.905 \n",
      "Average Independence Loss: 19.043 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 11377.647 \n",
      "Average KL Loss: 71.435 \n",
      "Average Regression Loss: 3.729 \n",
      "Average Shared Loss: 1.907 \n",
      "Average Independence Loss: 18.693 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 11308.626 \n",
      "Average KL Loss: 71.704 \n",
      "Average Regression Loss: 3.634 \n",
      "Average Shared Loss: 1.902 \n",
      "Average Independence Loss: 19.160 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 11325.437 \n",
      "Average KL Loss: 71.667 \n",
      "Average Regression Loss: 3.778 \n",
      "Average Shared Loss: 1.897 \n",
      "Average Independence Loss: 18.608 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 11217.235 \n",
      "Average KL Loss: 71.817 \n",
      "Average Regression Loss: 3.796 \n",
      "Average Shared Loss: 1.898 \n",
      "Average Independence Loss: 19.041 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 11132.065 \n",
      "Average KL Loss: 71.820 \n",
      "Average Regression Loss: 3.603 \n",
      "Average Shared Loss: 1.897 \n",
      "Average Independence Loss: 18.700 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 11080.929 \n",
      "Average KL Loss: 71.947 \n",
      "Average Regression Loss: 3.620 \n",
      "Average Shared Loss: 1.893 \n",
      "Average Independence Loss: 18.790 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 11067.792 \n",
      "Average KL Loss: 71.782 \n",
      "Average Regression Loss: 3.695 \n",
      "Average Shared Loss: 1.895 \n",
      "Average Independence Loss: 18.421 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 10961.732 \n",
      "Average KL Loss: 71.887 \n",
      "Average Regression Loss: 3.623 \n",
      "Average Shared Loss: 1.887 \n",
      "Average Independence Loss: 18.325 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 10907.810 \n",
      "Average KL Loss: 71.851 \n",
      "Average Regression Loss: 3.673 \n",
      "Average Shared Loss: 1.888 \n",
      "Average Independence Loss: 17.714 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 10866.197 \n",
      "Average KL Loss: 71.934 \n",
      "Average Regression Loss: 3.739 \n",
      "Average Shared Loss: 1.884 \n",
      "Average Independence Loss: 17.753 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 10840.123 \n",
      "Average KL Loss: 71.881 \n",
      "Average Regression Loss: 3.797 \n",
      "Average Shared Loss: 1.886 \n",
      "Average Independence Loss: 17.520 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 10786.021 \n",
      "Average KL Loss: 71.838 \n",
      "Average Regression Loss: 3.672 \n",
      "Average Shared Loss: 1.882 \n",
      "Average Independence Loss: 16.895 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 10720.352 \n",
      "Average KL Loss: 71.944 \n",
      "Average Regression Loss: 3.706 \n",
      "Average Shared Loss: 1.880 \n",
      "Average Independence Loss: 17.076 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 10681.833 \n",
      "Average KL Loss: 72.068 \n",
      "Average Regression Loss: 3.706 \n",
      "Average Shared Loss: 1.880 \n",
      "Average Independence Loss: 17.210 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 10604.116 \n",
      "Average KL Loss: 72.011 \n",
      "Average Regression Loss: 3.585 \n",
      "Average Shared Loss: 1.874 \n",
      "Average Independence Loss: 16.721 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 10572.123 \n",
      "Average KL Loss: 72.050 \n",
      "Average Regression Loss: 3.667 \n",
      "Average Shared Loss: 1.871 \n",
      "Average Independence Loss: 16.783 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 10520.733 \n",
      "Average KL Loss: 72.114 \n",
      "Average Regression Loss: 3.651 \n",
      "Average Shared Loss: 1.869 \n",
      "Average Independence Loss: 16.830 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 10459.436 \n",
      "Average KL Loss: 71.998 \n",
      "Average Regression Loss: 3.684 \n",
      "Average Shared Loss: 1.869 \n",
      "Average Independence Loss: 16.356 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 10447.027 \n",
      "Average KL Loss: 72.038 \n",
      "Average Regression Loss: 3.773 \n",
      "Average Shared Loss: 1.869 \n",
      "Average Independence Loss: 16.129 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 10434.603 \n",
      "Average KL Loss: 72.049 \n",
      "Average Regression Loss: 3.759 \n",
      "Average Shared Loss: 1.865 \n",
      "Average Independence Loss: 15.956 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 10370.595 \n",
      "Average KL Loss: 72.170 \n",
      "Average Regression Loss: 3.705 \n",
      "Average Shared Loss: 1.862 \n",
      "Average Independence Loss: 16.075 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 10316.916 \n",
      "Average KL Loss: 72.074 \n",
      "Average Regression Loss: 3.827 \n",
      "Average Shared Loss: 1.855 \n",
      "Average Independence Loss: 15.743 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 10311.487 \n",
      "Average KL Loss: 72.175 \n",
      "Average Regression Loss: 3.827 \n",
      "Average Shared Loss: 1.856 \n",
      "Average Independence Loss: 15.685 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 10261.491 \n",
      "Average KL Loss: 72.159 \n",
      "Average Regression Loss: 3.812 \n",
      "Average Shared Loss: 1.853 \n",
      "Average Independence Loss: 15.660 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 10277.488 \n",
      "Average KL Loss: 72.046 \n",
      "Average Regression Loss: 3.746 \n",
      "Average Shared Loss: 1.856 \n",
      "Average Independence Loss: 15.070 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 10462.751 \n",
      "Average KL Loss: 72.017 \n",
      "Average Regression Loss: 3.567 \n",
      "Average Shared Loss: 1.852 \n",
      "Average Independence Loss: 14.216 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 10483.281 \n",
      "Average KL Loss: 72.074 \n",
      "Average Regression Loss: 3.164 \n",
      "Average Shared Loss: 1.851 \n",
      "Average Independence Loss: 13.285 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 10452.980 \n",
      "Average KL Loss: 72.069 \n",
      "Average Regression Loss: 3.006 \n",
      "Average Shared Loss: 1.848 \n",
      "Average Independence Loss: 12.451 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 10449.582 \n",
      "Average KL Loss: 71.936 \n",
      "Average Regression Loss: 2.924 \n",
      "Average Shared Loss: 1.849 \n",
      "Average Independence Loss: 11.592 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 10465.716 \n",
      "Average KL Loss: 72.172 \n",
      "Average Regression Loss: 2.873 \n",
      "Average Shared Loss: 1.844 \n",
      "Average Independence Loss: 11.478 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 10468.271 \n",
      "Average KL Loss: 72.098 \n",
      "Average Regression Loss: 2.812 \n",
      "Average Shared Loss: 1.847 \n",
      "Average Independence Loss: 10.681 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 10418.395 \n",
      "Average KL Loss: 71.956 \n",
      "Average Regression Loss: 2.731 \n",
      "Average Shared Loss: 1.852 \n",
      "Average Independence Loss: 9.797 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 10471.184 \n",
      "Average KL Loss: 72.035 \n",
      "Average Regression Loss: 2.741 \n",
      "Average Shared Loss: 1.838 \n",
      "Average Independence Loss: 9.668 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 10476.016 \n",
      "Average KL Loss: 72.009 \n",
      "Average Regression Loss: 2.613 \n",
      "Average Shared Loss: 1.845 \n",
      "Average Independence Loss: 8.947 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 10454.383 \n",
      "Average KL Loss: 72.112 \n",
      "Average Regression Loss: 2.433 \n",
      "Average Shared Loss: 1.842 \n",
      "Average Independence Loss: 8.642 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 10434.255 \n",
      "Average KL Loss: 72.044 \n",
      "Average Regression Loss: 2.479 \n",
      "Average Shared Loss: 1.852 \n",
      "Average Independence Loss: 8.245 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 10511.656 \n",
      "Average KL Loss: 71.990 \n",
      "Average Regression Loss: 2.428 \n",
      "Average Shared Loss: 1.844 \n",
      "Average Independence Loss: 7.773 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 10439.350 \n",
      "Average KL Loss: 72.003 \n",
      "Average Regression Loss: 2.364 \n",
      "Average Shared Loss: 1.844 \n",
      "Average Independence Loss: 7.402 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 10496.303 \n",
      "Average KL Loss: 72.003 \n",
      "Average Regression Loss: 2.369 \n",
      "Average Shared Loss: 1.842 \n",
      "Average Independence Loss: 7.336 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 10442.191 \n",
      "Average KL Loss: 71.967 \n",
      "Average Regression Loss: 2.307 \n",
      "Average Shared Loss: 1.841 \n",
      "Average Independence Loss: 6.785 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 10486.220 \n",
      "Average KL Loss: 72.001 \n",
      "Average Regression Loss: 2.298 \n",
      "Average Shared Loss: 1.845 \n",
      "Average Independence Loss: 6.972 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 10438.889 \n",
      "Average KL Loss: 71.977 \n",
      "Average Regression Loss: 2.269 \n",
      "Average Shared Loss: 1.839 \n",
      "Average Independence Loss: 6.450 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 10495.424 \n",
      "Average KL Loss: 71.900 \n",
      "Average Regression Loss: 2.269 \n",
      "Average Shared Loss: 1.839 \n",
      "Average Independence Loss: 5.911 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 10441.707 \n",
      "Average KL Loss: 71.804 \n",
      "Average Regression Loss: 2.256 \n",
      "Average Shared Loss: 1.836 \n",
      "Average Independence Loss: 5.689 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 10455.589 \n",
      "Average KL Loss: 71.858 \n",
      "Average Regression Loss: 2.174 \n",
      "Average Shared Loss: 1.839 \n",
      "Average Independence Loss: 5.750 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 10447.606 \n",
      "Average KL Loss: 71.963 \n",
      "Average Regression Loss: 2.079 \n",
      "Average Shared Loss: 1.840 \n",
      "Average Independence Loss: 6.008 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 10491.359 \n",
      "Average KL Loss: 71.928 \n",
      "Average Regression Loss: 2.162 \n",
      "Average Shared Loss: 1.842 \n",
      "Average Independence Loss: 5.854 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 10455.346 \n",
      "Average KL Loss: 71.902 \n",
      "Average Regression Loss: 2.047 \n",
      "Average Shared Loss: 1.833 \n",
      "Average Independence Loss: 5.773 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 10465.054 \n",
      "Average KL Loss: 71.905 \n",
      "Average Regression Loss: 2.140 \n",
      "Average Shared Loss: 1.830 \n",
      "Average Independence Loss: 5.854 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 10478.465 \n",
      "Average KL Loss: 71.813 \n",
      "Average Regression Loss: 2.064 \n",
      "Average Shared Loss: 1.833 \n",
      "Average Independence Loss: 5.472 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 10435.933 \n",
      "Average KL Loss: 71.904 \n",
      "Average Regression Loss: 1.981 \n",
      "Average Shared Loss: 1.833 \n",
      "Average Independence Loss: 5.744 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 10512.802 \n",
      "Average KL Loss: 71.960 \n",
      "Average Regression Loss: 2.028 \n",
      "Average Shared Loss: 1.834 \n",
      "Average Independence Loss: 5.844 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 10449.631 \n",
      "Average KL Loss: 71.819 \n",
      "Average Regression Loss: 1.905 \n",
      "Average Shared Loss: 1.837 \n",
      "Average Independence Loss: 5.552 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 10454.734 \n",
      "Average KL Loss: 71.931 \n",
      "Average Regression Loss: 1.910 \n",
      "Average Shared Loss: 1.831 \n",
      "Average Independence Loss: 5.808 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 10444.061 \n",
      "Average KL Loss: 71.813 \n",
      "Average Regression Loss: 1.842 \n",
      "Average Shared Loss: 1.828 \n",
      "Average Independence Loss: 5.312 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 10460.609 \n",
      "Average KL Loss: 71.861 \n",
      "Average Regression Loss: 1.849 \n",
      "Average Shared Loss: 1.827 \n",
      "Average Independence Loss: 5.529 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 10480.872 \n",
      "Average KL Loss: 71.943 \n",
      "Average Regression Loss: 1.870 \n",
      "Average Shared Loss: 1.824 \n",
      "Average Independence Loss: 5.700 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 10414.230 \n",
      "Average KL Loss: 71.882 \n",
      "Average Regression Loss: 1.759 \n",
      "Average Shared Loss: 1.822 \n",
      "Average Independence Loss: 5.540 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 10453.621 \n",
      "Average KL Loss: 71.947 \n",
      "Average Regression Loss: 1.792 \n",
      "Average Shared Loss: 1.827 \n",
      "Average Independence Loss: 5.694 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 10477.364 \n",
      "Average KL Loss: 71.880 \n",
      "Average Regression Loss: 1.763 \n",
      "Average Shared Loss: 1.823 \n",
      "Average Independence Loss: 5.815 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 10478.150 \n",
      "Average KL Loss: 71.965 \n",
      "Average Regression Loss: 1.774 \n",
      "Average Shared Loss: 1.827 \n",
      "Average Independence Loss: 5.802 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 10449.752 \n",
      "Average KL Loss: 71.903 \n",
      "Average Regression Loss: 1.769 \n",
      "Average Shared Loss: 1.824 \n",
      "Average Independence Loss: 5.602 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 10393.577 \n",
      "Average KL Loss: 71.891 \n",
      "Average Regression Loss: 1.691 \n",
      "Average Shared Loss: 1.819 \n",
      "Average Independence Loss: 5.629 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 10490.829 \n",
      "Average KL Loss: 71.864 \n",
      "Average Regression Loss: 1.663 \n",
      "Average Shared Loss: 1.819 \n",
      "Average Independence Loss: 5.585 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 10447.188 \n",
      "Average KL Loss: 71.872 \n",
      "Average Regression Loss: 1.700 \n",
      "Average Shared Loss: 1.820 \n",
      "Average Independence Loss: 5.286 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 10472.076 \n",
      "Average KL Loss: 71.738 \n",
      "Average Regression Loss: 1.624 \n",
      "Average Shared Loss: 1.815 \n",
      "Average Independence Loss: 5.154 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 10438.737 \n",
      "Average KL Loss: 71.842 \n",
      "Average Regression Loss: 1.616 \n",
      "Average Shared Loss: 1.815 \n",
      "Average Independence Loss: 5.446 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 10440.366 \n",
      "Average KL Loss: 71.768 \n",
      "Average Regression Loss: 1.600 \n",
      "Average Shared Loss: 1.813 \n",
      "Average Independence Loss: 5.067 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 10463.973 \n",
      "Average KL Loss: 71.785 \n",
      "Average Regression Loss: 1.613 \n",
      "Average Shared Loss: 1.812 \n",
      "Average Independence Loss: 5.067 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 10471.894 \n",
      "Average KL Loss: 71.763 \n",
      "Average Regression Loss: 1.604 \n",
      "Average Shared Loss: 1.812 \n",
      "Average Independence Loss: 5.191 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 10476.208 \n",
      "Average KL Loss: 71.788 \n",
      "Average Regression Loss: 1.615 \n",
      "Average Shared Loss: 1.808 \n",
      "Average Independence Loss: 5.234 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 10420.878 \n",
      "Average KL Loss: 71.687 \n",
      "Average Regression Loss: 1.549 \n",
      "Average Shared Loss: 1.814 \n",
      "Average Independence Loss: 4.820 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 10443.543 \n",
      "Average KL Loss: 71.790 \n",
      "Average Regression Loss: 1.616 \n",
      "Average Shared Loss: 1.814 \n",
      "Average Independence Loss: 5.118 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 10451.311 \n",
      "Average KL Loss: 71.749 \n",
      "Average Regression Loss: 1.534 \n",
      "Average Shared Loss: 1.812 \n",
      "Average Independence Loss: 4.975 \n",
      "\n",
      "Average DNA Recon Loss: 34998.863 \n",
      "Average Gene Recon Loss: 6584.505 \n",
      "Average RPPA Recon Loss: 143.929 \n",
      "Average DNA KL Loss: 185.288 \n",
      "Average Gene KL Loss: 7.693 \n",
      "Average RPPA KL Loss: 0.368 \n",
      "Average Regressor Loss: 0.012 \n",
      "Average RMSE Loss: 0.109 \n",
      "Average Shared MSE Loss: 0.005 \n",
      "Average Shared RMSE Loss: 0.074 \n",
      "Average R2: 0.607 \n",
      "Average Indepence Loss: 2.687 \n",
      "\n",
      "Fold 4\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 10809.975 \n",
      "Average KL Loss: 71.882 \n",
      "Average Regression Loss: 1.930 \n",
      "Average Shared Loss: 1.816 \n",
      "Average Independence Loss: 6.043 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 10684.870 \n",
      "Average KL Loss: 71.863 \n",
      "Average Regression Loss: 2.041 \n",
      "Average Shared Loss: 1.846 \n",
      "Average Independence Loss: 7.071 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 10615.047 \n",
      "Average KL Loss: 71.998 \n",
      "Average Regression Loss: 2.046 \n",
      "Average Shared Loss: 1.849 \n",
      "Average Independence Loss: 8.715 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 10512.680 \n",
      "Average KL Loss: 72.116 \n",
      "Average Regression Loss: 2.052 \n",
      "Average Shared Loss: 1.857 \n",
      "Average Independence Loss: 9.525 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 10446.859 \n",
      "Average KL Loss: 72.017 \n",
      "Average Regression Loss: 2.155 \n",
      "Average Shared Loss: 1.857 \n",
      "Average Independence Loss: 9.365 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 10337.949 \n",
      "Average KL Loss: 72.219 \n",
      "Average Regression Loss: 2.058 \n",
      "Average Shared Loss: 1.848 \n",
      "Average Independence Loss: 9.718 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 10279.379 \n",
      "Average KL Loss: 72.207 \n",
      "Average Regression Loss: 2.076 \n",
      "Average Shared Loss: 1.847 \n",
      "Average Independence Loss: 9.616 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 10178.641 \n",
      "Average KL Loss: 72.121 \n",
      "Average Regression Loss: 2.092 \n",
      "Average Shared Loss: 1.850 \n",
      "Average Independence Loss: 9.361 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 10144.760 \n",
      "Average KL Loss: 72.197 \n",
      "Average Regression Loss: 2.122 \n",
      "Average Shared Loss: 1.843 \n",
      "Average Independence Loss: 9.558 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 10090.187 \n",
      "Average KL Loss: 72.249 \n",
      "Average Regression Loss: 2.214 \n",
      "Average Shared Loss: 1.841 \n",
      "Average Independence Loss: 9.587 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 10017.949 \n",
      "Average KL Loss: 72.160 \n",
      "Average Regression Loss: 2.023 \n",
      "Average Shared Loss: 1.842 \n",
      "Average Independence Loss: 9.293 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 9947.805 \n",
      "Average KL Loss: 72.227 \n",
      "Average Regression Loss: 1.981 \n",
      "Average Shared Loss: 1.837 \n",
      "Average Independence Loss: 9.188 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 9881.996 \n",
      "Average KL Loss: 72.276 \n",
      "Average Regression Loss: 2.069 \n",
      "Average Shared Loss: 1.829 \n",
      "Average Independence Loss: 9.288 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 9857.544 \n",
      "Average KL Loss: 72.227 \n",
      "Average Regression Loss: 2.079 \n",
      "Average Shared Loss: 1.828 \n",
      "Average Independence Loss: 9.212 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 9850.220 \n",
      "Average KL Loss: 72.285 \n",
      "Average Regression Loss: 2.128 \n",
      "Average Shared Loss: 1.827 \n",
      "Average Independence Loss: 9.175 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 9774.194 \n",
      "Average KL Loss: 72.274 \n",
      "Average Regression Loss: 2.177 \n",
      "Average Shared Loss: 1.824 \n",
      "Average Independence Loss: 9.021 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 9771.050 \n",
      "Average KL Loss: 72.281 \n",
      "Average Regression Loss: 2.099 \n",
      "Average Shared Loss: 1.822 \n",
      "Average Independence Loss: 8.905 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 9721.844 \n",
      "Average KL Loss: 72.234 \n",
      "Average Regression Loss: 2.103 \n",
      "Average Shared Loss: 1.814 \n",
      "Average Independence Loss: 8.712 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 9684.074 \n",
      "Average KL Loss: 72.220 \n",
      "Average Regression Loss: 2.107 \n",
      "Average Shared Loss: 1.812 \n",
      "Average Independence Loss: 8.664 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 9617.471 \n",
      "Average KL Loss: 72.225 \n",
      "Average Regression Loss: 2.099 \n",
      "Average Shared Loss: 1.808 \n",
      "Average Independence Loss: 8.530 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 9620.605 \n",
      "Average KL Loss: 72.243 \n",
      "Average Regression Loss: 2.193 \n",
      "Average Shared Loss: 1.801 \n",
      "Average Independence Loss: 8.604 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 9558.052 \n",
      "Average KL Loss: 72.254 \n",
      "Average Regression Loss: 2.171 \n",
      "Average Shared Loss: 1.801 \n",
      "Average Independence Loss: 8.526 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 9538.004 \n",
      "Average KL Loss: 72.205 \n",
      "Average Regression Loss: 2.162 \n",
      "Average Shared Loss: 1.797 \n",
      "Average Independence Loss: 8.416 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 9528.828 \n",
      "Average KL Loss: 72.265 \n",
      "Average Regression Loss: 2.245 \n",
      "Average Shared Loss: 1.796 \n",
      "Average Independence Loss: 8.479 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 9483.605 \n",
      "Average KL Loss: 72.148 \n",
      "Average Regression Loss: 2.200 \n",
      "Average Shared Loss: 1.794 \n",
      "Average Independence Loss: 8.218 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 9418.606 \n",
      "Average KL Loss: 72.087 \n",
      "Average Regression Loss: 2.106 \n",
      "Average Shared Loss: 1.786 \n",
      "Average Independence Loss: 7.931 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 9427.210 \n",
      "Average KL Loss: 72.163 \n",
      "Average Regression Loss: 2.215 \n",
      "Average Shared Loss: 1.784 \n",
      "Average Independence Loss: 8.149 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 9399.264 \n",
      "Average KL Loss: 72.057 \n",
      "Average Regression Loss: 2.204 \n",
      "Average Shared Loss: 1.782 \n",
      "Average Independence Loss: 7.888 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 9353.017 \n",
      "Average KL Loss: 72.034 \n",
      "Average Regression Loss: 2.234 \n",
      "Average Shared Loss: 1.780 \n",
      "Average Independence Loss: 7.735 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 9340.595 \n",
      "Average KL Loss: 71.980 \n",
      "Average Regression Loss: 2.160 \n",
      "Average Shared Loss: 1.770 \n",
      "Average Independence Loss: 7.575 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 9319.926 \n",
      "Average KL Loss: 71.998 \n",
      "Average Regression Loss: 2.202 \n",
      "Average Shared Loss: 1.766 \n",
      "Average Independence Loss: 7.565 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 9315.962 \n",
      "Average KL Loss: 72.018 \n",
      "Average Regression Loss: 2.243 \n",
      "Average Shared Loss: 1.765 \n",
      "Average Independence Loss: 7.616 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 9259.894 \n",
      "Average KL Loss: 72.087 \n",
      "Average Regression Loss: 2.194 \n",
      "Average Shared Loss: 1.763 \n",
      "Average Independence Loss: 7.661 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 9258.548 \n",
      "Average KL Loss: 72.002 \n",
      "Average Regression Loss: 2.190 \n",
      "Average Shared Loss: 1.761 \n",
      "Average Independence Loss: 7.463 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 9210.000 \n",
      "Average KL Loss: 71.821 \n",
      "Average Regression Loss: 2.144 \n",
      "Average Shared Loss: 1.760 \n",
      "Average Independence Loss: 7.284 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 9184.277 \n",
      "Average KL Loss: 71.787 \n",
      "Average Regression Loss: 2.144 \n",
      "Average Shared Loss: 1.756 \n",
      "Average Independence Loss: 7.191 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 9162.327 \n",
      "Average KL Loss: 71.852 \n",
      "Average Regression Loss: 2.186 \n",
      "Average Shared Loss: 1.753 \n",
      "Average Independence Loss: 7.322 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 9113.819 \n",
      "Average KL Loss: 71.767 \n",
      "Average Regression Loss: 2.145 \n",
      "Average Shared Loss: 1.750 \n",
      "Average Independence Loss: 7.122 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 9135.339 \n",
      "Average KL Loss: 71.738 \n",
      "Average Regression Loss: 2.207 \n",
      "Average Shared Loss: 1.745 \n",
      "Average Independence Loss: 7.141 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 9177.220 \n",
      "Average KL Loss: 71.670 \n",
      "Average Regression Loss: 2.203 \n",
      "Average Shared Loss: 1.743 \n",
      "Average Independence Loss: 7.124 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 9061.932 \n",
      "Average KL Loss: 71.728 \n",
      "Average Regression Loss: 2.139 \n",
      "Average Shared Loss: 1.737 \n",
      "Average Independence Loss: 7.120 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 9034.444 \n",
      "Average KL Loss: 71.694 \n",
      "Average Regression Loss: 2.171 \n",
      "Average Shared Loss: 1.736 \n",
      "Average Independence Loss: 7.042 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 9017.145 \n",
      "Average KL Loss: 71.725 \n",
      "Average Regression Loss: 2.208 \n",
      "Average Shared Loss: 1.733 \n",
      "Average Independence Loss: 7.070 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 8988.804 \n",
      "Average KL Loss: 71.614 \n",
      "Average Regression Loss: 2.189 \n",
      "Average Shared Loss: 1.729 \n",
      "Average Independence Loss: 6.948 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 8958.409 \n",
      "Average KL Loss: 71.563 \n",
      "Average Regression Loss: 2.177 \n",
      "Average Shared Loss: 1.728 \n",
      "Average Independence Loss: 6.862 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 8981.128 \n",
      "Average KL Loss: 71.525 \n",
      "Average Regression Loss: 2.184 \n",
      "Average Shared Loss: 1.722 \n",
      "Average Independence Loss: 6.812 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 8965.342 \n",
      "Average KL Loss: 71.432 \n",
      "Average Regression Loss: 2.153 \n",
      "Average Shared Loss: 1.715 \n",
      "Average Independence Loss: 6.722 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 8946.626 \n",
      "Average KL Loss: 71.299 \n",
      "Average Regression Loss: 2.306 \n",
      "Average Shared Loss: 1.717 \n",
      "Average Independence Loss: 6.536 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 8884.337 \n",
      "Average KL Loss: 71.375 \n",
      "Average Regression Loss: 2.144 \n",
      "Average Shared Loss: 1.715 \n",
      "Average Independence Loss: 6.598 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 8893.677 \n",
      "Average KL Loss: 71.284 \n",
      "Average Regression Loss: 2.147 \n",
      "Average Shared Loss: 1.707 \n",
      "Average Independence Loss: 6.493 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 8936.490 \n",
      "Average KL Loss: 71.175 \n",
      "Average Regression Loss: 2.155 \n",
      "Average Shared Loss: 1.711 \n",
      "Average Independence Loss: 6.223 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 9034.378 \n",
      "Average KL Loss: 71.070 \n",
      "Average Regression Loss: 1.995 \n",
      "Average Shared Loss: 1.710 \n",
      "Average Independence Loss: 5.509 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 9026.130 \n",
      "Average KL Loss: 71.163 \n",
      "Average Regression Loss: 1.853 \n",
      "Average Shared Loss: 1.708 \n",
      "Average Independence Loss: 4.820 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 9063.538 \n",
      "Average KL Loss: 71.154 \n",
      "Average Regression Loss: 1.721 \n",
      "Average Shared Loss: 1.709 \n",
      "Average Independence Loss: 4.101 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 9036.533 \n",
      "Average KL Loss: 71.136 \n",
      "Average Regression Loss: 1.685 \n",
      "Average Shared Loss: 1.705 \n",
      "Average Independence Loss: 3.465 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 9061.078 \n",
      "Average KL Loss: 71.154 \n",
      "Average Regression Loss: 1.694 \n",
      "Average Shared Loss: 1.704 \n",
      "Average Independence Loss: 2.851 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 9053.325 \n",
      "Average KL Loss: 71.223 \n",
      "Average Regression Loss: 1.612 \n",
      "Average Shared Loss: 1.703 \n",
      "Average Independence Loss: 2.341 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 9024.742 \n",
      "Average KL Loss: 71.128 \n",
      "Average Regression Loss: 1.538 \n",
      "Average Shared Loss: 1.702 \n",
      "Average Independence Loss: 1.989 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 9075.068 \n",
      "Average KL Loss: 71.126 \n",
      "Average Regression Loss: 1.550 \n",
      "Average Shared Loss: 1.704 \n",
      "Average Independence Loss: 1.836 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 9010.243 \n",
      "Average KL Loss: 71.163 \n",
      "Average Regression Loss: 1.463 \n",
      "Average Shared Loss: 1.701 \n",
      "Average Independence Loss: 1.765 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 9029.888 \n",
      "Average KL Loss: 71.099 \n",
      "Average Regression Loss: 1.415 \n",
      "Average Shared Loss: 1.704 \n",
      "Average Independence Loss: 1.662 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 9085.124 \n",
      "Average KL Loss: 71.140 \n",
      "Average Regression Loss: 1.373 \n",
      "Average Shared Loss: 1.700 \n",
      "Average Independence Loss: 1.715 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 9083.027 \n",
      "Average KL Loss: 71.094 \n",
      "Average Regression Loss: 1.418 \n",
      "Average Shared Loss: 1.701 \n",
      "Average Independence Loss: 1.557 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 9044.580 \n",
      "Average KL Loss: 71.121 \n",
      "Average Regression Loss: 1.342 \n",
      "Average Shared Loss: 1.700 \n",
      "Average Independence Loss: 1.514 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 9052.460 \n",
      "Average KL Loss: 71.132 \n",
      "Average Regression Loss: 1.364 \n",
      "Average Shared Loss: 1.699 \n",
      "Average Independence Loss: 1.458 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 9061.486 \n",
      "Average KL Loss: 71.139 \n",
      "Average Regression Loss: 1.401 \n",
      "Average Shared Loss: 1.694 \n",
      "Average Independence Loss: 1.409 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 9037.098 \n",
      "Average KL Loss: 71.095 \n",
      "Average Regression Loss: 1.303 \n",
      "Average Shared Loss: 1.698 \n",
      "Average Independence Loss: 1.371 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 9033.121 \n",
      "Average KL Loss: 71.137 \n",
      "Average Regression Loss: 1.320 \n",
      "Average Shared Loss: 1.698 \n",
      "Average Independence Loss: 1.307 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 9032.338 \n",
      "Average KL Loss: 71.164 \n",
      "Average Regression Loss: 1.285 \n",
      "Average Shared Loss: 1.696 \n",
      "Average Independence Loss: 1.315 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 9079.969 \n",
      "Average KL Loss: 71.133 \n",
      "Average Regression Loss: 1.283 \n",
      "Average Shared Loss: 1.697 \n",
      "Average Independence Loss: 1.261 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 9037.222 \n",
      "Average KL Loss: 71.065 \n",
      "Average Regression Loss: 1.184 \n",
      "Average Shared Loss: 1.694 \n",
      "Average Independence Loss: 1.182 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 9035.583 \n",
      "Average KL Loss: 71.162 \n",
      "Average Regression Loss: 1.171 \n",
      "Average Shared Loss: 1.690 \n",
      "Average Independence Loss: 1.278 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 9062.209 \n",
      "Average KL Loss: 71.116 \n",
      "Average Regression Loss: 1.215 \n",
      "Average Shared Loss: 1.694 \n",
      "Average Independence Loss: 1.154 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 9067.248 \n",
      "Average KL Loss: 71.159 \n",
      "Average Regression Loss: 1.156 \n",
      "Average Shared Loss: 1.692 \n",
      "Average Independence Loss: 1.182 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 9055.634 \n",
      "Average KL Loss: 71.221 \n",
      "Average Regression Loss: 1.203 \n",
      "Average Shared Loss: 1.694 \n",
      "Average Independence Loss: 1.178 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 9040.989 \n",
      "Average KL Loss: 71.092 \n",
      "Average Regression Loss: 1.191 \n",
      "Average Shared Loss: 1.693 \n",
      "Average Independence Loss: 1.058 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 9074.633 \n",
      "Average KL Loss: 71.141 \n",
      "Average Regression Loss: 1.146 \n",
      "Average Shared Loss: 1.691 \n",
      "Average Independence Loss: 1.103 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 9098.162 \n",
      "Average KL Loss: 71.154 \n",
      "Average Regression Loss: 1.182 \n",
      "Average Shared Loss: 1.689 \n",
      "Average Independence Loss: 1.070 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 9070.196 \n",
      "Average KL Loss: 71.093 \n",
      "Average Regression Loss: 1.155 \n",
      "Average Shared Loss: 1.686 \n",
      "Average Independence Loss: 1.012 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 9095.795 \n",
      "Average KL Loss: 71.108 \n",
      "Average Regression Loss: 1.083 \n",
      "Average Shared Loss: 1.688 \n",
      "Average Independence Loss: 1.023 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 9094.022 \n",
      "Average KL Loss: 71.088 \n",
      "Average Regression Loss: 1.169 \n",
      "Average Shared Loss: 1.687 \n",
      "Average Independence Loss: 0.980 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 9077.227 \n",
      "Average KL Loss: 71.244 \n",
      "Average Regression Loss: 1.096 \n",
      "Average Shared Loss: 1.687 \n",
      "Average Independence Loss: 1.027 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 9046.234 \n",
      "Average KL Loss: 71.111 \n",
      "Average Regression Loss: 1.063 \n",
      "Average Shared Loss: 1.684 \n",
      "Average Independence Loss: 0.947 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 9078.011 \n",
      "Average KL Loss: 71.143 \n",
      "Average Regression Loss: 1.056 \n",
      "Average Shared Loss: 1.684 \n",
      "Average Independence Loss: 0.956 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 9057.868 \n",
      "Average KL Loss: 71.086 \n",
      "Average Regression Loss: 1.006 \n",
      "Average Shared Loss: 1.686 \n",
      "Average Independence Loss: 0.876 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 9076.910 \n",
      "Average KL Loss: 71.117 \n",
      "Average Regression Loss: 1.095 \n",
      "Average Shared Loss: 1.684 \n",
      "Average Independence Loss: 0.887 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 9035.931 \n",
      "Average KL Loss: 71.091 \n",
      "Average Regression Loss: 1.018 \n",
      "Average Shared Loss: 1.682 \n",
      "Average Independence Loss: 0.838 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 9082.352 \n",
      "Average KL Loss: 71.065 \n",
      "Average Regression Loss: 1.046 \n",
      "Average Shared Loss: 1.682 \n",
      "Average Independence Loss: 0.815 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 9041.614 \n",
      "Average KL Loss: 71.145 \n",
      "Average Regression Loss: 1.044 \n",
      "Average Shared Loss: 1.679 \n",
      "Average Independence Loss: 0.846 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 9041.601 \n",
      "Average KL Loss: 71.168 \n",
      "Average Regression Loss: 1.037 \n",
      "Average Shared Loss: 1.680 \n",
      "Average Independence Loss: 0.819 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 9050.160 \n",
      "Average KL Loss: 71.095 \n",
      "Average Regression Loss: 1.078 \n",
      "Average Shared Loss: 1.683 \n",
      "Average Independence Loss: 0.769 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 9075.003 \n",
      "Average KL Loss: 71.234 \n",
      "Average Regression Loss: 1.015 \n",
      "Average Shared Loss: 1.680 \n",
      "Average Independence Loss: 0.850 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 9103.768 \n",
      "Average KL Loss: 71.094 \n",
      "Average Regression Loss: 1.119 \n",
      "Average Shared Loss: 1.678 \n",
      "Average Independence Loss: 0.746 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 9105.871 \n",
      "Average KL Loss: 71.144 \n",
      "Average Regression Loss: 0.996 \n",
      "Average Shared Loss: 1.676 \n",
      "Average Independence Loss: 0.786 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 9038.114 \n",
      "Average KL Loss: 71.050 \n",
      "Average Regression Loss: 0.954 \n",
      "Average Shared Loss: 1.677 \n",
      "Average Independence Loss: 0.706 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 9091.172 \n",
      "Average KL Loss: 71.105 \n",
      "Average Regression Loss: 1.008 \n",
      "Average Shared Loss: 1.676 \n",
      "Average Independence Loss: 0.688 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 9082.440 \n",
      "Average KL Loss: 71.117 \n",
      "Average Regression Loss: 0.991 \n",
      "Average Shared Loss: 1.673 \n",
      "Average Independence Loss: 0.697 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 9061.692 \n",
      "Average KL Loss: 71.089 \n",
      "Average Regression Loss: 0.912 \n",
      "Average Shared Loss: 1.673 \n",
      "Average Independence Loss: 0.681 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 9039.016 \n",
      "Average KL Loss: 71.052 \n",
      "Average Regression Loss: 0.880 \n",
      "Average Shared Loss: 1.672 \n",
      "Average Independence Loss: 0.624 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 9051.335 \n",
      "Average KL Loss: 70.999 \n",
      "Average Regression Loss: 0.928 \n",
      "Average Shared Loss: 1.670 \n",
      "Average Independence Loss: 0.603 \n",
      "\n",
      "Average DNA Recon Loss: 27410.397 \n",
      "Average Gene Recon Loss: 3696.228 \n",
      "Average RPPA Recon Loss: 142.062 \n",
      "Average DNA KL Loss: 205.728 \n",
      "Average Gene KL Loss: 10.835 \n",
      "Average RPPA KL Loss: 0.662 \n",
      "Average Regressor Loss: 0.008 \n",
      "Average RMSE Loss: 0.088 \n",
      "Average Shared MSE Loss: 0.007 \n",
      "Average Shared RMSE Loss: 0.081 \n",
      "Average R2: 0.746 \n",
      "Average Indepence Loss: 1.010 \n",
      "\n",
      "Fold 5\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 9205.994 \n",
      "Average KL Loss: 71.148 \n",
      "Average Regression Loss: 1.235 \n",
      "Average Shared Loss: 1.678 \n",
      "Average Independence Loss: 0.958 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 9144.892 \n",
      "Average KL Loss: 71.164 \n",
      "Average Regression Loss: 1.258 \n",
      "Average Shared Loss: 1.690 \n",
      "Average Independence Loss: 1.958 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 9108.083 \n",
      "Average KL Loss: 71.201 \n",
      "Average Regression Loss: 1.319 \n",
      "Average Shared Loss: 1.697 \n",
      "Average Independence Loss: 2.737 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 9030.552 \n",
      "Average KL Loss: 71.367 \n",
      "Average Regression Loss: 1.349 \n",
      "Average Shared Loss: 1.697 \n",
      "Average Independence Loss: 3.258 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 8953.514 \n",
      "Average KL Loss: 71.422 \n",
      "Average Regression Loss: 1.266 \n",
      "Average Shared Loss: 1.696 \n",
      "Average Independence Loss: 3.482 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 8933.842 \n",
      "Average KL Loss: 71.336 \n",
      "Average Regression Loss: 1.243 \n",
      "Average Shared Loss: 1.691 \n",
      "Average Independence Loss: 3.478 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 8848.594 \n",
      "Average KL Loss: 71.387 \n",
      "Average Regression Loss: 1.256 \n",
      "Average Shared Loss: 1.689 \n",
      "Average Independence Loss: 3.513 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 8846.167 \n",
      "Average KL Loss: 71.405 \n",
      "Average Regression Loss: 1.325 \n",
      "Average Shared Loss: 1.683 \n",
      "Average Independence Loss: 3.574 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 8792.397 \n",
      "Average KL Loss: 71.490 \n",
      "Average Regression Loss: 1.297 \n",
      "Average Shared Loss: 1.673 \n",
      "Average Independence Loss: 3.609 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 8732.472 \n",
      "Average KL Loss: 71.290 \n",
      "Average Regression Loss: 1.279 \n",
      "Average Shared Loss: 1.671 \n",
      "Average Independence Loss: 3.538 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 8744.645 \n",
      "Average KL Loss: 71.354 \n",
      "Average Regression Loss: 1.366 \n",
      "Average Shared Loss: 1.669 \n",
      "Average Independence Loss: 3.572 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 8687.643 \n",
      "Average KL Loss: 71.329 \n",
      "Average Regression Loss: 1.262 \n",
      "Average Shared Loss: 1.672 \n",
      "Average Independence Loss: 3.555 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 8658.438 \n",
      "Average KL Loss: 71.183 \n",
      "Average Regression Loss: 1.221 \n",
      "Average Shared Loss: 1.664 \n",
      "Average Independence Loss: 3.501 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 8629.769 \n",
      "Average KL Loss: 71.224 \n",
      "Average Regression Loss: 1.261 \n",
      "Average Shared Loss: 1.662 \n",
      "Average Independence Loss: 3.527 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 8644.336 \n",
      "Average KL Loss: 71.121 \n",
      "Average Regression Loss: 1.273 \n",
      "Average Shared Loss: 1.659 \n",
      "Average Independence Loss: 3.540 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 8567.759 \n",
      "Average KL Loss: 71.172 \n",
      "Average Regression Loss: 1.281 \n",
      "Average Shared Loss: 1.659 \n",
      "Average Independence Loss: 3.551 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 8565.937 \n",
      "Average KL Loss: 71.055 \n",
      "Average Regression Loss: 1.337 \n",
      "Average Shared Loss: 1.656 \n",
      "Average Independence Loss: 3.487 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 8531.917 \n",
      "Average KL Loss: 70.911 \n",
      "Average Regression Loss: 1.327 \n",
      "Average Shared Loss: 1.651 \n",
      "Average Independence Loss: 3.394 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 8529.425 \n",
      "Average KL Loss: 70.930 \n",
      "Average Regression Loss: 1.277 \n",
      "Average Shared Loss: 1.644 \n",
      "Average Independence Loss: 3.407 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 8509.546 \n",
      "Average KL Loss: 70.790 \n",
      "Average Regression Loss: 1.271 \n",
      "Average Shared Loss: 1.639 \n",
      "Average Independence Loss: 3.353 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 8529.385 \n",
      "Average KL Loss: 70.852 \n",
      "Average Regression Loss: 1.375 \n",
      "Average Shared Loss: 1.634 \n",
      "Average Independence Loss: 3.328 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 8479.653 \n",
      "Average KL Loss: 70.688 \n",
      "Average Regression Loss: 1.315 \n",
      "Average Shared Loss: 1.630 \n",
      "Average Independence Loss: 3.336 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 8461.509 \n",
      "Average KL Loss: 70.519 \n",
      "Average Regression Loss: 1.265 \n",
      "Average Shared Loss: 1.630 \n",
      "Average Independence Loss: 3.314 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 8452.061 \n",
      "Average KL Loss: 70.476 \n",
      "Average Regression Loss: 1.326 \n",
      "Average Shared Loss: 1.622 \n",
      "Average Independence Loss: 3.333 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 8433.903 \n",
      "Average KL Loss: 70.465 \n",
      "Average Regression Loss: 1.298 \n",
      "Average Shared Loss: 1.624 \n",
      "Average Independence Loss: 3.334 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 8436.351 \n",
      "Average KL Loss: 70.358 \n",
      "Average Regression Loss: 1.278 \n",
      "Average Shared Loss: 1.617 \n",
      "Average Independence Loss: 3.281 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 8424.110 \n",
      "Average KL Loss: 70.319 \n",
      "Average Regression Loss: 1.189 \n",
      "Average Shared Loss: 1.614 \n",
      "Average Independence Loss: 3.249 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 8405.294 \n",
      "Average KL Loss: 70.227 \n",
      "Average Regression Loss: 1.319 \n",
      "Average Shared Loss: 1.606 \n",
      "Average Independence Loss: 3.197 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 8385.765 \n",
      "Average KL Loss: 70.212 \n",
      "Average Regression Loss: 1.374 \n",
      "Average Shared Loss: 1.606 \n",
      "Average Independence Loss: 3.226 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 8389.371 \n",
      "Average KL Loss: 70.112 \n",
      "Average Regression Loss: 1.253 \n",
      "Average Shared Loss: 1.604 \n",
      "Average Independence Loss: 3.239 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 8416.426 \n",
      "Average KL Loss: 70.189 \n",
      "Average Regression Loss: 1.380 \n",
      "Average Shared Loss: 1.601 \n",
      "Average Independence Loss: 3.187 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 8333.379 \n",
      "Average KL Loss: 70.096 \n",
      "Average Regression Loss: 1.281 \n",
      "Average Shared Loss: 1.596 \n",
      "Average Independence Loss: 3.146 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 8317.417 \n",
      "Average KL Loss: 70.015 \n",
      "Average Regression Loss: 1.214 \n",
      "Average Shared Loss: 1.595 \n",
      "Average Independence Loss: 3.164 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 8324.717 \n",
      "Average KL Loss: 69.916 \n",
      "Average Regression Loss: 1.242 \n",
      "Average Shared Loss: 1.591 \n",
      "Average Independence Loss: 3.120 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 8291.432 \n",
      "Average KL Loss: 69.834 \n",
      "Average Regression Loss: 1.273 \n",
      "Average Shared Loss: 1.587 \n",
      "Average Independence Loss: 3.112 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 8327.065 \n",
      "Average KL Loss: 69.747 \n",
      "Average Regression Loss: 1.290 \n",
      "Average Shared Loss: 1.584 \n",
      "Average Independence Loss: 3.087 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 8271.216 \n",
      "Average KL Loss: 69.661 \n",
      "Average Regression Loss: 1.256 \n",
      "Average Shared Loss: 1.581 \n",
      "Average Independence Loss: 3.046 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 8280.685 \n",
      "Average KL Loss: 69.623 \n",
      "Average Regression Loss: 1.362 \n",
      "Average Shared Loss: 1.573 \n",
      "Average Independence Loss: 3.027 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 8217.645 \n",
      "Average KL Loss: 69.542 \n",
      "Average Regression Loss: 1.259 \n",
      "Average Shared Loss: 1.576 \n",
      "Average Independence Loss: 3.051 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 8218.973 \n",
      "Average KL Loss: 69.487 \n",
      "Average Regression Loss: 1.323 \n",
      "Average Shared Loss: 1.566 \n",
      "Average Independence Loss: 3.134 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 8182.950 \n",
      "Average KL Loss: 69.420 \n",
      "Average Regression Loss: 1.296 \n",
      "Average Shared Loss: 1.566 \n",
      "Average Independence Loss: 3.149 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 8202.818 \n",
      "Average KL Loss: 69.373 \n",
      "Average Regression Loss: 1.379 \n",
      "Average Shared Loss: 1.563 \n",
      "Average Independence Loss: 3.128 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 8197.956 \n",
      "Average KL Loss: 69.300 \n",
      "Average Regression Loss: 1.374 \n",
      "Average Shared Loss: 1.563 \n",
      "Average Independence Loss: 3.107 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 8218.486 \n",
      "Average KL Loss: 69.162 \n",
      "Average Regression Loss: 1.279 \n",
      "Average Shared Loss: 1.558 \n",
      "Average Independence Loss: 3.096 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 8186.533 \n",
      "Average KL Loss: 69.140 \n",
      "Average Regression Loss: 1.289 \n",
      "Average Shared Loss: 1.557 \n",
      "Average Independence Loss: 3.088 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 8160.661 \n",
      "Average KL Loss: 68.988 \n",
      "Average Regression Loss: 1.394 \n",
      "Average Shared Loss: 1.553 \n",
      "Average Independence Loss: 3.082 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 8150.867 \n",
      "Average KL Loss: 68.944 \n",
      "Average Regression Loss: 1.250 \n",
      "Average Shared Loss: 1.546 \n",
      "Average Independence Loss: 3.072 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 8145.005 \n",
      "Average KL Loss: 68.900 \n",
      "Average Regression Loss: 1.238 \n",
      "Average Shared Loss: 1.542 \n",
      "Average Independence Loss: 3.039 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 8172.316 \n",
      "Average KL Loss: 68.825 \n",
      "Average Regression Loss: 1.281 \n",
      "Average Shared Loss: 1.537 \n",
      "Average Independence Loss: 3.078 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 8139.135 \n",
      "Average KL Loss: 68.657 \n",
      "Average Regression Loss: 1.352 \n",
      "Average Shared Loss: 1.536 \n",
      "Average Independence Loss: 3.037 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 8120.956 \n",
      "Average KL Loss: 68.691 \n",
      "Average Regression Loss: 1.368 \n",
      "Average Shared Loss: 1.528 \n",
      "Average Independence Loss: 2.769 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 8184.700 \n",
      "Average KL Loss: 68.603 \n",
      "Average Regression Loss: 1.301 \n",
      "Average Shared Loss: 1.524 \n",
      "Average Independence Loss: 1.962 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 8189.845 \n",
      "Average KL Loss: 68.589 \n",
      "Average Regression Loss: 1.141 \n",
      "Average Shared Loss: 1.522 \n",
      "Average Independence Loss: 1.106 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 8216.786 \n",
      "Average KL Loss: 68.615 \n",
      "Average Regression Loss: 1.153 \n",
      "Average Shared Loss: 1.520 \n",
      "Average Independence Loss: 0.522 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 8221.374 \n",
      "Average KL Loss: 68.629 \n",
      "Average Regression Loss: 1.099 \n",
      "Average Shared Loss: 1.519 \n",
      "Average Independence Loss: 0.357 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 8256.164 \n",
      "Average KL Loss: 68.577 \n",
      "Average Regression Loss: 1.134 \n",
      "Average Shared Loss: 1.518 \n",
      "Average Independence Loss: 0.253 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 8228.201 \n",
      "Average KL Loss: 68.644 \n",
      "Average Regression Loss: 1.062 \n",
      "Average Shared Loss: 1.520 \n",
      "Average Independence Loss: 0.195 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 8213.284 \n",
      "Average KL Loss: 68.658 \n",
      "Average Regression Loss: 1.005 \n",
      "Average Shared Loss: 1.520 \n",
      "Average Independence Loss: 0.155 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 8209.980 \n",
      "Average KL Loss: 68.582 \n",
      "Average Regression Loss: 0.976 \n",
      "Average Shared Loss: 1.519 \n",
      "Average Independence Loss: 0.112 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 8238.160 \n",
      "Average KL Loss: 68.566 \n",
      "Average Regression Loss: 0.998 \n",
      "Average Shared Loss: 1.518 \n",
      "Average Independence Loss: 0.079 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 8215.648 \n",
      "Average KL Loss: 68.539 \n",
      "Average Regression Loss: 0.937 \n",
      "Average Shared Loss: 1.516 \n",
      "Average Independence Loss: 0.053 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 8222.345 \n",
      "Average KL Loss: 68.685 \n",
      "Average Regression Loss: 0.930 \n",
      "Average Shared Loss: 1.512 \n",
      "Average Independence Loss: 0.037 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 8179.034 \n",
      "Average KL Loss: 68.548 \n",
      "Average Regression Loss: 0.848 \n",
      "Average Shared Loss: 1.517 \n",
      "Average Independence Loss: 0.022 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 8240.245 \n",
      "Average KL Loss: 68.661 \n",
      "Average Regression Loss: 0.928 \n",
      "Average Shared Loss: 1.514 \n",
      "Average Independence Loss: 0.013 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 8267.569 \n",
      "Average KL Loss: 68.541 \n",
      "Average Regression Loss: 1.007 \n",
      "Average Shared Loss: 1.516 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 8210.352 \n",
      "Average KL Loss: 68.588 \n",
      "Average Regression Loss: 0.873 \n",
      "Average Shared Loss: 1.511 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 8230.196 \n",
      "Average KL Loss: 68.579 \n",
      "Average Regression Loss: 0.978 \n",
      "Average Shared Loss: 1.514 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 8193.369 \n",
      "Average KL Loss: 68.572 \n",
      "Average Regression Loss: 0.805 \n",
      "Average Shared Loss: 1.511 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 8221.166 \n",
      "Average KL Loss: 68.606 \n",
      "Average Regression Loss: 0.953 \n",
      "Average Shared Loss: 1.507 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 8202.112 \n",
      "Average KL Loss: 68.553 \n",
      "Average Regression Loss: 0.854 \n",
      "Average Shared Loss: 1.511 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 8238.368 \n",
      "Average KL Loss: 68.614 \n",
      "Average Regression Loss: 0.850 \n",
      "Average Shared Loss: 1.510 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 8229.724 \n",
      "Average KL Loss: 68.536 \n",
      "Average Regression Loss: 0.898 \n",
      "Average Shared Loss: 1.508 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 8224.964 \n",
      "Average KL Loss: 68.580 \n",
      "Average Regression Loss: 0.889 \n",
      "Average Shared Loss: 1.505 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 8191.052 \n",
      "Average KL Loss: 68.529 \n",
      "Average Regression Loss: 0.779 \n",
      "Average Shared Loss: 1.508 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 8222.626 \n",
      "Average KL Loss: 68.598 \n",
      "Average Regression Loss: 0.853 \n",
      "Average Shared Loss: 1.508 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 8203.725 \n",
      "Average KL Loss: 68.544 \n",
      "Average Regression Loss: 0.815 \n",
      "Average Shared Loss: 1.508 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 8244.287 \n",
      "Average KL Loss: 68.548 \n",
      "Average Regression Loss: 0.786 \n",
      "Average Shared Loss: 1.506 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 8232.239 \n",
      "Average KL Loss: 68.560 \n",
      "Average Regression Loss: 0.850 \n",
      "Average Shared Loss: 1.506 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 8220.269 \n",
      "Average KL Loss: 68.513 \n",
      "Average Regression Loss: 0.798 \n",
      "Average Shared Loss: 1.502 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 8218.138 \n",
      "Average KL Loss: 68.564 \n",
      "Average Regression Loss: 0.758 \n",
      "Average Shared Loss: 1.503 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 8212.356 \n",
      "Average KL Loss: 68.591 \n",
      "Average Regression Loss: 0.754 \n",
      "Average Shared Loss: 1.501 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 8220.526 \n",
      "Average KL Loss: 68.582 \n",
      "Average Regression Loss: 0.757 \n",
      "Average Shared Loss: 1.501 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 8212.911 \n",
      "Average KL Loss: 68.578 \n",
      "Average Regression Loss: 0.795 \n",
      "Average Shared Loss: 1.499 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 8220.337 \n",
      "Average KL Loss: 68.590 \n",
      "Average Regression Loss: 0.758 \n",
      "Average Shared Loss: 1.501 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 8260.931 \n",
      "Average KL Loss: 68.562 \n",
      "Average Regression Loss: 0.765 \n",
      "Average Shared Loss: 1.501 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 8206.727 \n",
      "Average KL Loss: 68.557 \n",
      "Average Regression Loss: 0.737 \n",
      "Average Shared Loss: 1.499 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 8203.375 \n",
      "Average KL Loss: 68.606 \n",
      "Average Regression Loss: 0.777 \n",
      "Average Shared Loss: 1.498 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 8219.909 \n",
      "Average KL Loss: 68.630 \n",
      "Average Regression Loss: 0.760 \n",
      "Average Shared Loss: 1.498 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 8209.227 \n",
      "Average KL Loss: 68.538 \n",
      "Average Regression Loss: 0.669 \n",
      "Average Shared Loss: 1.497 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 8214.330 \n",
      "Average KL Loss: 68.569 \n",
      "Average Regression Loss: 0.726 \n",
      "Average Shared Loss: 1.495 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 8198.284 \n",
      "Average KL Loss: 68.550 \n",
      "Average Regression Loss: 0.715 \n",
      "Average Shared Loss: 1.492 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 8221.716 \n",
      "Average KL Loss: 68.614 \n",
      "Average Regression Loss: 0.701 \n",
      "Average Shared Loss: 1.494 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 8199.367 \n",
      "Average KL Loss: 68.633 \n",
      "Average Regression Loss: 0.744 \n",
      "Average Shared Loss: 1.492 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 8225.266 \n",
      "Average KL Loss: 68.623 \n",
      "Average Regression Loss: 0.726 \n",
      "Average Shared Loss: 1.492 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 8222.997 \n",
      "Average KL Loss: 68.525 \n",
      "Average Regression Loss: 0.733 \n",
      "Average Shared Loss: 1.490 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 8211.265 \n",
      "Average KL Loss: 68.594 \n",
      "Average Regression Loss: 0.685 \n",
      "Average Shared Loss: 1.491 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 8218.329 \n",
      "Average KL Loss: 68.602 \n",
      "Average Regression Loss: 0.690 \n",
      "Average Shared Loss: 1.488 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 8218.565 \n",
      "Average KL Loss: 68.581 \n",
      "Average Regression Loss: 0.680 \n",
      "Average Shared Loss: 1.488 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 8192.075 \n",
      "Average KL Loss: 68.533 \n",
      "Average Regression Loss: 0.705 \n",
      "Average Shared Loss: 1.489 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 8200.238 \n",
      "Average KL Loss: 68.550 \n",
      "Average Regression Loss: 0.669 \n",
      "Average Shared Loss: 1.487 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "Average DNA Recon Loss: 25093.244 \n",
      "Average Gene Recon Loss: 2115.898 \n",
      "Average RPPA Recon Loss: 142.872 \n",
      "Average DNA KL Loss: 197.888 \n",
      "Average Gene KL Loss: 5.000 \n",
      "Average RPPA KL Loss: 0.848 \n",
      "Average Regressor Loss: 0.006 \n",
      "Average RMSE Loss: 0.079 \n",
      "Average Shared MSE Loss: 0.005 \n",
      "Average Shared RMSE Loss: 0.073 \n",
      "Average R2: 0.824 \n",
      "Average Indepence Loss: 0.006 \n",
      "\n",
      "Fold 6\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 8390.032 \n",
      "Average KL Loss: 68.503 \n",
      "Average Regression Loss: 0.898 \n",
      "Average Shared Loss: 1.506 \n",
      "Average Independence Loss: 0.295 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 8353.265 \n",
      "Average KL Loss: 68.662 \n",
      "Average Regression Loss: 1.000 \n",
      "Average Shared Loss: 1.513 \n",
      "Average Independence Loss: 1.204 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 8374.215 \n",
      "Average KL Loss: 68.742 \n",
      "Average Regression Loss: 1.021 \n",
      "Average Shared Loss: 1.521 \n",
      "Average Independence Loss: 1.860 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 8346.397 \n",
      "Average KL Loss: 68.877 \n",
      "Average Regression Loss: 1.038 \n",
      "Average Shared Loss: 1.518 \n",
      "Average Independence Loss: 2.157 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 8283.305 \n",
      "Average KL Loss: 68.940 \n",
      "Average Regression Loss: 1.020 \n",
      "Average Shared Loss: 1.512 \n",
      "Average Independence Loss: 2.270 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 8217.664 \n",
      "Average KL Loss: 68.966 \n",
      "Average Regression Loss: 0.973 \n",
      "Average Shared Loss: 1.506 \n",
      "Average Independence Loss: 2.365 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 8176.044 \n",
      "Average KL Loss: 68.944 \n",
      "Average Regression Loss: 1.001 \n",
      "Average Shared Loss: 1.509 \n",
      "Average Independence Loss: 2.424 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 8101.906 \n",
      "Average KL Loss: 68.929 \n",
      "Average Regression Loss: 0.962 \n",
      "Average Shared Loss: 1.506 \n",
      "Average Independence Loss: 2.470 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 8083.228 \n",
      "Average KL Loss: 68.878 \n",
      "Average Regression Loss: 0.995 \n",
      "Average Shared Loss: 1.502 \n",
      "Average Independence Loss: 2.513 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 8087.223 \n",
      "Average KL Loss: 68.833 \n",
      "Average Regression Loss: 0.881 \n",
      "Average Shared Loss: 1.502 \n",
      "Average Independence Loss: 2.542 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 8072.372 \n",
      "Average KL Loss: 68.785 \n",
      "Average Regression Loss: 0.966 \n",
      "Average Shared Loss: 1.498 \n",
      "Average Independence Loss: 2.542 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 8068.128 \n",
      "Average KL Loss: 68.741 \n",
      "Average Regression Loss: 0.966 \n",
      "Average Shared Loss: 1.497 \n",
      "Average Independence Loss: 2.537 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 8042.050 \n",
      "Average KL Loss: 68.761 \n",
      "Average Regression Loss: 0.987 \n",
      "Average Shared Loss: 1.494 \n",
      "Average Independence Loss: 2.569 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 8013.678 \n",
      "Average KL Loss: 68.640 \n",
      "Average Regression Loss: 1.007 \n",
      "Average Shared Loss: 1.491 \n",
      "Average Independence Loss: 2.560 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 7975.620 \n",
      "Average KL Loss: 68.597 \n",
      "Average Regression Loss: 0.919 \n",
      "Average Shared Loss: 1.488 \n",
      "Average Independence Loss: 2.608 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 7992.686 \n",
      "Average KL Loss: 68.570 \n",
      "Average Regression Loss: 0.927 \n",
      "Average Shared Loss: 1.482 \n",
      "Average Independence Loss: 2.593 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 7985.164 \n",
      "Average KL Loss: 68.431 \n",
      "Average Regression Loss: 0.912 \n",
      "Average Shared Loss: 1.479 \n",
      "Average Independence Loss: 2.553 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 7956.311 \n",
      "Average KL Loss: 68.487 \n",
      "Average Regression Loss: 0.958 \n",
      "Average Shared Loss: 1.479 \n",
      "Average Independence Loss: 2.558 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 7945.623 \n",
      "Average KL Loss: 68.315 \n",
      "Average Regression Loss: 0.991 \n",
      "Average Shared Loss: 1.476 \n",
      "Average Independence Loss: 2.589 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 7980.493 \n",
      "Average KL Loss: 68.271 \n",
      "Average Regression Loss: 1.020 \n",
      "Average Shared Loss: 1.472 \n",
      "Average Independence Loss: 2.542 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 7940.890 \n",
      "Average KL Loss: 68.167 \n",
      "Average Regression Loss: 0.974 \n",
      "Average Shared Loss: 1.469 \n",
      "Average Independence Loss: 2.553 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 7945.154 \n",
      "Average KL Loss: 68.175 \n",
      "Average Regression Loss: 0.948 \n",
      "Average Shared Loss: 1.466 \n",
      "Average Independence Loss: 2.547 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 7952.793 \n",
      "Average KL Loss: 68.114 \n",
      "Average Regression Loss: 1.001 \n",
      "Average Shared Loss: 1.464 \n",
      "Average Independence Loss: 2.516 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 7890.918 \n",
      "Average KL Loss: 68.037 \n",
      "Average Regression Loss: 0.992 \n",
      "Average Shared Loss: 1.460 \n",
      "Average Independence Loss: 2.518 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 7875.925 \n",
      "Average KL Loss: 67.976 \n",
      "Average Regression Loss: 0.843 \n",
      "Average Shared Loss: 1.457 \n",
      "Average Independence Loss: 2.555 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 7873.483 \n",
      "Average KL Loss: 67.914 \n",
      "Average Regression Loss: 0.958 \n",
      "Average Shared Loss: 1.456 \n",
      "Average Independence Loss: 2.525 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 7876.869 \n",
      "Average KL Loss: 67.828 \n",
      "Average Regression Loss: 1.030 \n",
      "Average Shared Loss: 1.456 \n",
      "Average Independence Loss: 2.521 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 7859.051 \n",
      "Average KL Loss: 67.755 \n",
      "Average Regression Loss: 0.935 \n",
      "Average Shared Loss: 1.449 \n",
      "Average Independence Loss: 2.508 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 7833.271 \n",
      "Average KL Loss: 67.660 \n",
      "Average Regression Loss: 0.911 \n",
      "Average Shared Loss: 1.445 \n",
      "Average Independence Loss: 2.502 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 7823.776 \n",
      "Average KL Loss: 67.567 \n",
      "Average Regression Loss: 0.874 \n",
      "Average Shared Loss: 1.442 \n",
      "Average Independence Loss: 2.582 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 7804.495 \n",
      "Average KL Loss: 67.498 \n",
      "Average Regression Loss: 0.989 \n",
      "Average Shared Loss: 1.439 \n",
      "Average Independence Loss: 2.569 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 7799.201 \n",
      "Average KL Loss: 67.399 \n",
      "Average Regression Loss: 0.934 \n",
      "Average Shared Loss: 1.435 \n",
      "Average Independence Loss: 2.586 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 7784.922 \n",
      "Average KL Loss: 67.283 \n",
      "Average Regression Loss: 0.943 \n",
      "Average Shared Loss: 1.431 \n",
      "Average Independence Loss: 2.578 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 7809.916 \n",
      "Average KL Loss: 67.272 \n",
      "Average Regression Loss: 1.000 \n",
      "Average Shared Loss: 1.430 \n",
      "Average Independence Loss: 2.573 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 7789.491 \n",
      "Average KL Loss: 67.142 \n",
      "Average Regression Loss: 1.019 \n",
      "Average Shared Loss: 1.425 \n",
      "Average Independence Loss: 2.621 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 7841.445 \n",
      "Average KL Loss: 67.100 \n",
      "Average Regression Loss: 1.014 \n",
      "Average Shared Loss: 1.427 \n",
      "Average Independence Loss: 2.677 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 7784.590 \n",
      "Average KL Loss: 67.108 \n",
      "Average Regression Loss: 1.007 \n",
      "Average Shared Loss: 1.425 \n",
      "Average Independence Loss: 2.636 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 7799.463 \n",
      "Average KL Loss: 67.061 \n",
      "Average Regression Loss: 0.950 \n",
      "Average Shared Loss: 1.419 \n",
      "Average Independence Loss: 2.656 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 7792.552 \n",
      "Average KL Loss: 66.937 \n",
      "Average Regression Loss: 0.983 \n",
      "Average Shared Loss: 1.417 \n",
      "Average Independence Loss: 2.680 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 7759.045 \n",
      "Average KL Loss: 66.837 \n",
      "Average Regression Loss: 0.924 \n",
      "Average Shared Loss: 1.415 \n",
      "Average Independence Loss: 2.675 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 7762.385 \n",
      "Average KL Loss: 66.755 \n",
      "Average Regression Loss: 0.933 \n",
      "Average Shared Loss: 1.411 \n",
      "Average Independence Loss: 2.684 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 7760.455 \n",
      "Average KL Loss: 66.717 \n",
      "Average Regression Loss: 0.889 \n",
      "Average Shared Loss: 1.409 \n",
      "Average Independence Loss: 2.646 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 7734.681 \n",
      "Average KL Loss: 66.622 \n",
      "Average Regression Loss: 0.962 \n",
      "Average Shared Loss: 1.405 \n",
      "Average Independence Loss: 2.600 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 7780.302 \n",
      "Average KL Loss: 66.558 \n",
      "Average Regression Loss: 0.934 \n",
      "Average Shared Loss: 1.402 \n",
      "Average Independence Loss: 2.546 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 7725.755 \n",
      "Average KL Loss: 66.479 \n",
      "Average Regression Loss: 0.920 \n",
      "Average Shared Loss: 1.397 \n",
      "Average Independence Loss: 2.523 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 7737.158 \n",
      "Average KL Loss: 66.430 \n",
      "Average Regression Loss: 0.951 \n",
      "Average Shared Loss: 1.395 \n",
      "Average Independence Loss: 2.477 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 7708.432 \n",
      "Average KL Loss: 66.349 \n",
      "Average Regression Loss: 0.917 \n",
      "Average Shared Loss: 1.389 \n",
      "Average Independence Loss: 2.456 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 7708.642 \n",
      "Average KL Loss: 66.369 \n",
      "Average Regression Loss: 0.880 \n",
      "Average Shared Loss: 1.386 \n",
      "Average Independence Loss: 2.490 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 7695.141 \n",
      "Average KL Loss: 66.282 \n",
      "Average Regression Loss: 0.963 \n",
      "Average Shared Loss: 1.386 \n",
      "Average Independence Loss: 2.466 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 7678.484 \n",
      "Average KL Loss: 66.210 \n",
      "Average Regression Loss: 0.968 \n",
      "Average Shared Loss: 1.381 \n",
      "Average Independence Loss: 2.417 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 7702.533 \n",
      "Average KL Loss: 66.122 \n",
      "Average Regression Loss: 0.914 \n",
      "Average Shared Loss: 1.377 \n",
      "Average Independence Loss: 2.080 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 7755.700 \n",
      "Average KL Loss: 66.069 \n",
      "Average Regression Loss: 0.888 \n",
      "Average Shared Loss: 1.372 \n",
      "Average Independence Loss: 1.120 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 7761.045 \n",
      "Average KL Loss: 66.058 \n",
      "Average Regression Loss: 0.910 \n",
      "Average Shared Loss: 1.374 \n",
      "Average Independence Loss: 0.366 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 7797.823 \n",
      "Average KL Loss: 66.055 \n",
      "Average Regression Loss: 0.896 \n",
      "Average Shared Loss: 1.370 \n",
      "Average Independence Loss: 0.141 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 7810.789 \n",
      "Average KL Loss: 66.064 \n",
      "Average Regression Loss: 0.815 \n",
      "Average Shared Loss: 1.373 \n",
      "Average Independence Loss: 0.052 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 7782.430 \n",
      "Average KL Loss: 66.047 \n",
      "Average Regression Loss: 0.886 \n",
      "Average Shared Loss: 1.372 \n",
      "Average Independence Loss: 0.018 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 7764.310 \n",
      "Average KL Loss: 66.046 \n",
      "Average Regression Loss: 0.842 \n",
      "Average Shared Loss: 1.369 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 7783.294 \n",
      "Average KL Loss: 66.065 \n",
      "Average Regression Loss: 0.816 \n",
      "Average Shared Loss: 1.368 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 7761.394 \n",
      "Average KL Loss: 66.052 \n",
      "Average Regression Loss: 0.805 \n",
      "Average Shared Loss: 1.367 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 7737.212 \n",
      "Average KL Loss: 66.036 \n",
      "Average Regression Loss: 0.755 \n",
      "Average Shared Loss: 1.369 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 7786.214 \n",
      "Average KL Loss: 66.049 \n",
      "Average Regression Loss: 0.814 \n",
      "Average Shared Loss: 1.367 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 7786.504 \n",
      "Average KL Loss: 66.080 \n",
      "Average Regression Loss: 0.706 \n",
      "Average Shared Loss: 1.365 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 7744.514 \n",
      "Average KL Loss: 66.047 \n",
      "Average Regression Loss: 0.720 \n",
      "Average Shared Loss: 1.364 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 7749.128 \n",
      "Average KL Loss: 66.034 \n",
      "Average Regression Loss: 0.712 \n",
      "Average Shared Loss: 1.362 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 7781.583 \n",
      "Average KL Loss: 66.015 \n",
      "Average Regression Loss: 0.720 \n",
      "Average Shared Loss: 1.362 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 7782.811 \n",
      "Average KL Loss: 66.017 \n",
      "Average Regression Loss: 0.764 \n",
      "Average Shared Loss: 1.359 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 7752.190 \n",
      "Average KL Loss: 66.061 \n",
      "Average Regression Loss: 0.691 \n",
      "Average Shared Loss: 1.360 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 7787.969 \n",
      "Average KL Loss: 66.056 \n",
      "Average Regression Loss: 0.693 \n",
      "Average Shared Loss: 1.361 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 7782.454 \n",
      "Average KL Loss: 66.073 \n",
      "Average Regression Loss: 0.740 \n",
      "Average Shared Loss: 1.356 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 7772.958 \n",
      "Average KL Loss: 66.059 \n",
      "Average Regression Loss: 0.653 \n",
      "Average Shared Loss: 1.359 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 7764.955 \n",
      "Average KL Loss: 66.045 \n",
      "Average Regression Loss: 0.684 \n",
      "Average Shared Loss: 1.357 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 7769.298 \n",
      "Average KL Loss: 66.060 \n",
      "Average Regression Loss: 0.672 \n",
      "Average Shared Loss: 1.351 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 7764.413 \n",
      "Average KL Loss: 66.041 \n",
      "Average Regression Loss: 0.637 \n",
      "Average Shared Loss: 1.353 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 7768.500 \n",
      "Average KL Loss: 66.046 \n",
      "Average Regression Loss: 0.656 \n",
      "Average Shared Loss: 1.355 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 7791.691 \n",
      "Average KL Loss: 66.047 \n",
      "Average Regression Loss: 0.785 \n",
      "Average Shared Loss: 1.353 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 7757.681 \n",
      "Average KL Loss: 66.059 \n",
      "Average Regression Loss: 0.688 \n",
      "Average Shared Loss: 1.352 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 7789.573 \n",
      "Average KL Loss: 66.015 \n",
      "Average Regression Loss: 0.704 \n",
      "Average Shared Loss: 1.351 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 7753.730 \n",
      "Average KL Loss: 66.014 \n",
      "Average Regression Loss: 0.652 \n",
      "Average Shared Loss: 1.349 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 7796.680 \n",
      "Average KL Loss: 66.049 \n",
      "Average Regression Loss: 0.654 \n",
      "Average Shared Loss: 1.351 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 7767.455 \n",
      "Average KL Loss: 66.000 \n",
      "Average Regression Loss: 0.744 \n",
      "Average Shared Loss: 1.349 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 7769.985 \n",
      "Average KL Loss: 66.037 \n",
      "Average Regression Loss: 0.653 \n",
      "Average Shared Loss: 1.347 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 7774.081 \n",
      "Average KL Loss: 66.026 \n",
      "Average Regression Loss: 0.650 \n",
      "Average Shared Loss: 1.346 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 7745.034 \n",
      "Average KL Loss: 66.043 \n",
      "Average Regression Loss: 0.642 \n",
      "Average Shared Loss: 1.346 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 7742.094 \n",
      "Average KL Loss: 66.012 \n",
      "Average Regression Loss: 0.639 \n",
      "Average Shared Loss: 1.347 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 7796.992 \n",
      "Average KL Loss: 66.033 \n",
      "Average Regression Loss: 0.622 \n",
      "Average Shared Loss: 1.343 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 7731.845 \n",
      "Average KL Loss: 66.043 \n",
      "Average Regression Loss: 0.631 \n",
      "Average Shared Loss: 1.344 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 7762.368 \n",
      "Average KL Loss: 66.049 \n",
      "Average Regression Loss: 0.607 \n",
      "Average Shared Loss: 1.342 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 7787.703 \n",
      "Average KL Loss: 66.030 \n",
      "Average Regression Loss: 0.654 \n",
      "Average Shared Loss: 1.340 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 7788.506 \n",
      "Average KL Loss: 66.025 \n",
      "Average Regression Loss: 0.629 \n",
      "Average Shared Loss: 1.338 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 7776.213 \n",
      "Average KL Loss: 66.022 \n",
      "Average Regression Loss: 0.640 \n",
      "Average Shared Loss: 1.341 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 7739.646 \n",
      "Average KL Loss: 66.041 \n",
      "Average Regression Loss: 0.568 \n",
      "Average Shared Loss: 1.340 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 7765.921 \n",
      "Average KL Loss: 65.977 \n",
      "Average Regression Loss: 0.629 \n",
      "Average Shared Loss: 1.340 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 7756.076 \n",
      "Average KL Loss: 66.014 \n",
      "Average Regression Loss: 0.607 \n",
      "Average Shared Loss: 1.339 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 7797.139 \n",
      "Average KL Loss: 66.001 \n",
      "Average Regression Loss: 0.626 \n",
      "Average Shared Loss: 1.336 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 7720.347 \n",
      "Average KL Loss: 65.983 \n",
      "Average Regression Loss: 0.595 \n",
      "Average Shared Loss: 1.337 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 7762.361 \n",
      "Average KL Loss: 66.006 \n",
      "Average Regression Loss: 0.549 \n",
      "Average Shared Loss: 1.334 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 7781.838 \n",
      "Average KL Loss: 65.989 \n",
      "Average Regression Loss: 0.617 \n",
      "Average Shared Loss: 1.332 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 7769.270 \n",
      "Average KL Loss: 66.019 \n",
      "Average Regression Loss: 0.575 \n",
      "Average Shared Loss: 1.334 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 7798.732 \n",
      "Average KL Loss: 66.054 \n",
      "Average Regression Loss: 0.718 \n",
      "Average Shared Loss: 1.333 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 7792.941 \n",
      "Average KL Loss: 65.993 \n",
      "Average Regression Loss: 0.636 \n",
      "Average Shared Loss: 1.331 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "Average DNA Recon Loss: 24437.121 \n",
      "Average Gene Recon Loss: 1247.034 \n",
      "Average RPPA Recon Loss: 142.749 \n",
      "Average DNA KL Loss: 187.222 \n",
      "Average Gene KL Loss: 1.652 \n",
      "Average RPPA KL Loss: 0.930 \n",
      "Average Regressor Loss: 0.004 \n",
      "Average RMSE Loss: 0.063 \n",
      "Average Shared MSE Loss: 0.005 \n",
      "Average Shared RMSE Loss: 0.069 \n",
      "Average R2: 0.891 \n",
      "Average Indepence Loss: 0.005 \n",
      "\n",
      "Fold 7\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 7861.872 \n",
      "Average KL Loss: 65.997 \n",
      "Average Regression Loss: 0.795 \n",
      "Average Shared Loss: 1.335 \n",
      "Average Independence Loss: 0.274 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 7840.852 \n",
      "Average KL Loss: 65.997 \n",
      "Average Regression Loss: 0.794 \n",
      "Average Shared Loss: 1.354 \n",
      "Average Independence Loss: 1.100 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 7802.487 \n",
      "Average KL Loss: 66.119 \n",
      "Average Regression Loss: 0.812 \n",
      "Average Shared Loss: 1.354 \n",
      "Average Independence Loss: 1.671 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 7804.610 \n",
      "Average KL Loss: 66.146 \n",
      "Average Regression Loss: 0.799 \n",
      "Average Shared Loss: 1.354 \n",
      "Average Independence Loss: 1.928 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 7717.084 \n",
      "Average KL Loss: 66.183 \n",
      "Average Regression Loss: 0.902 \n",
      "Average Shared Loss: 1.358 \n",
      "Average Independence Loss: 2.012 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 7686.456 \n",
      "Average KL Loss: 66.169 \n",
      "Average Regression Loss: 0.819 \n",
      "Average Shared Loss: 1.359 \n",
      "Average Independence Loss: 2.015 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 7680.502 \n",
      "Average KL Loss: 66.139 \n",
      "Average Regression Loss: 0.821 \n",
      "Average Shared Loss: 1.356 \n",
      "Average Independence Loss: 2.016 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 7674.401 \n",
      "Average KL Loss: 66.115 \n",
      "Average Regression Loss: 0.801 \n",
      "Average Shared Loss: 1.352 \n",
      "Average Independence Loss: 2.013 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 7653.384 \n",
      "Average KL Loss: 66.122 \n",
      "Average Regression Loss: 0.864 \n",
      "Average Shared Loss: 1.349 \n",
      "Average Independence Loss: 2.106 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 7645.378 \n",
      "Average KL Loss: 66.058 \n",
      "Average Regression Loss: 0.875 \n",
      "Average Shared Loss: 1.351 \n",
      "Average Independence Loss: 2.130 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 7676.024 \n",
      "Average KL Loss: 65.985 \n",
      "Average Regression Loss: 0.910 \n",
      "Average Shared Loss: 1.346 \n",
      "Average Independence Loss: 2.148 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 7664.524 \n",
      "Average KL Loss: 65.986 \n",
      "Average Regression Loss: 0.909 \n",
      "Average Shared Loss: 1.343 \n",
      "Average Independence Loss: 2.234 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 7627.442 \n",
      "Average KL Loss: 65.919 \n",
      "Average Regression Loss: 0.790 \n",
      "Average Shared Loss: 1.342 \n",
      "Average Independence Loss: 2.228 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 7571.049 \n",
      "Average KL Loss: 65.845 \n",
      "Average Regression Loss: 0.795 \n",
      "Average Shared Loss: 1.338 \n",
      "Average Independence Loss: 2.186 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 7610.486 \n",
      "Average KL Loss: 65.780 \n",
      "Average Regression Loss: 0.915 \n",
      "Average Shared Loss: 1.340 \n",
      "Average Independence Loss: 2.169 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 7594.532 \n",
      "Average KL Loss: 65.716 \n",
      "Average Regression Loss: 0.787 \n",
      "Average Shared Loss: 1.336 \n",
      "Average Independence Loss: 2.176 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 7592.317 \n",
      "Average KL Loss: 65.631 \n",
      "Average Regression Loss: 0.850 \n",
      "Average Shared Loss: 1.331 \n",
      "Average Independence Loss: 2.168 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 7553.711 \n",
      "Average KL Loss: 65.562 \n",
      "Average Regression Loss: 0.782 \n",
      "Average Shared Loss: 1.330 \n",
      "Average Independence Loss: 2.179 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 7587.822 \n",
      "Average KL Loss: 65.462 \n",
      "Average Regression Loss: 0.872 \n",
      "Average Shared Loss: 1.324 \n",
      "Average Independence Loss: 2.201 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 7545.040 \n",
      "Average KL Loss: 65.378 \n",
      "Average Regression Loss: 0.834 \n",
      "Average Shared Loss: 1.325 \n",
      "Average Independence Loss: 2.178 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 7554.079 \n",
      "Average KL Loss: 65.358 \n",
      "Average Regression Loss: 0.783 \n",
      "Average Shared Loss: 1.324 \n",
      "Average Independence Loss: 2.191 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 7582.421 \n",
      "Average KL Loss: 65.300 \n",
      "Average Regression Loss: 0.780 \n",
      "Average Shared Loss: 1.313 \n",
      "Average Independence Loss: 2.206 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 7566.822 \n",
      "Average KL Loss: 65.208 \n",
      "Average Regression Loss: 0.867 \n",
      "Average Shared Loss: 1.314 \n",
      "Average Independence Loss: 2.249 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 7592.909 \n",
      "Average KL Loss: 65.180 \n",
      "Average Regression Loss: 0.848 \n",
      "Average Shared Loss: 1.313 \n",
      "Average Independence Loss: 2.308 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 7497.035 \n",
      "Average KL Loss: 65.098 \n",
      "Average Regression Loss: 0.719 \n",
      "Average Shared Loss: 1.311 \n",
      "Average Independence Loss: 2.311 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 7508.228 \n",
      "Average KL Loss: 65.025 \n",
      "Average Regression Loss: 0.823 \n",
      "Average Shared Loss: 1.309 \n",
      "Average Independence Loss: 2.271 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 7500.819 \n",
      "Average KL Loss: 64.943 \n",
      "Average Regression Loss: 0.736 \n",
      "Average Shared Loss: 1.309 \n",
      "Average Independence Loss: 2.224 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 7495.777 \n",
      "Average KL Loss: 64.880 \n",
      "Average Regression Loss: 0.823 \n",
      "Average Shared Loss: 1.305 \n",
      "Average Independence Loss: 2.205 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 7509.245 \n",
      "Average KL Loss: 64.796 \n",
      "Average Regression Loss: 0.842 \n",
      "Average Shared Loss: 1.301 \n",
      "Average Independence Loss: 2.194 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 7511.581 \n",
      "Average KL Loss: 64.739 \n",
      "Average Regression Loss: 0.774 \n",
      "Average Shared Loss: 1.299 \n",
      "Average Independence Loss: 2.211 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 7482.246 \n",
      "Average KL Loss: 64.635 \n",
      "Average Regression Loss: 0.785 \n",
      "Average Shared Loss: 1.294 \n",
      "Average Independence Loss: 2.241 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 7508.939 \n",
      "Average KL Loss: 64.578 \n",
      "Average Regression Loss: 0.822 \n",
      "Average Shared Loss: 1.293 \n",
      "Average Independence Loss: 2.269 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 7466.584 \n",
      "Average KL Loss: 64.481 \n",
      "Average Regression Loss: 0.810 \n",
      "Average Shared Loss: 1.289 \n",
      "Average Independence Loss: 2.237 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 7505.193 \n",
      "Average KL Loss: 64.409 \n",
      "Average Regression Loss: 0.866 \n",
      "Average Shared Loss: 1.283 \n",
      "Average Independence Loss: 2.193 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 7493.299 \n",
      "Average KL Loss: 64.335 \n",
      "Average Regression Loss: 0.827 \n",
      "Average Shared Loss: 1.282 \n",
      "Average Independence Loss: 2.185 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 7488.099 \n",
      "Average KL Loss: 64.268 \n",
      "Average Regression Loss: 0.794 \n",
      "Average Shared Loss: 1.279 \n",
      "Average Independence Loss: 2.255 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 7508.793 \n",
      "Average KL Loss: 64.278 \n",
      "Average Regression Loss: 0.853 \n",
      "Average Shared Loss: 1.278 \n",
      "Average Independence Loss: 2.270 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 7457.404 \n",
      "Average KL Loss: 64.230 \n",
      "Average Regression Loss: 0.840 \n",
      "Average Shared Loss: 1.272 \n",
      "Average Independence Loss: 2.218 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 7493.513 \n",
      "Average KL Loss: 64.178 \n",
      "Average Regression Loss: 0.879 \n",
      "Average Shared Loss: 1.273 \n",
      "Average Independence Loss: 2.189 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 7446.216 \n",
      "Average KL Loss: 64.114 \n",
      "Average Regression Loss: 0.818 \n",
      "Average Shared Loss: 1.265 \n",
      "Average Independence Loss: 2.177 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 7483.130 \n",
      "Average KL Loss: 64.024 \n",
      "Average Regression Loss: 0.918 \n",
      "Average Shared Loss: 1.269 \n",
      "Average Independence Loss: 2.134 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 7527.603 \n",
      "Average KL Loss: 63.979 \n",
      "Average Regression Loss: 0.912 \n",
      "Average Shared Loss: 1.262 \n",
      "Average Independence Loss: 2.064 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 7466.423 \n",
      "Average KL Loss: 63.914 \n",
      "Average Regression Loss: 0.788 \n",
      "Average Shared Loss: 1.261 \n",
      "Average Independence Loss: 2.029 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 7414.008 \n",
      "Average KL Loss: 63.848 \n",
      "Average Regression Loss: 0.846 \n",
      "Average Shared Loss: 1.260 \n",
      "Average Independence Loss: 2.032 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 7414.239 \n",
      "Average KL Loss: 63.798 \n",
      "Average Regression Loss: 0.730 \n",
      "Average Shared Loss: 1.255 \n",
      "Average Independence Loss: 2.094 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 7423.771 \n",
      "Average KL Loss: 63.731 \n",
      "Average Regression Loss: 0.821 \n",
      "Average Shared Loss: 1.254 \n",
      "Average Independence Loss: 2.147 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 7392.479 \n",
      "Average KL Loss: 63.650 \n",
      "Average Regression Loss: 0.803 \n",
      "Average Shared Loss: 1.253 \n",
      "Average Independence Loss: 2.108 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 7420.530 \n",
      "Average KL Loss: 63.594 \n",
      "Average Regression Loss: 0.803 \n",
      "Average Shared Loss: 1.254 \n",
      "Average Independence Loss: 2.131 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 7417.864 \n",
      "Average KL Loss: 63.519 \n",
      "Average Regression Loss: 0.841 \n",
      "Average Shared Loss: 1.249 \n",
      "Average Independence Loss: 2.132 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 7412.872 \n",
      "Average KL Loss: 63.463 \n",
      "Average Regression Loss: 0.955 \n",
      "Average Shared Loss: 1.250 \n",
      "Average Independence Loss: 2.168 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 7469.740 \n",
      "Average KL Loss: 63.404 \n",
      "Average Regression Loss: 0.831 \n",
      "Average Shared Loss: 1.243 \n",
      "Average Independence Loss: 1.871 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 7476.922 \n",
      "Average KL Loss: 63.389 \n",
      "Average Regression Loss: 0.796 \n",
      "Average Shared Loss: 1.242 \n",
      "Average Independence Loss: 0.861 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 7500.198 \n",
      "Average KL Loss: 63.379 \n",
      "Average Regression Loss: 0.855 \n",
      "Average Shared Loss: 1.239 \n",
      "Average Independence Loss: 0.229 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 7470.604 \n",
      "Average KL Loss: 63.384 \n",
      "Average Regression Loss: 0.739 \n",
      "Average Shared Loss: 1.238 \n",
      "Average Independence Loss: 0.082 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 7481.169 \n",
      "Average KL Loss: 63.373 \n",
      "Average Regression Loss: 0.682 \n",
      "Average Shared Loss: 1.239 \n",
      "Average Independence Loss: 0.030 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 7497.016 \n",
      "Average KL Loss: 63.375 \n",
      "Average Regression Loss: 0.772 \n",
      "Average Shared Loss: 1.240 \n",
      "Average Independence Loss: 0.013 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 7514.096 \n",
      "Average KL Loss: 63.384 \n",
      "Average Regression Loss: 0.717 \n",
      "Average Shared Loss: 1.237 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 7456.706 \n",
      "Average KL Loss: 63.380 \n",
      "Average Regression Loss: 0.683 \n",
      "Average Shared Loss: 1.234 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 7497.665 \n",
      "Average KL Loss: 63.373 \n",
      "Average Regression Loss: 0.644 \n",
      "Average Shared Loss: 1.235 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 7464.064 \n",
      "Average KL Loss: 63.372 \n",
      "Average Regression Loss: 0.710 \n",
      "Average Shared Loss: 1.232 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 7497.593 \n",
      "Average KL Loss: 63.376 \n",
      "Average Regression Loss: 0.664 \n",
      "Average Shared Loss: 1.233 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 7473.708 \n",
      "Average KL Loss: 63.377 \n",
      "Average Regression Loss: 0.629 \n",
      "Average Shared Loss: 1.233 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 7472.461 \n",
      "Average KL Loss: 63.395 \n",
      "Average Regression Loss: 0.650 \n",
      "Average Shared Loss: 1.232 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 7467.749 \n",
      "Average KL Loss: 63.363 \n",
      "Average Regression Loss: 0.647 \n",
      "Average Shared Loss: 1.232 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 7478.418 \n",
      "Average KL Loss: 63.399 \n",
      "Average Regression Loss: 0.630 \n",
      "Average Shared Loss: 1.229 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 7501.362 \n",
      "Average KL Loss: 63.377 \n",
      "Average Regression Loss: 0.646 \n",
      "Average Shared Loss: 1.227 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 7506.107 \n",
      "Average KL Loss: 63.370 \n",
      "Average Regression Loss: 0.670 \n",
      "Average Shared Loss: 1.228 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 7506.766 \n",
      "Average KL Loss: 63.353 \n",
      "Average Regression Loss: 0.614 \n",
      "Average Shared Loss: 1.227 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 7483.480 \n",
      "Average KL Loss: 63.375 \n",
      "Average Regression Loss: 0.609 \n",
      "Average Shared Loss: 1.227 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 7445.744 \n",
      "Average KL Loss: 63.355 \n",
      "Average Regression Loss: 0.523 \n",
      "Average Shared Loss: 1.226 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 7477.472 \n",
      "Average KL Loss: 63.382 \n",
      "Average Regression Loss: 0.604 \n",
      "Average Shared Loss: 1.225 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 7477.723 \n",
      "Average KL Loss: 63.350 \n",
      "Average Regression Loss: 0.663 \n",
      "Average Shared Loss: 1.225 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 7500.595 \n",
      "Average KL Loss: 63.347 \n",
      "Average Regression Loss: 0.623 \n",
      "Average Shared Loss: 1.223 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 7483.019 \n",
      "Average KL Loss: 63.364 \n",
      "Average Regression Loss: 0.607 \n",
      "Average Shared Loss: 1.222 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 7511.712 \n",
      "Average KL Loss: 63.370 \n",
      "Average Regression Loss: 0.695 \n",
      "Average Shared Loss: 1.222 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 7535.855 \n",
      "Average KL Loss: 63.373 \n",
      "Average Regression Loss: 0.689 \n",
      "Average Shared Loss: 1.222 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 7488.343 \n",
      "Average KL Loss: 63.376 \n",
      "Average Regression Loss: 0.622 \n",
      "Average Shared Loss: 1.221 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 7458.584 \n",
      "Average KL Loss: 63.370 \n",
      "Average Regression Loss: 0.565 \n",
      "Average Shared Loss: 1.220 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 7489.303 \n",
      "Average KL Loss: 63.355 \n",
      "Average Regression Loss: 0.592 \n",
      "Average Shared Loss: 1.217 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 7498.401 \n",
      "Average KL Loss: 63.370 \n",
      "Average Regression Loss: 0.587 \n",
      "Average Shared Loss: 1.217 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 7508.017 \n",
      "Average KL Loss: 63.352 \n",
      "Average Regression Loss: 0.529 \n",
      "Average Shared Loss: 1.215 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 7467.924 \n",
      "Average KL Loss: 63.358 \n",
      "Average Regression Loss: 0.543 \n",
      "Average Shared Loss: 1.215 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 7504.434 \n",
      "Average KL Loss: 63.364 \n",
      "Average Regression Loss: 0.640 \n",
      "Average Shared Loss: 1.212 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 7516.567 \n",
      "Average KL Loss: 63.348 \n",
      "Average Regression Loss: 0.525 \n",
      "Average Shared Loss: 1.215 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 7511.309 \n",
      "Average KL Loss: 63.356 \n",
      "Average Regression Loss: 0.665 \n",
      "Average Shared Loss: 1.214 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 7478.573 \n",
      "Average KL Loss: 63.358 \n",
      "Average Regression Loss: 0.530 \n",
      "Average Shared Loss: 1.213 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 7536.239 \n",
      "Average KL Loss: 63.340 \n",
      "Average Regression Loss: 0.647 \n",
      "Average Shared Loss: 1.213 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 7508.258 \n",
      "Average KL Loss: 63.340 \n",
      "Average Regression Loss: 0.620 \n",
      "Average Shared Loss: 1.212 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 7494.399 \n",
      "Average KL Loss: 63.346 \n",
      "Average Regression Loss: 0.540 \n",
      "Average Shared Loss: 1.209 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 7462.962 \n",
      "Average KL Loss: 63.347 \n",
      "Average Regression Loss: 0.554 \n",
      "Average Shared Loss: 1.207 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 7481.865 \n",
      "Average KL Loss: 63.343 \n",
      "Average Regression Loss: 0.509 \n",
      "Average Shared Loss: 1.206 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 7479.317 \n",
      "Average KL Loss: 63.338 \n",
      "Average Regression Loss: 0.534 \n",
      "Average Shared Loss: 1.207 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 7474.055 \n",
      "Average KL Loss: 63.346 \n",
      "Average Regression Loss: 0.525 \n",
      "Average Shared Loss: 1.207 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 7506.082 \n",
      "Average KL Loss: 63.338 \n",
      "Average Regression Loss: 0.566 \n",
      "Average Shared Loss: 1.203 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 7502.975 \n",
      "Average KL Loss: 63.336 \n",
      "Average Regression Loss: 0.574 \n",
      "Average Shared Loss: 1.204 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 7424.887 \n",
      "Average KL Loss: 63.341 \n",
      "Average Regression Loss: 0.523 \n",
      "Average Shared Loss: 1.205 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 7450.257 \n",
      "Average KL Loss: 63.326 \n",
      "Average Regression Loss: 0.587 \n",
      "Average Shared Loss: 1.201 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 7483.626 \n",
      "Average KL Loss: 63.325 \n",
      "Average Regression Loss: 0.520 \n",
      "Average Shared Loss: 1.201 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 7497.889 \n",
      "Average KL Loss: 63.346 \n",
      "Average Regression Loss: 0.604 \n",
      "Average Shared Loss: 1.197 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 7451.076 \n",
      "Average KL Loss: 63.340 \n",
      "Average Regression Loss: 0.536 \n",
      "Average Shared Loss: 1.202 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "Average DNA Recon Loss: 25003.785 \n",
      "Average Gene Recon Loss: 757.930 \n",
      "Average RPPA Recon Loss: 141.129 \n",
      "Average DNA KL Loss: 176.156 \n",
      "Average Gene KL Loss: 1.556 \n",
      "Average RPPA KL Loss: 1.058 \n",
      "Average Regressor Loss: 0.005 \n",
      "Average RMSE Loss: 0.068 \n",
      "Average Shared MSE Loss: 0.004 \n",
      "Average Shared RMSE Loss: 0.065 \n",
      "Average R2: 0.877 \n",
      "Average Indepence Loss: 0.005 \n",
      "\n",
      "Fold 8\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 7554.122 \n",
      "Average KL Loss: 63.324 \n",
      "Average Regression Loss: 0.775 \n",
      "Average Shared Loss: 1.203 \n",
      "Average Independence Loss: 0.277 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 7577.964 \n",
      "Average KL Loss: 63.360 \n",
      "Average Regression Loss: 0.828 \n",
      "Average Shared Loss: 1.216 \n",
      "Average Independence Loss: 1.170 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 7555.333 \n",
      "Average KL Loss: 63.413 \n",
      "Average Regression Loss: 0.804 \n",
      "Average Shared Loss: 1.220 \n",
      "Average Independence Loss: 1.767 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 7544.962 \n",
      "Average KL Loss: 63.473 \n",
      "Average Regression Loss: 0.863 \n",
      "Average Shared Loss: 1.219 \n",
      "Average Independence Loss: 1.989 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 7503.400 \n",
      "Average KL Loss: 63.525 \n",
      "Average Regression Loss: 0.796 \n",
      "Average Shared Loss: 1.222 \n",
      "Average Independence Loss: 1.986 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 7473.678 \n",
      "Average KL Loss: 63.559 \n",
      "Average Regression Loss: 0.765 \n",
      "Average Shared Loss: 1.219 \n",
      "Average Independence Loss: 1.970 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 7473.436 \n",
      "Average KL Loss: 63.542 \n",
      "Average Regression Loss: 0.852 \n",
      "Average Shared Loss: 1.218 \n",
      "Average Independence Loss: 2.000 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 7455.884 \n",
      "Average KL Loss: 63.540 \n",
      "Average Regression Loss: 0.862 \n",
      "Average Shared Loss: 1.215 \n",
      "Average Independence Loss: 2.030 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 7428.197 \n",
      "Average KL Loss: 63.506 \n",
      "Average Regression Loss: 0.737 \n",
      "Average Shared Loss: 1.213 \n",
      "Average Independence Loss: 2.020 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 7449.126 \n",
      "Average KL Loss: 63.451 \n",
      "Average Regression Loss: 0.827 \n",
      "Average Shared Loss: 1.209 \n",
      "Average Independence Loss: 1.990 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 7436.260 \n",
      "Average KL Loss: 63.423 \n",
      "Average Regression Loss: 0.795 \n",
      "Average Shared Loss: 1.206 \n",
      "Average Independence Loss: 2.042 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 7468.427 \n",
      "Average KL Loss: 63.379 \n",
      "Average Regression Loss: 0.777 \n",
      "Average Shared Loss: 1.205 \n",
      "Average Independence Loss: 2.017 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 7417.143 \n",
      "Average KL Loss: 63.343 \n",
      "Average Regression Loss: 0.816 \n",
      "Average Shared Loss: 1.201 \n",
      "Average Independence Loss: 1.971 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 7398.974 \n",
      "Average KL Loss: 63.301 \n",
      "Average Regression Loss: 0.843 \n",
      "Average Shared Loss: 1.203 \n",
      "Average Independence Loss: 1.910 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 7403.996 \n",
      "Average KL Loss: 63.252 \n",
      "Average Regression Loss: 0.850 \n",
      "Average Shared Loss: 1.200 \n",
      "Average Independence Loss: 1.923 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 7372.370 \n",
      "Average KL Loss: 63.179 \n",
      "Average Regression Loss: 0.721 \n",
      "Average Shared Loss: 1.199 \n",
      "Average Independence Loss: 1.975 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 7377.624 \n",
      "Average KL Loss: 63.094 \n",
      "Average Regression Loss: 0.772 \n",
      "Average Shared Loss: 1.195 \n",
      "Average Independence Loss: 1.971 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 7380.677 \n",
      "Average KL Loss: 63.002 \n",
      "Average Regression Loss: 0.764 \n",
      "Average Shared Loss: 1.192 \n",
      "Average Independence Loss: 1.960 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 7403.796 \n",
      "Average KL Loss: 62.941 \n",
      "Average Regression Loss: 0.831 \n",
      "Average Shared Loss: 1.191 \n",
      "Average Independence Loss: 1.974 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 7368.611 \n",
      "Average KL Loss: 62.915 \n",
      "Average Regression Loss: 0.757 \n",
      "Average Shared Loss: 1.192 \n",
      "Average Independence Loss: 1.998 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 7369.820 \n",
      "Average KL Loss: 62.857 \n",
      "Average Regression Loss: 0.819 \n",
      "Average Shared Loss: 1.189 \n",
      "Average Independence Loss: 1.941 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 7361.766 \n",
      "Average KL Loss: 62.792 \n",
      "Average Regression Loss: 0.777 \n",
      "Average Shared Loss: 1.183 \n",
      "Average Independence Loss: 1.906 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 7344.075 \n",
      "Average KL Loss: 62.739 \n",
      "Average Regression Loss: 0.747 \n",
      "Average Shared Loss: 1.181 \n",
      "Average Independence Loss: 1.903 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 7388.144 \n",
      "Average KL Loss: 62.674 \n",
      "Average Regression Loss: 0.862 \n",
      "Average Shared Loss: 1.182 \n",
      "Average Independence Loss: 1.898 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 7410.102 \n",
      "Average KL Loss: 62.626 \n",
      "Average Regression Loss: 0.861 \n",
      "Average Shared Loss: 1.179 \n",
      "Average Independence Loss: 1.919 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 7357.976 \n",
      "Average KL Loss: 62.571 \n",
      "Average Regression Loss: 0.849 \n",
      "Average Shared Loss: 1.178 \n",
      "Average Independence Loss: 1.907 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 7367.516 \n",
      "Average KL Loss: 62.519 \n",
      "Average Regression Loss: 0.774 \n",
      "Average Shared Loss: 1.173 \n",
      "Average Independence Loss: 1.886 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 7350.865 \n",
      "Average KL Loss: 62.471 \n",
      "Average Regression Loss: 0.806 \n",
      "Average Shared Loss: 1.171 \n",
      "Average Independence Loss: 1.896 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 7341.013 \n",
      "Average KL Loss: 62.442 \n",
      "Average Regression Loss: 0.682 \n",
      "Average Shared Loss: 1.170 \n",
      "Average Independence Loss: 1.929 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 7368.395 \n",
      "Average KL Loss: 62.379 \n",
      "Average Regression Loss: 0.792 \n",
      "Average Shared Loss: 1.169 \n",
      "Average Independence Loss: 1.931 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 7336.685 \n",
      "Average KL Loss: 62.330 \n",
      "Average Regression Loss: 0.808 \n",
      "Average Shared Loss: 1.166 \n",
      "Average Independence Loss: 1.913 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 7338.479 \n",
      "Average KL Loss: 62.283 \n",
      "Average Regression Loss: 0.933 \n",
      "Average Shared Loss: 1.164 \n",
      "Average Independence Loss: 1.906 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 7311.472 \n",
      "Average KL Loss: 62.227 \n",
      "Average Regression Loss: 0.730 \n",
      "Average Shared Loss: 1.167 \n",
      "Average Independence Loss: 1.923 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 7352.562 \n",
      "Average KL Loss: 62.147 \n",
      "Average Regression Loss: 0.756 \n",
      "Average Shared Loss: 1.163 \n",
      "Average Independence Loss: 1.873 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 7331.729 \n",
      "Average KL Loss: 62.085 \n",
      "Average Regression Loss: 0.762 \n",
      "Average Shared Loss: 1.159 \n",
      "Average Independence Loss: 1.799 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 7316.238 \n",
      "Average KL Loss: 62.035 \n",
      "Average Regression Loss: 0.714 \n",
      "Average Shared Loss: 1.154 \n",
      "Average Independence Loss: 1.825 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 7310.254 \n",
      "Average KL Loss: 61.963 \n",
      "Average Regression Loss: 0.778 \n",
      "Average Shared Loss: 1.159 \n",
      "Average Independence Loss: 1.846 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 7346.049 \n",
      "Average KL Loss: 61.901 \n",
      "Average Regression Loss: 0.836 \n",
      "Average Shared Loss: 1.156 \n",
      "Average Independence Loss: 1.781 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 7303.811 \n",
      "Average KL Loss: 61.852 \n",
      "Average Regression Loss: 0.774 \n",
      "Average Shared Loss: 1.154 \n",
      "Average Independence Loss: 1.741 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 7316.680 \n",
      "Average KL Loss: 61.801 \n",
      "Average Regression Loss: 0.769 \n",
      "Average Shared Loss: 1.151 \n",
      "Average Independence Loss: 1.826 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 7322.394 \n",
      "Average KL Loss: 61.741 \n",
      "Average Regression Loss: 0.846 \n",
      "Average Shared Loss: 1.153 \n",
      "Average Independence Loss: 1.853 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 7310.848 \n",
      "Average KL Loss: 61.695 \n",
      "Average Regression Loss: 0.823 \n",
      "Average Shared Loss: 1.146 \n",
      "Average Independence Loss: 1.802 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 7297.158 \n",
      "Average KL Loss: 61.642 \n",
      "Average Regression Loss: 0.748 \n",
      "Average Shared Loss: 1.150 \n",
      "Average Independence Loss: 1.812 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 7342.836 \n",
      "Average KL Loss: 61.586 \n",
      "Average Regression Loss: 0.831 \n",
      "Average Shared Loss: 1.143 \n",
      "Average Independence Loss: 1.783 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 7301.441 \n",
      "Average KL Loss: 61.553 \n",
      "Average Regression Loss: 0.802 \n",
      "Average Shared Loss: 1.146 \n",
      "Average Independence Loss: 1.753 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 7315.866 \n",
      "Average KL Loss: 61.507 \n",
      "Average Regression Loss: 0.844 \n",
      "Average Shared Loss: 1.142 \n",
      "Average Independence Loss: 1.756 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 7276.382 \n",
      "Average KL Loss: 61.462 \n",
      "Average Regression Loss: 0.745 \n",
      "Average Shared Loss: 1.135 \n",
      "Average Independence Loss: 1.786 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 7251.574 \n",
      "Average KL Loss: 61.383 \n",
      "Average Regression Loss: 0.750 \n",
      "Average Shared Loss: 1.136 \n",
      "Average Independence Loss: 1.763 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 7263.834 \n",
      "Average KL Loss: 61.298 \n",
      "Average Regression Loss: 0.784 \n",
      "Average Shared Loss: 1.134 \n",
      "Average Independence Loss: 1.754 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 7281.816 \n",
      "Average KL Loss: 61.213 \n",
      "Average Regression Loss: 0.743 \n",
      "Average Shared Loss: 1.129 \n",
      "Average Independence Loss: 1.722 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 7276.076 \n",
      "Average KL Loss: 61.150 \n",
      "Average Regression Loss: 0.781 \n",
      "Average Shared Loss: 1.132 \n",
      "Average Independence Loss: 1.367 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 7320.522 \n",
      "Average KL Loss: 61.121 \n",
      "Average Regression Loss: 0.716 \n",
      "Average Shared Loss: 1.131 \n",
      "Average Independence Loss: 0.438 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 7315.620 \n",
      "Average KL Loss: 61.116 \n",
      "Average Regression Loss: 0.741 \n",
      "Average Shared Loss: 1.133 \n",
      "Average Independence Loss: 0.127 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 7318.045 \n",
      "Average KL Loss: 61.116 \n",
      "Average Regression Loss: 0.692 \n",
      "Average Shared Loss: 1.129 \n",
      "Average Independence Loss: 0.045 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 7341.403 \n",
      "Average KL Loss: 61.113 \n",
      "Average Regression Loss: 0.653 \n",
      "Average Shared Loss: 1.127 \n",
      "Average Independence Loss: 0.018 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 7346.202 \n",
      "Average KL Loss: 61.112 \n",
      "Average Regression Loss: 0.675 \n",
      "Average Shared Loss: 1.127 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 7311.300 \n",
      "Average KL Loss: 61.110 \n",
      "Average Regression Loss: 0.629 \n",
      "Average Shared Loss: 1.126 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 7358.680 \n",
      "Average KL Loss: 61.112 \n",
      "Average Regression Loss: 0.738 \n",
      "Average Shared Loss: 1.124 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 7357.780 \n",
      "Average KL Loss: 61.113 \n",
      "Average Regression Loss: 0.726 \n",
      "Average Shared Loss: 1.124 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 7342.501 \n",
      "Average KL Loss: 61.105 \n",
      "Average Regression Loss: 0.629 \n",
      "Average Shared Loss: 1.126 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 7319.691 \n",
      "Average KL Loss: 61.107 \n",
      "Average Regression Loss: 0.691 \n",
      "Average Shared Loss: 1.126 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 7356.719 \n",
      "Average KL Loss: 61.107 \n",
      "Average Regression Loss: 0.684 \n",
      "Average Shared Loss: 1.125 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 7326.860 \n",
      "Average KL Loss: 61.105 \n",
      "Average Regression Loss: 0.621 \n",
      "Average Shared Loss: 1.123 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 7404.435 \n",
      "Average KL Loss: 61.103 \n",
      "Average Regression Loss: 0.685 \n",
      "Average Shared Loss: 1.122 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 7341.079 \n",
      "Average KL Loss: 61.103 \n",
      "Average Regression Loss: 0.575 \n",
      "Average Shared Loss: 1.122 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 7342.933 \n",
      "Average KL Loss: 61.104 \n",
      "Average Regression Loss: 0.613 \n",
      "Average Shared Loss: 1.121 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 7357.390 \n",
      "Average KL Loss: 61.102 \n",
      "Average Regression Loss: 0.634 \n",
      "Average Shared Loss: 1.120 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 7366.835 \n",
      "Average KL Loss: 61.100 \n",
      "Average Regression Loss: 0.573 \n",
      "Average Shared Loss: 1.121 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 7364.782 \n",
      "Average KL Loss: 61.099 \n",
      "Average Regression Loss: 0.577 \n",
      "Average Shared Loss: 1.120 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 7328.231 \n",
      "Average KL Loss: 61.099 \n",
      "Average Regression Loss: 0.571 \n",
      "Average Shared Loss: 1.117 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 7344.903 \n",
      "Average KL Loss: 61.097 \n",
      "Average Regression Loss: 0.708 \n",
      "Average Shared Loss: 1.116 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 7352.549 \n",
      "Average KL Loss: 61.096 \n",
      "Average Regression Loss: 0.605 \n",
      "Average Shared Loss: 1.117 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 7367.401 \n",
      "Average KL Loss: 61.094 \n",
      "Average Regression Loss: 0.631 \n",
      "Average Shared Loss: 1.115 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 7368.038 \n",
      "Average KL Loss: 61.093 \n",
      "Average Regression Loss: 0.571 \n",
      "Average Shared Loss: 1.115 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 7341.237 \n",
      "Average KL Loss: 61.093 \n",
      "Average Regression Loss: 0.637 \n",
      "Average Shared Loss: 1.114 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 7327.703 \n",
      "Average KL Loss: 61.090 \n",
      "Average Regression Loss: 0.504 \n",
      "Average Shared Loss: 1.113 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 7362.259 \n",
      "Average KL Loss: 61.091 \n",
      "Average Regression Loss: 0.615 \n",
      "Average Shared Loss: 1.113 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 7322.545 \n",
      "Average KL Loss: 61.090 \n",
      "Average Regression Loss: 0.524 \n",
      "Average Shared Loss: 1.111 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 7375.915 \n",
      "Average KL Loss: 61.088 \n",
      "Average Regression Loss: 0.582 \n",
      "Average Shared Loss: 1.111 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 7361.967 \n",
      "Average KL Loss: 61.093 \n",
      "Average Regression Loss: 0.542 \n",
      "Average Shared Loss: 1.112 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 7328.292 \n",
      "Average KL Loss: 61.088 \n",
      "Average Regression Loss: 0.517 \n",
      "Average Shared Loss: 1.108 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 7309.969 \n",
      "Average KL Loss: 61.087 \n",
      "Average Regression Loss: 0.554 \n",
      "Average Shared Loss: 1.110 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 7314.762 \n",
      "Average KL Loss: 61.083 \n",
      "Average Regression Loss: 0.538 \n",
      "Average Shared Loss: 1.110 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 7318.816 \n",
      "Average KL Loss: 61.087 \n",
      "Average Regression Loss: 0.613 \n",
      "Average Shared Loss: 1.106 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 7319.732 \n",
      "Average KL Loss: 61.082 \n",
      "Average Regression Loss: 0.510 \n",
      "Average Shared Loss: 1.108 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 7379.261 \n",
      "Average KL Loss: 61.082 \n",
      "Average Regression Loss: 0.598 \n",
      "Average Shared Loss: 1.107 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 7350.641 \n",
      "Average KL Loss: 61.079 \n",
      "Average Regression Loss: 0.535 \n",
      "Average Shared Loss: 1.104 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 7302.574 \n",
      "Average KL Loss: 61.081 \n",
      "Average Regression Loss: 0.482 \n",
      "Average Shared Loss: 1.106 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 7354.802 \n",
      "Average KL Loss: 61.077 \n",
      "Average Regression Loss: 0.569 \n",
      "Average Shared Loss: 1.106 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 7317.728 \n",
      "Average KL Loss: 61.077 \n",
      "Average Regression Loss: 0.548 \n",
      "Average Shared Loss: 1.103 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 7318.518 \n",
      "Average KL Loss: 61.077 \n",
      "Average Regression Loss: 0.539 \n",
      "Average Shared Loss: 1.103 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 7326.961 \n",
      "Average KL Loss: 61.078 \n",
      "Average Regression Loss: 0.575 \n",
      "Average Shared Loss: 1.102 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 7335.516 \n",
      "Average KL Loss: 61.073 \n",
      "Average Regression Loss: 0.478 \n",
      "Average Shared Loss: 1.102 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 7317.292 \n",
      "Average KL Loss: 61.073 \n",
      "Average Regression Loss: 0.518 \n",
      "Average Shared Loss: 1.098 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 7324.830 \n",
      "Average KL Loss: 61.072 \n",
      "Average Regression Loss: 0.539 \n",
      "Average Shared Loss: 1.101 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 7310.902 \n",
      "Average KL Loss: 61.071 \n",
      "Average Regression Loss: 0.498 \n",
      "Average Shared Loss: 1.101 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 7324.844 \n",
      "Average KL Loss: 61.069 \n",
      "Average Regression Loss: 0.535 \n",
      "Average Shared Loss: 1.100 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 7325.225 \n",
      "Average KL Loss: 61.071 \n",
      "Average Regression Loss: 0.650 \n",
      "Average Shared Loss: 1.097 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 7332.225 \n",
      "Average KL Loss: 61.068 \n",
      "Average Regression Loss: 0.533 \n",
      "Average Shared Loss: 1.097 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 7340.745 \n",
      "Average KL Loss: 61.068 \n",
      "Average Regression Loss: 0.488 \n",
      "Average Shared Loss: 1.096 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "Average DNA Recon Loss: 24842.833 \n",
      "Average Gene Recon Loss: 488.498 \n",
      "Average RPPA Recon Loss: 139.736 \n",
      "Average DNA KL Loss: 174.105 \n",
      "Average Gene KL Loss: 1.471 \n",
      "Average RPPA KL Loss: 1.129 \n",
      "Average Regressor Loss: 0.003 \n",
      "Average RMSE Loss: 0.050 \n",
      "Average Shared MSE Loss: 0.004 \n",
      "Average Shared RMSE Loss: 0.067 \n",
      "Average R2: 0.915 \n",
      "Average Indepence Loss: 0.006 \n",
      "\n",
      "Fold 9\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 7394.571 \n",
      "Average KL Loss: 61.077 \n",
      "Average Regression Loss: 0.637 \n",
      "Average Shared Loss: 1.100 \n",
      "Average Independence Loss: 0.265 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 7365.689 \n",
      "Average KL Loss: 61.130 \n",
      "Average Regression Loss: 0.712 \n",
      "Average Shared Loss: 1.110 \n",
      "Average Independence Loss: 1.019 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 7384.724 \n",
      "Average KL Loss: 61.185 \n",
      "Average Regression Loss: 0.724 \n",
      "Average Shared Loss: 1.113 \n",
      "Average Independence Loss: 1.478 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 7337.165 \n",
      "Average KL Loss: 61.214 \n",
      "Average Regression Loss: 0.710 \n",
      "Average Shared Loss: 1.116 \n",
      "Average Independence Loss: 1.646 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 7359.495 \n",
      "Average KL Loss: 61.270 \n",
      "Average Regression Loss: 0.728 \n",
      "Average Shared Loss: 1.113 \n",
      "Average Independence Loss: 1.720 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 7329.755 \n",
      "Average KL Loss: 61.295 \n",
      "Average Regression Loss: 0.723 \n",
      "Average Shared Loss: 1.119 \n",
      "Average Independence Loss: 1.752 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 7294.490 \n",
      "Average KL Loss: 61.298 \n",
      "Average Regression Loss: 0.712 \n",
      "Average Shared Loss: 1.114 \n",
      "Average Independence Loss: 1.726 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 7264.437 \n",
      "Average KL Loss: 61.287 \n",
      "Average Regression Loss: 0.720 \n",
      "Average Shared Loss: 1.114 \n",
      "Average Independence Loss: 1.684 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 7278.157 \n",
      "Average KL Loss: 61.238 \n",
      "Average Regression Loss: 0.772 \n",
      "Average Shared Loss: 1.109 \n",
      "Average Independence Loss: 1.622 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 7304.967 \n",
      "Average KL Loss: 61.180 \n",
      "Average Regression Loss: 0.724 \n",
      "Average Shared Loss: 1.109 \n",
      "Average Independence Loss: 1.608 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 7289.174 \n",
      "Average KL Loss: 61.142 \n",
      "Average Regression Loss: 0.756 \n",
      "Average Shared Loss: 1.105 \n",
      "Average Independence Loss: 1.660 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 7268.561 \n",
      "Average KL Loss: 61.130 \n",
      "Average Regression Loss: 0.712 \n",
      "Average Shared Loss: 1.107 \n",
      "Average Independence Loss: 1.755 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 7244.518 \n",
      "Average KL Loss: 61.100 \n",
      "Average Regression Loss: 0.785 \n",
      "Average Shared Loss: 1.104 \n",
      "Average Independence Loss: 1.745 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 7259.874 \n",
      "Average KL Loss: 61.049 \n",
      "Average Regression Loss: 0.700 \n",
      "Average Shared Loss: 1.106 \n",
      "Average Independence Loss: 1.730 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 7247.150 \n",
      "Average KL Loss: 61.001 \n",
      "Average Regression Loss: 0.717 \n",
      "Average Shared Loss: 1.104 \n",
      "Average Independence Loss: 1.731 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 7247.901 \n",
      "Average KL Loss: 60.939 \n",
      "Average Regression Loss: 0.746 \n",
      "Average Shared Loss: 1.101 \n",
      "Average Independence Loss: 1.750 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 7250.863 \n",
      "Average KL Loss: 60.881 \n",
      "Average Regression Loss: 0.751 \n",
      "Average Shared Loss: 1.101 \n",
      "Average Independence Loss: 1.715 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 7257.326 \n",
      "Average KL Loss: 60.838 \n",
      "Average Regression Loss: 0.707 \n",
      "Average Shared Loss: 1.095 \n",
      "Average Independence Loss: 1.655 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 7236.892 \n",
      "Average KL Loss: 60.815 \n",
      "Average Regression Loss: 0.725 \n",
      "Average Shared Loss: 1.093 \n",
      "Average Independence Loss: 1.698 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 7237.830 \n",
      "Average KL Loss: 60.777 \n",
      "Average Regression Loss: 0.780 \n",
      "Average Shared Loss: 1.091 \n",
      "Average Independence Loss: 1.722 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 7246.937 \n",
      "Average KL Loss: 60.734 \n",
      "Average Regression Loss: 0.733 \n",
      "Average Shared Loss: 1.094 \n",
      "Average Independence Loss: 1.720 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 7246.029 \n",
      "Average KL Loss: 60.682 \n",
      "Average Regression Loss: 0.770 \n",
      "Average Shared Loss: 1.092 \n",
      "Average Independence Loss: 1.695 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 7215.271 \n",
      "Average KL Loss: 60.615 \n",
      "Average Regression Loss: 0.675 \n",
      "Average Shared Loss: 1.090 \n",
      "Average Independence Loss: 1.648 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 7251.777 \n",
      "Average KL Loss: 60.560 \n",
      "Average Regression Loss: 0.723 \n",
      "Average Shared Loss: 1.089 \n",
      "Average Independence Loss: 1.634 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 7219.269 \n",
      "Average KL Loss: 60.512 \n",
      "Average Regression Loss: 0.775 \n",
      "Average Shared Loss: 1.087 \n",
      "Average Independence Loss: 1.654 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 7255.395 \n",
      "Average KL Loss: 60.470 \n",
      "Average Regression Loss: 0.777 \n",
      "Average Shared Loss: 1.085 \n",
      "Average Independence Loss: 1.627 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 7207.513 \n",
      "Average KL Loss: 60.439 \n",
      "Average Regression Loss: 0.735 \n",
      "Average Shared Loss: 1.085 \n",
      "Average Independence Loss: 1.575 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 7227.195 \n",
      "Average KL Loss: 60.382 \n",
      "Average Regression Loss: 0.848 \n",
      "Average Shared Loss: 1.084 \n",
      "Average Independence Loss: 1.579 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 7241.618 \n",
      "Average KL Loss: 60.335 \n",
      "Average Regression Loss: 0.718 \n",
      "Average Shared Loss: 1.081 \n",
      "Average Independence Loss: 1.631 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 7226.424 \n",
      "Average KL Loss: 60.285 \n",
      "Average Regression Loss: 0.778 \n",
      "Average Shared Loss: 1.083 \n",
      "Average Independence Loss: 1.657 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 7224.382 \n",
      "Average KL Loss: 60.240 \n",
      "Average Regression Loss: 0.775 \n",
      "Average Shared Loss: 1.081 \n",
      "Average Independence Loss: 1.656 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 7212.487 \n",
      "Average KL Loss: 60.183 \n",
      "Average Regression Loss: 0.742 \n",
      "Average Shared Loss: 1.082 \n",
      "Average Independence Loss: 1.671 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 7193.080 \n",
      "Average KL Loss: 60.122 \n",
      "Average Regression Loss: 0.719 \n",
      "Average Shared Loss: 1.081 \n",
      "Average Independence Loss: 1.656 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 7182.327 \n",
      "Average KL Loss: 60.048 \n",
      "Average Regression Loss: 0.703 \n",
      "Average Shared Loss: 1.074 \n",
      "Average Independence Loss: 1.623 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 7200.932 \n",
      "Average KL Loss: 59.969 \n",
      "Average Regression Loss: 0.747 \n",
      "Average Shared Loss: 1.072 \n",
      "Average Independence Loss: 1.603 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 7200.908 \n",
      "Average KL Loss: 59.904 \n",
      "Average Regression Loss: 0.722 \n",
      "Average Shared Loss: 1.070 \n",
      "Average Independence Loss: 1.639 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 7159.266 \n",
      "Average KL Loss: 59.840 \n",
      "Average Regression Loss: 0.673 \n",
      "Average Shared Loss: 1.067 \n",
      "Average Independence Loss: 1.619 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 7206.629 \n",
      "Average KL Loss: 59.773 \n",
      "Average Regression Loss: 0.662 \n",
      "Average Shared Loss: 1.067 \n",
      "Average Independence Loss: 1.639 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 7203.113 \n",
      "Average KL Loss: 59.707 \n",
      "Average Regression Loss: 0.678 \n",
      "Average Shared Loss: 1.067 \n",
      "Average Independence Loss: 1.647 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 7183.566 \n",
      "Average KL Loss: 59.643 \n",
      "Average Regression Loss: 0.731 \n",
      "Average Shared Loss: 1.066 \n",
      "Average Independence Loss: 1.637 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 7184.893 \n",
      "Average KL Loss: 59.590 \n",
      "Average Regression Loss: 0.701 \n",
      "Average Shared Loss: 1.064 \n",
      "Average Independence Loss: 1.614 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 7188.109 \n",
      "Average KL Loss: 59.541 \n",
      "Average Regression Loss: 0.732 \n",
      "Average Shared Loss: 1.058 \n",
      "Average Independence Loss: 1.651 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 7188.254 \n",
      "Average KL Loss: 59.486 \n",
      "Average Regression Loss: 0.760 \n",
      "Average Shared Loss: 1.057 \n",
      "Average Independence Loss: 1.645 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 7165.948 \n",
      "Average KL Loss: 59.414 \n",
      "Average Regression Loss: 0.758 \n",
      "Average Shared Loss: 1.056 \n",
      "Average Independence Loss: 1.650 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 7166.186 \n",
      "Average KL Loss: 59.342 \n",
      "Average Regression Loss: 0.799 \n",
      "Average Shared Loss: 1.056 \n",
      "Average Independence Loss: 1.669 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 7174.324 \n",
      "Average KL Loss: 59.296 \n",
      "Average Regression Loss: 0.709 \n",
      "Average Shared Loss: 1.053 \n",
      "Average Independence Loss: 1.677 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 7192.642 \n",
      "Average KL Loss: 59.271 \n",
      "Average Regression Loss: 0.846 \n",
      "Average Shared Loss: 1.054 \n",
      "Average Independence Loss: 1.633 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 7182.662 \n",
      "Average KL Loss: 59.251 \n",
      "Average Regression Loss: 0.762 \n",
      "Average Shared Loss: 1.051 \n",
      "Average Independence Loss: 1.647 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 7193.177 \n",
      "Average KL Loss: 59.234 \n",
      "Average Regression Loss: 0.723 \n",
      "Average Shared Loss: 1.050 \n",
      "Average Independence Loss: 1.664 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 7179.263 \n",
      "Average KL Loss: 59.190 \n",
      "Average Regression Loss: 0.713 \n",
      "Average Shared Loss: 1.049 \n",
      "Average Independence Loss: 1.647 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 7225.851 \n",
      "Average KL Loss: 59.152 \n",
      "Average Regression Loss: 0.747 \n",
      "Average Shared Loss: 1.049 \n",
      "Average Independence Loss: 1.311 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 7267.568 \n",
      "Average KL Loss: 59.131 \n",
      "Average Regression Loss: 0.670 \n",
      "Average Shared Loss: 1.048 \n",
      "Average Independence Loss: 0.433 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 7205.099 \n",
      "Average KL Loss: 59.126 \n",
      "Average Regression Loss: 0.722 \n",
      "Average Shared Loss: 1.049 \n",
      "Average Independence Loss: 0.120 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 7214.682 \n",
      "Average KL Loss: 59.125 \n",
      "Average Regression Loss: 0.633 \n",
      "Average Shared Loss: 1.049 \n",
      "Average Independence Loss: 0.044 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 7226.284 \n",
      "Average KL Loss: 59.125 \n",
      "Average Regression Loss: 0.697 \n",
      "Average Shared Loss: 1.044 \n",
      "Average Independence Loss: 0.018 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 7230.501 \n",
      "Average KL Loss: 59.124 \n",
      "Average Regression Loss: 0.590 \n",
      "Average Shared Loss: 1.046 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 7254.589 \n",
      "Average KL Loss: 59.123 \n",
      "Average Regression Loss: 0.696 \n",
      "Average Shared Loss: 1.042 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 7282.413 \n",
      "Average KL Loss: 59.123 \n",
      "Average Regression Loss: 0.653 \n",
      "Average Shared Loss: 1.044 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 7244.096 \n",
      "Average KL Loss: 59.122 \n",
      "Average Regression Loss: 0.719 \n",
      "Average Shared Loss: 1.044 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 7232.031 \n",
      "Average KL Loss: 59.121 \n",
      "Average Regression Loss: 0.586 \n",
      "Average Shared Loss: 1.044 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 7230.369 \n",
      "Average KL Loss: 59.120 \n",
      "Average Regression Loss: 0.637 \n",
      "Average Shared Loss: 1.043 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 7248.607 \n",
      "Average KL Loss: 59.119 \n",
      "Average Regression Loss: 0.696 \n",
      "Average Shared Loss: 1.042 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 7236.574 \n",
      "Average KL Loss: 59.118 \n",
      "Average Regression Loss: 0.637 \n",
      "Average Shared Loss: 1.040 \n",
      "Average Independence Loss: 0.007 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 7234.202 \n",
      "Average KL Loss: 59.118 \n",
      "Average Regression Loss: 0.610 \n",
      "Average Shared Loss: 1.043 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 7249.294 \n",
      "Average KL Loss: 59.117 \n",
      "Average Regression Loss: 0.611 \n",
      "Average Shared Loss: 1.041 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 7229.463 \n",
      "Average KL Loss: 59.116 \n",
      "Average Regression Loss: 0.581 \n",
      "Average Shared Loss: 1.038 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 7247.769 \n",
      "Average KL Loss: 59.115 \n",
      "Average Regression Loss: 0.594 \n",
      "Average Shared Loss: 1.038 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 7279.870 \n",
      "Average KL Loss: 59.114 \n",
      "Average Regression Loss: 0.551 \n",
      "Average Shared Loss: 1.038 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 7243.253 \n",
      "Average KL Loss: 59.114 \n",
      "Average Regression Loss: 0.618 \n",
      "Average Shared Loss: 1.036 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 7293.683 \n",
      "Average KL Loss: 59.113 \n",
      "Average Regression Loss: 0.582 \n",
      "Average Shared Loss: 1.035 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 7234.966 \n",
      "Average KL Loss: 59.112 \n",
      "Average Regression Loss: 0.601 \n",
      "Average Shared Loss: 1.034 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 7235.022 \n",
      "Average KL Loss: 59.111 \n",
      "Average Regression Loss: 0.620 \n",
      "Average Shared Loss: 1.033 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 7260.281 \n",
      "Average KL Loss: 59.110 \n",
      "Average Regression Loss: 0.581 \n",
      "Average Shared Loss: 1.032 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 7239.123 \n",
      "Average KL Loss: 59.110 \n",
      "Average Regression Loss: 0.589 \n",
      "Average Shared Loss: 1.031 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 7251.039 \n",
      "Average KL Loss: 59.109 \n",
      "Average Regression Loss: 0.573 \n",
      "Average Shared Loss: 1.030 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 7238.047 \n",
      "Average KL Loss: 59.108 \n",
      "Average Regression Loss: 0.542 \n",
      "Average Shared Loss: 1.030 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 7232.614 \n",
      "Average KL Loss: 59.107 \n",
      "Average Regression Loss: 0.518 \n",
      "Average Shared Loss: 1.029 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 7253.086 \n",
      "Average KL Loss: 59.106 \n",
      "Average Regression Loss: 0.572 \n",
      "Average Shared Loss: 1.028 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 7271.419 \n",
      "Average KL Loss: 59.105 \n",
      "Average Regression Loss: 0.612 \n",
      "Average Shared Loss: 1.030 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 7233.399 \n",
      "Average KL Loss: 59.105 \n",
      "Average Regression Loss: 0.529 \n",
      "Average Shared Loss: 1.028 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 7224.189 \n",
      "Average KL Loss: 59.103 \n",
      "Average Regression Loss: 0.570 \n",
      "Average Shared Loss: 1.026 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 7247.116 \n",
      "Average KL Loss: 59.102 \n",
      "Average Regression Loss: 0.577 \n",
      "Average Shared Loss: 1.028 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 7223.940 \n",
      "Average KL Loss: 59.102 \n",
      "Average Regression Loss: 0.511 \n",
      "Average Shared Loss: 1.027 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 7222.045 \n",
      "Average KL Loss: 59.100 \n",
      "Average Regression Loss: 0.509 \n",
      "Average Shared Loss: 1.024 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 7226.913 \n",
      "Average KL Loss: 59.099 \n",
      "Average Regression Loss: 0.498 \n",
      "Average Shared Loss: 1.026 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 7227.818 \n",
      "Average KL Loss: 59.098 \n",
      "Average Regression Loss: 0.511 \n",
      "Average Shared Loss: 1.025 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 7223.734 \n",
      "Average KL Loss: 59.098 \n",
      "Average Regression Loss: 0.506 \n",
      "Average Shared Loss: 1.021 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 7290.976 \n",
      "Average KL Loss: 59.097 \n",
      "Average Regression Loss: 0.543 \n",
      "Average Shared Loss: 1.022 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 7283.437 \n",
      "Average KL Loss: 59.096 \n",
      "Average Regression Loss: 0.453 \n",
      "Average Shared Loss: 1.022 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 7210.559 \n",
      "Average KL Loss: 59.095 \n",
      "Average Regression Loss: 0.598 \n",
      "Average Shared Loss: 1.021 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 7203.041 \n",
      "Average KL Loss: 59.094 \n",
      "Average Regression Loss: 0.520 \n",
      "Average Shared Loss: 1.022 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 7236.536 \n",
      "Average KL Loss: 59.093 \n",
      "Average Regression Loss: 0.497 \n",
      "Average Shared Loss: 1.019 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 7205.715 \n",
      "Average KL Loss: 59.092 \n",
      "Average Regression Loss: 0.522 \n",
      "Average Shared Loss: 1.016 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 7245.577 \n",
      "Average KL Loss: 59.092 \n",
      "Average Regression Loss: 0.490 \n",
      "Average Shared Loss: 1.021 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 7288.115 \n",
      "Average KL Loss: 59.091 \n",
      "Average Regression Loss: 0.624 \n",
      "Average Shared Loss: 1.016 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 7272.014 \n",
      "Average KL Loss: 59.090 \n",
      "Average Regression Loss: 0.458 \n",
      "Average Shared Loss: 1.015 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 7264.351 \n",
      "Average KL Loss: 59.089 \n",
      "Average Regression Loss: 0.554 \n",
      "Average Shared Loss: 1.014 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 7253.049 \n",
      "Average KL Loss: 59.088 \n",
      "Average Regression Loss: 0.533 \n",
      "Average Shared Loss: 1.013 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 7239.090 \n",
      "Average KL Loss: 59.088 \n",
      "Average Regression Loss: 0.470 \n",
      "Average Shared Loss: 1.012 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 7262.327 \n",
      "Average KL Loss: 59.087 \n",
      "Average Regression Loss: 0.494 \n",
      "Average Shared Loss: 1.012 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "Average DNA Recon Loss: 25089.760 \n",
      "Average Gene Recon Loss: 337.261 \n",
      "Average RPPA Recon Loss: 142.370 \n",
      "Average DNA KL Loss: 167.248 \n",
      "Average Gene KL Loss: 0.486 \n",
      "Average RPPA KL Loss: 1.349 \n",
      "Average Regressor Loss: 0.003 \n",
      "Average RMSE Loss: 0.052 \n",
      "Average Shared MSE Loss: 0.004 \n",
      "Average Shared RMSE Loss: 0.060 \n",
      "Average R2: 0.920 \n",
      "Average Indepence Loss: 0.005 \n",
      "\n",
      "Fold 10\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 7330.338 \n",
      "Average KL Loss: 59.095 \n",
      "Average Regression Loss: 0.679 \n",
      "Average Shared Loss: 1.015 \n",
      "Average Independence Loss: 0.246 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 7344.847 \n",
      "Average KL Loss: 59.178 \n",
      "Average Regression Loss: 0.784 \n",
      "Average Shared Loss: 1.025 \n",
      "Average Independence Loss: 1.018 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 7329.864 \n",
      "Average KL Loss: 59.296 \n",
      "Average Regression Loss: 0.693 \n",
      "Average Shared Loss: 1.031 \n",
      "Average Independence Loss: 1.436 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 7282.265 \n",
      "Average KL Loss: 59.380 \n",
      "Average Regression Loss: 0.771 \n",
      "Average Shared Loss: 1.034 \n",
      "Average Independence Loss: 1.530 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 7255.862 \n",
      "Average KL Loss: 59.418 \n",
      "Average Regression Loss: 0.650 \n",
      "Average Shared Loss: 1.034 \n",
      "Average Independence Loss: 1.560 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 7296.020 \n",
      "Average KL Loss: 59.430 \n",
      "Average Regression Loss: 0.788 \n",
      "Average Shared Loss: 1.035 \n",
      "Average Independence Loss: 1.551 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 7273.883 \n",
      "Average KL Loss: 59.457 \n",
      "Average Regression Loss: 0.691 \n",
      "Average Shared Loss: 1.033 \n",
      "Average Independence Loss: 1.537 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 7253.561 \n",
      "Average KL Loss: 59.469 \n",
      "Average Regression Loss: 0.872 \n",
      "Average Shared Loss: 1.032 \n",
      "Average Independence Loss: 1.547 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 7257.750 \n",
      "Average KL Loss: 59.468 \n",
      "Average Regression Loss: 0.644 \n",
      "Average Shared Loss: 1.034 \n",
      "Average Independence Loss: 1.547 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 7215.704 \n",
      "Average KL Loss: 59.470 \n",
      "Average Regression Loss: 0.692 \n",
      "Average Shared Loss: 1.034 \n",
      "Average Independence Loss: 1.554 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 7230.938 \n",
      "Average KL Loss: 59.462 \n",
      "Average Regression Loss: 0.689 \n",
      "Average Shared Loss: 1.035 \n",
      "Average Independence Loss: 1.606 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 7210.374 \n",
      "Average KL Loss: 59.430 \n",
      "Average Regression Loss: 0.670 \n",
      "Average Shared Loss: 1.037 \n",
      "Average Independence Loss: 1.579 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 7238.490 \n",
      "Average KL Loss: 59.399 \n",
      "Average Regression Loss: 0.720 \n",
      "Average Shared Loss: 1.036 \n",
      "Average Independence Loss: 1.486 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 7228.071 \n",
      "Average KL Loss: 59.353 \n",
      "Average Regression Loss: 0.738 \n",
      "Average Shared Loss: 1.032 \n",
      "Average Independence Loss: 1.429 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 7235.623 \n",
      "Average KL Loss: 59.331 \n",
      "Average Regression Loss: 0.642 \n",
      "Average Shared Loss: 1.029 \n",
      "Average Independence Loss: 1.431 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 7250.008 \n",
      "Average KL Loss: 59.325 \n",
      "Average Regression Loss: 0.753 \n",
      "Average Shared Loss: 1.031 \n",
      "Average Independence Loss: 1.494 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 7211.584 \n",
      "Average KL Loss: 59.321 \n",
      "Average Regression Loss: 0.753 \n",
      "Average Shared Loss: 1.033 \n",
      "Average Independence Loss: 1.560 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 7197.572 \n",
      "Average KL Loss: 59.313 \n",
      "Average Regression Loss: 0.690 \n",
      "Average Shared Loss: 1.033 \n",
      "Average Independence Loss: 1.579 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 7196.618 \n",
      "Average KL Loss: 59.286 \n",
      "Average Regression Loss: 0.680 \n",
      "Average Shared Loss: 1.029 \n",
      "Average Independence Loss: 1.543 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 7203.585 \n",
      "Average KL Loss: 59.253 \n",
      "Average Regression Loss: 0.695 \n",
      "Average Shared Loss: 1.031 \n",
      "Average Independence Loss: 1.514 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 7189.726 \n",
      "Average KL Loss: 59.216 \n",
      "Average Regression Loss: 0.678 \n",
      "Average Shared Loss: 1.029 \n",
      "Average Independence Loss: 1.479 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 7217.066 \n",
      "Average KL Loss: 59.173 \n",
      "Average Regression Loss: 0.694 \n",
      "Average Shared Loss: 1.025 \n",
      "Average Independence Loss: 1.441 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 7193.380 \n",
      "Average KL Loss: 59.141 \n",
      "Average Regression Loss: 0.643 \n",
      "Average Shared Loss: 1.020 \n",
      "Average Independence Loss: 1.419 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 7162.617 \n",
      "Average KL Loss: 59.109 \n",
      "Average Regression Loss: 0.661 \n",
      "Average Shared Loss: 1.020 \n",
      "Average Independence Loss: 1.420 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 7166.890 \n",
      "Average KL Loss: 59.048 \n",
      "Average Regression Loss: 0.625 \n",
      "Average Shared Loss: 1.018 \n",
      "Average Independence Loss: 1.429 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 7193.989 \n",
      "Average KL Loss: 58.989 \n",
      "Average Regression Loss: 0.687 \n",
      "Average Shared Loss: 1.017 \n",
      "Average Independence Loss: 1.395 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 7182.553 \n",
      "Average KL Loss: 58.950 \n",
      "Average Regression Loss: 0.689 \n",
      "Average Shared Loss: 1.020 \n",
      "Average Independence Loss: 1.423 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 7163.217 \n",
      "Average KL Loss: 58.896 \n",
      "Average Regression Loss: 0.663 \n",
      "Average Shared Loss: 1.017 \n",
      "Average Independence Loss: 1.424 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 7198.926 \n",
      "Average KL Loss: 58.839 \n",
      "Average Regression Loss: 0.685 \n",
      "Average Shared Loss: 1.018 \n",
      "Average Independence Loss: 1.459 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 7172.917 \n",
      "Average KL Loss: 58.798 \n",
      "Average Regression Loss: 0.650 \n",
      "Average Shared Loss: 1.017 \n",
      "Average Independence Loss: 1.491 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 7225.996 \n",
      "Average KL Loss: 58.755 \n",
      "Average Regression Loss: 0.728 \n",
      "Average Shared Loss: 1.016 \n",
      "Average Independence Loss: 1.482 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 7224.299 \n",
      "Average KL Loss: 58.727 \n",
      "Average Regression Loss: 0.841 \n",
      "Average Shared Loss: 1.016 \n",
      "Average Independence Loss: 1.462 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 7160.672 \n",
      "Average KL Loss: 58.699 \n",
      "Average Regression Loss: 0.621 \n",
      "Average Shared Loss: 1.013 \n",
      "Average Independence Loss: 1.413 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 7191.016 \n",
      "Average KL Loss: 58.642 \n",
      "Average Regression Loss: 0.710 \n",
      "Average Shared Loss: 1.008 \n",
      "Average Independence Loss: 1.412 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 7200.288 \n",
      "Average KL Loss: 58.591 \n",
      "Average Regression Loss: 0.732 \n",
      "Average Shared Loss: 1.010 \n",
      "Average Independence Loss: 1.424 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 7169.444 \n",
      "Average KL Loss: 58.565 \n",
      "Average Regression Loss: 0.672 \n",
      "Average Shared Loss: 1.007 \n",
      "Average Independence Loss: 1.444 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 7160.641 \n",
      "Average KL Loss: 58.528 \n",
      "Average Regression Loss: 0.690 \n",
      "Average Shared Loss: 1.005 \n",
      "Average Independence Loss: 1.453 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 7174.700 \n",
      "Average KL Loss: 58.478 \n",
      "Average Regression Loss: 0.711 \n",
      "Average Shared Loss: 1.004 \n",
      "Average Independence Loss: 1.433 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 7188.226 \n",
      "Average KL Loss: 58.435 \n",
      "Average Regression Loss: 0.703 \n",
      "Average Shared Loss: 1.004 \n",
      "Average Independence Loss: 1.423 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 7175.765 \n",
      "Average KL Loss: 58.411 \n",
      "Average Regression Loss: 0.682 \n",
      "Average Shared Loss: 1.001 \n",
      "Average Independence Loss: 1.445 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 7176.590 \n",
      "Average KL Loss: 58.371 \n",
      "Average Regression Loss: 0.726 \n",
      "Average Shared Loss: 1.006 \n",
      "Average Independence Loss: 1.433 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 7171.545 \n",
      "Average KL Loss: 58.325 \n",
      "Average Regression Loss: 0.729 \n",
      "Average Shared Loss: 1.004 \n",
      "Average Independence Loss: 1.396 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 7148.419 \n",
      "Average KL Loss: 58.275 \n",
      "Average Regression Loss: 0.674 \n",
      "Average Shared Loss: 1.001 \n",
      "Average Independence Loss: 1.392 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 7215.839 \n",
      "Average KL Loss: 58.233 \n",
      "Average Regression Loss: 0.713 \n",
      "Average Shared Loss: 0.999 \n",
      "Average Independence Loss: 1.391 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 7159.680 \n",
      "Average KL Loss: 58.211 \n",
      "Average Regression Loss: 0.703 \n",
      "Average Shared Loss: 0.997 \n",
      "Average Independence Loss: 1.410 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 7168.084 \n",
      "Average KL Loss: 58.184 \n",
      "Average Regression Loss: 0.627 \n",
      "Average Shared Loss: 0.997 \n",
      "Average Independence Loss: 1.421 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 7156.811 \n",
      "Average KL Loss: 58.134 \n",
      "Average Regression Loss: 0.768 \n",
      "Average Shared Loss: 0.996 \n",
      "Average Independence Loss: 1.395 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 7150.083 \n",
      "Average KL Loss: 58.083 \n",
      "Average Regression Loss: 0.695 \n",
      "Average Shared Loss: 0.994 \n",
      "Average Independence Loss: 1.395 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 7144.213 \n",
      "Average KL Loss: 58.026 \n",
      "Average Regression Loss: 0.747 \n",
      "Average Shared Loss: 0.992 \n",
      "Average Independence Loss: 1.370 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 7147.034 \n",
      "Average KL Loss: 57.974 \n",
      "Average Regression Loss: 0.654 \n",
      "Average Shared Loss: 0.990 \n",
      "Average Independence Loss: 1.380 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 7162.034 \n",
      "Average KL Loss: 57.930 \n",
      "Average Regression Loss: 0.614 \n",
      "Average Shared Loss: 0.989 \n",
      "Average Independence Loss: 1.082 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 7172.511 \n",
      "Average KL Loss: 57.908 \n",
      "Average Regression Loss: 0.672 \n",
      "Average Shared Loss: 0.985 \n",
      "Average Independence Loss: 0.316 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 7202.240 \n",
      "Average KL Loss: 57.904 \n",
      "Average Regression Loss: 0.666 \n",
      "Average Shared Loss: 0.983 \n",
      "Average Independence Loss: 0.102 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 7185.462 \n",
      "Average KL Loss: 57.903 \n",
      "Average Regression Loss: 0.623 \n",
      "Average Shared Loss: 0.982 \n",
      "Average Independence Loss: 0.036 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 7183.408 \n",
      "Average KL Loss: 57.902 \n",
      "Average Regression Loss: 0.657 \n",
      "Average Shared Loss: 0.984 \n",
      "Average Independence Loss: 0.015 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 7190.850 \n",
      "Average KL Loss: 57.902 \n",
      "Average Regression Loss: 0.683 \n",
      "Average Shared Loss: 0.984 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 7244.980 \n",
      "Average KL Loss: 57.901 \n",
      "Average Regression Loss: 0.681 \n",
      "Average Shared Loss: 0.980 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 7214.947 \n",
      "Average KL Loss: 57.900 \n",
      "Average Regression Loss: 0.606 \n",
      "Average Shared Loss: 0.980 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 7215.179 \n",
      "Average KL Loss: 57.899 \n",
      "Average Regression Loss: 0.571 \n",
      "Average Shared Loss: 0.980 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 7233.481 \n",
      "Average KL Loss: 57.898 \n",
      "Average Regression Loss: 0.527 \n",
      "Average Shared Loss: 0.981 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 7189.510 \n",
      "Average KL Loss: 57.898 \n",
      "Average Regression Loss: 0.538 \n",
      "Average Shared Loss: 0.979 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 7198.788 \n",
      "Average KL Loss: 57.897 \n",
      "Average Regression Loss: 0.565 \n",
      "Average Shared Loss: 0.979 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 7207.832 \n",
      "Average KL Loss: 57.896 \n",
      "Average Regression Loss: 0.580 \n",
      "Average Shared Loss: 0.974 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 7223.216 \n",
      "Average KL Loss: 57.896 \n",
      "Average Regression Loss: 0.536 \n",
      "Average Shared Loss: 0.976 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 7207.091 \n",
      "Average KL Loss: 57.895 \n",
      "Average Regression Loss: 0.478 \n",
      "Average Shared Loss: 0.976 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 7191.045 \n",
      "Average KL Loss: 57.894 \n",
      "Average Regression Loss: 0.500 \n",
      "Average Shared Loss: 0.972 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 7171.397 \n",
      "Average KL Loss: 57.894 \n",
      "Average Regression Loss: 0.533 \n",
      "Average Shared Loss: 0.973 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 7188.530 \n",
      "Average KL Loss: 57.893 \n",
      "Average Regression Loss: 0.571 \n",
      "Average Shared Loss: 0.971 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 7166.412 \n",
      "Average KL Loss: 57.892 \n",
      "Average Regression Loss: 0.438 \n",
      "Average Shared Loss: 0.971 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 7187.424 \n",
      "Average KL Loss: 57.891 \n",
      "Average Regression Loss: 0.514 \n",
      "Average Shared Loss: 0.970 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 7201.564 \n",
      "Average KL Loss: 57.891 \n",
      "Average Regression Loss: 0.518 \n",
      "Average Shared Loss: 0.971 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 7199.506 \n",
      "Average KL Loss: 57.890 \n",
      "Average Regression Loss: 0.474 \n",
      "Average Shared Loss: 0.970 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 7174.631 \n",
      "Average KL Loss: 57.889 \n",
      "Average Regression Loss: 0.489 \n",
      "Average Shared Loss: 0.969 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 7190.147 \n",
      "Average KL Loss: 57.888 \n",
      "Average Regression Loss: 0.573 \n",
      "Average Shared Loss: 0.969 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 7177.291 \n",
      "Average KL Loss: 57.888 \n",
      "Average Regression Loss: 0.457 \n",
      "Average Shared Loss: 0.968 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 7194.009 \n",
      "Average KL Loss: 57.887 \n",
      "Average Regression Loss: 0.492 \n",
      "Average Shared Loss: 0.967 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 7187.511 \n",
      "Average KL Loss: 57.886 \n",
      "Average Regression Loss: 0.441 \n",
      "Average Shared Loss: 0.967 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 7182.457 \n",
      "Average KL Loss: 57.885 \n",
      "Average Regression Loss: 0.540 \n",
      "Average Shared Loss: 0.962 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 7161.370 \n",
      "Average KL Loss: 57.884 \n",
      "Average Regression Loss: 0.535 \n",
      "Average Shared Loss: 0.963 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 7196.544 \n",
      "Average KL Loss: 57.883 \n",
      "Average Regression Loss: 0.484 \n",
      "Average Shared Loss: 0.964 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 7210.019 \n",
      "Average KL Loss: 57.882 \n",
      "Average Regression Loss: 0.497 \n",
      "Average Shared Loss: 0.963 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 7196.637 \n",
      "Average KL Loss: 57.881 \n",
      "Average Regression Loss: 0.539 \n",
      "Average Shared Loss: 0.963 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 7202.548 \n",
      "Average KL Loss: 57.880 \n",
      "Average Regression Loss: 0.458 \n",
      "Average Shared Loss: 0.961 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 7195.251 \n",
      "Average KL Loss: 57.879 \n",
      "Average Regression Loss: 0.496 \n",
      "Average Shared Loss: 0.959 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 7198.817 \n",
      "Average KL Loss: 57.878 \n",
      "Average Regression Loss: 0.522 \n",
      "Average Shared Loss: 0.960 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 7197.273 \n",
      "Average KL Loss: 57.878 \n",
      "Average Regression Loss: 0.506 \n",
      "Average Shared Loss: 0.958 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 7199.133 \n",
      "Average KL Loss: 57.877 \n",
      "Average Regression Loss: 0.489 \n",
      "Average Shared Loss: 0.958 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 7180.602 \n",
      "Average KL Loss: 57.876 \n",
      "Average Regression Loss: 0.513 \n",
      "Average Shared Loss: 0.956 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 7216.441 \n",
      "Average KL Loss: 57.876 \n",
      "Average Regression Loss: 0.554 \n",
      "Average Shared Loss: 0.954 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 7227.621 \n",
      "Average KL Loss: 57.875 \n",
      "Average Regression Loss: 0.484 \n",
      "Average Shared Loss: 0.954 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 7189.155 \n",
      "Average KL Loss: 57.875 \n",
      "Average Regression Loss: 0.490 \n",
      "Average Shared Loss: 0.955 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 7173.711 \n",
      "Average KL Loss: 57.874 \n",
      "Average Regression Loss: 0.479 \n",
      "Average Shared Loss: 0.954 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 7209.810 \n",
      "Average KL Loss: 57.873 \n",
      "Average Regression Loss: 0.465 \n",
      "Average Shared Loss: 0.952 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 7205.017 \n",
      "Average KL Loss: 57.873 \n",
      "Average Regression Loss: 0.539 \n",
      "Average Shared Loss: 0.952 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 7225.218 \n",
      "Average KL Loss: 57.873 \n",
      "Average Regression Loss: 0.487 \n",
      "Average Shared Loss: 0.953 \n",
      "Average Independence Loss: 0.006 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 7194.346 \n",
      "Average KL Loss: 57.872 \n",
      "Average Regression Loss: 0.535 \n",
      "Average Shared Loss: 0.952 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 7181.651 \n",
      "Average KL Loss: 57.871 \n",
      "Average Regression Loss: 0.561 \n",
      "Average Shared Loss: 0.951 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 7179.910 \n",
      "Average KL Loss: 57.870 \n",
      "Average Regression Loss: 0.451 \n",
      "Average Shared Loss: 0.949 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 7202.428 \n",
      "Average KL Loss: 57.869 \n",
      "Average Regression Loss: 0.481 \n",
      "Average Shared Loss: 0.946 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 7160.485 \n",
      "Average KL Loss: 57.868 \n",
      "Average Regression Loss: 0.492 \n",
      "Average Shared Loss: 0.949 \n",
      "Average Independence Loss: 0.005 \n",
      "\n",
      "Average DNA Recon Loss: 23847.195 \n",
      "Average Gene Recon Loss: 237.131 \n",
      "Average RPPA Recon Loss: 141.009 \n",
      "Average DNA KL Loss: 163.879 \n",
      "Average Gene KL Loss: 0.671 \n",
      "Average RPPA KL Loss: 1.221 \n",
      "Average Regressor Loss: 0.003 \n",
      "Average RMSE Loss: 0.051 \n",
      "Average Shared MSE Loss: 0.003 \n",
      "Average Shared RMSE Loss: 0.058 \n",
      "Average R2: 0.919 \n",
      "Average Indepence Loss: 0.005 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_train = {'Train Recon Loss': [], 'Train KL Loss': [], 'Train MSE Loss': [], 'Train Shared Loss': [], 'Train Independence Loss': []}\n",
    "history_test = {'Test DNA Recon Loss': [],'Test Gene Recon Loss': [],'Test RPPA Recon Loss': [], 'Test KL DNA Loss': [], 'Test KL Gene Loss': [],'Test KL RPPA Loss': [], 'Test MSE Loss': [], 'Test RMSE Loss': [], 'Test Shared Loss': [], 'Test Shared RMSE Loss': [],  'Test R2': [], 'Test Independence Loss': []}\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(full_dataset)))):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(full_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(full_dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "    print(\"Start Training (Unsupervised Phase)\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_recon_ave, train_kl_ave, train_reg_ave, train_shared_ave, train_ind_ave = train(model, train_loader, epoch, optimizer, w_recon_loss= 1, w_kl_loss= 1, w_reg_loss= 0, w_shared_loss= 0, w_ind_loss= 0)\n",
    "        history_train['Train Recon Loss'].append(train_recon_ave)\n",
    "        history_train['Train KL Loss'].append(train_kl_ave)\n",
    "        history_train['Train MSE Loss'].append(train_reg_ave)\n",
    "        history_train['Train Shared Loss'].append(train_shared_ave)\n",
    "        history_train['Train Independence Loss'].append(train_ind_ave)\n",
    "\n",
    "    print(\"Start Training (Supervised Phase)\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_recon_ave, train_kl_ave, train_reg_ave, train_shared_ave, train_ind_ave = train(model, train_loader, epoch, optimizer, w_recon_loss= 0, w_kl_loss= 0, w_reg_loss= 1, w_shared_loss= 1, w_ind_loss=1)\n",
    "        history_train['Train Recon Loss'].append(train_recon_ave)\n",
    "        history_train['Train KL Loss'].append(train_kl_ave)\n",
    "        history_train['Train MSE Loss'].append(train_reg_ave)\n",
    "        history_train['Train Shared Loss'].append(train_shared_ave)\n",
    "        history_train['Train Independence Loss'].append(train_ind_ave)\n",
    "\n",
    "    test_recon_dna_ave, test_recon_gene_ave, test_recon_rppa_ave, test_kl_dna_ave, test_kl_gene_ave, test_kl_rppa_ave, test_reg_ave, test_rmse_ave, test_shared_ave, test_shared_rmse_ave, test_r2_ave, test_ind_ave = test(test_loader, model)\n",
    "    history_test['Test DNA Recon Loss'].append(test_recon_dna_ave)\n",
    "    history_test['Test Gene Recon Loss'].append(test_recon_gene_ave)\n",
    "    history_test['Test RPPA Recon Loss'].append(test_recon_rppa_ave)\n",
    "    history_test['Test KL DNA Loss'].append(test_kl_dna_ave)\n",
    "    history_test['Test KL Gene Loss'].append(test_kl_gene_ave)\n",
    "    history_test['Test KL RPPA Loss'].append(test_kl_rppa_ave)\n",
    "    history_test['Test MSE Loss'].append(test_reg_ave)\n",
    "    history_test['Test RMSE Loss'].append(test_rmse_ave)\n",
    "    history_test['Test Shared Loss'].append(test_shared_ave)\n",
    "    history_test['Test Shared RMSE Loss'].append(test_shared_rmse_ave)\n",
    "    history_test['Test R2'].append(test_r2_ave)\n",
    "    history_test['Test Independence Loss'].append(test_ind_ave)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of model through 5 fold cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_dna_recon_loss = np.mean(history_test['Test DNA Recon Loss'])\n",
    "cv_gene_recon_loss = np.mean(history_test['Test Gene Recon Loss'])\n",
    "cv_rppa_recon_loss = np.mean(history_test['Test RPPA Recon Loss'])\n",
    "cv_dna_kl_loss = np.mean(history_test['Test KL DNA Loss'])\n",
    "cv_gene_kl_loss = np.mean(history_test['Test KL Gene Loss'])\n",
    "cv_rppa_kl_loss = np.mean(history_test['Test KL RPPA Loss'])\n",
    "cv_mse_loss = np.mean(history_test['Test MSE Loss'])\n",
    "cv_rmse_loss = np.mean(history_test['Test RMSE Loss'])\n",
    "cv_shared_loss = np.mean(history_test['Test Shared Loss'])\n",
    "cv_shared_rmse_loss = np.mean(history_test['Test Shared RMSE Loss'])\n",
    "cv_r2_loss = np.mean(history_test['Test R2'])\n",
    "cv_ind_loss = np.mean(history_test['Test Independence Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of 10 fold cross validation using common and unique factors:\n",
      "\n",
      "Average Testing DNA Recon Loss: 29330.9559 \n",
      "Average Testing Gene Recon Loss: 5271.5578 \n",
      "Average Testing RPPA Recon Loss: 143.1580 \n",
      " Average Testing DNA KL Loss: 169.9868 \n",
      " Average Testing Gene KL Loss: 11.2267 \n",
      "Average Testing RPPA KL Loss: 1.3940 \n",
      " Average Testing MSE Loss: 0.010 \n",
      " Average Testing RMSE Loss: 0.089 \n",
      " Average Testing Shared Loss: 0.005 \n",
      " Average Testing Shared RMSE Loss: 0.069 \n",
      " Average Testing R2 Loss: 0.709 \n",
      " Average Testing Independence Loss: 3.498 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Performance of {} fold cross validation using common and unique factors:'.format(k))\n",
    "print(\"\\nAverage Testing DNA Recon Loss: {:.4f} \\nAverage Testing Gene Recon Loss: {:.4f} \\nAverage Testing RPPA Recon Loss: {:.4f} \\n Average Testing DNA KL Loss: {:.4f} \\n Average Testing Gene KL Loss: {:.4f} \\nAverage Testing RPPA KL Loss: {:.4f} \\n Average Testing MSE Loss: {:.3f} \\n Average Testing RMSE Loss: {:.3f} \\n Average Testing Shared Loss: {:.3f} \\n Average Testing Shared RMSE Loss: {:.3f} \\n Average Testing R2 Loss: {:.3f} \\n Average Testing Independence Loss: {:.3f} \\n\".format(cv_dna_recon_loss, cv_gene_recon_loss,cv_rppa_recon_loss, cv_dna_kl_loss, cv_gene_kl_loss, cv_rppa_kl_loss, cv_mse_loss, cv_rmse_loss, cv_shared_loss,cv_shared_rmse_loss, cv_r2_loss, cv_ind_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
