{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mdzak\\anaconda3\\envs\\fyp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e57d471090>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import matplotlib\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "matplotlib.style.use('ggplot')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightning.pytorch as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "\n",
    "torch.manual_seed(33)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CELL_LINE_NAME</th>\n",
       "      <th>1:134999</th>\n",
       "      <th>1:135191;1:135218</th>\n",
       "      <th>1:135203;1:135208</th>\n",
       "      <th>1:713376;1:713388;1:713400;1:713448;1:713450;1:713454</th>\n",
       "      <th>1:713901;1:713921;1:714178;1:714182;1:714199;1:714254;1:714258;1:714261;1:714264;1:714277;1:714278;1:714293;1:714301;1:714491;1:714511;1:714566;1:714584</th>\n",
       "      <th>1:715390;1:715392;1:715405;1:715415</th>\n",
       "      <th>1:804993;1:804999;1:805282;1:805284;1:805290;1:805327;1:805338;1:805341;1:805352;1:805445;1:805450;1:805467;1:805468</th>\n",
       "      <th>1:805474;1:805477;1:805479</th>\n",
       "      <th>1:805484;1:805486</th>\n",
       "      <th>...</th>\n",
       "      <th>Dactinomycin</th>\n",
       "      <th>Daporinad</th>\n",
       "      <th>Dasatinib</th>\n",
       "      <th>Rapamycin</th>\n",
       "      <th>Romidepsin</th>\n",
       "      <th>SN-38</th>\n",
       "      <th>Temsirolimus</th>\n",
       "      <th>Trametinib</th>\n",
       "      <th>Vinblastine</th>\n",
       "      <th>Vinorelbine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22rv1</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.41803</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.23610</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.840963</td>\n",
       "      <td>-3.112784</td>\n",
       "      <td>4.203067</td>\n",
       "      <td>-4.382138</td>\n",
       "      <td>-4.917572</td>\n",
       "      <td>-4.972312</td>\n",
       "      <td>-0.974396</td>\n",
       "      <td>1.325603</td>\n",
       "      <td>-4.384381</td>\n",
       "      <td>-3.401996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2313287</td>\n",
       "      <td>0.8897</td>\n",
       "      <td>0.80360</td>\n",
       "      <td>0.99105</td>\n",
       "      <td>0.65648</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.24032</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.584971</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>1.797167</td>\n",
       "      <td>-3.486065</td>\n",
       "      <td>-6.017003</td>\n",
       "      <td>-4.132899</td>\n",
       "      <td>3.279539</td>\n",
       "      <td>-0.040150</td>\n",
       "      <td>-4.849422</td>\n",
       "      <td>-5.303091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42mgba</td>\n",
       "      <td>0.9222</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.49984</td>\n",
       "      <td>0.00897</td>\n",
       "      <td>0.95695</td>\n",
       "      <td>0.02262</td>\n",
       "      <td>0.12001</td>\n",
       "      <td>0.1765</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.240733</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>-2.233603</td>\n",
       "      <td>-3.671158</td>\n",
       "      <td>-6.415517</td>\n",
       "      <td>-4.737156</td>\n",
       "      <td>-0.074308</td>\n",
       "      <td>1.621553</td>\n",
       "      <td>-4.784344</td>\n",
       "      <td>-3.927690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5637</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.95915</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.75370</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03540</td>\n",
       "      <td>0.07694</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.321517</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>-1.829021</td>\n",
       "      <td>-0.116397</td>\n",
       "      <td>-5.559258</td>\n",
       "      <td>-4.537337</td>\n",
       "      <td>1.818908</td>\n",
       "      <td>-0.275098</td>\n",
       "      <td>-3.494500</td>\n",
       "      <td>-5.252216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>639v</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.70308</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.16765</td>\n",
       "      <td>0.05868</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.725271</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>1.391965</td>\n",
       "      <td>-1.822279</td>\n",
       "      <td>-4.955669</td>\n",
       "      <td>-6.639479</td>\n",
       "      <td>1.248546</td>\n",
       "      <td>1.437469</td>\n",
       "      <td>-3.986408</td>\n",
       "      <td>-3.138535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>wsudlcl2</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>0.76473</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.70492</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.72915</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.120747</td>\n",
       "      <td>-6.196767</td>\n",
       "      <td>-1.463565</td>\n",
       "      <td>-5.903482</td>\n",
       "      <td>-5.240336</td>\n",
       "      <td>-4.959568</td>\n",
       "      <td>-5.974346</td>\n",
       "      <td>0.641326</td>\n",
       "      <td>-7.103903</td>\n",
       "      <td>-6.196508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>yapc</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>0.98215</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.96413</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.86138</td>\n",
       "      <td>0.00357</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.932980</td>\n",
       "      <td>-5.076682</td>\n",
       "      <td>1.407784</td>\n",
       "      <td>1.214362</td>\n",
       "      <td>-3.897758</td>\n",
       "      <td>5.073883</td>\n",
       "      <td>1.195720</td>\n",
       "      <td>3.598733</td>\n",
       "      <td>-0.095525</td>\n",
       "      <td>2.307072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>yh13</td>\n",
       "      <td>0.9211</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.97220</td>\n",
       "      <td>0.59999</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.94020</td>\n",
       "      <td>0.01222</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.595530</td>\n",
       "      <td>-3.469021</td>\n",
       "      <td>-0.787238</td>\n",
       "      <td>-2.743620</td>\n",
       "      <td>-5.957548</td>\n",
       "      <td>-3.526360</td>\n",
       "      <td>-1.963500</td>\n",
       "      <td>1.153223</td>\n",
       "      <td>-5.412392</td>\n",
       "      <td>-4.207989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>ykg1</td>\n",
       "      <td>0.8617</td>\n",
       "      <td>0.92310</td>\n",
       "      <td>0.93395</td>\n",
       "      <td>0.82007</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.99590</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.262451</td>\n",
       "      <td>3.115970</td>\n",
       "      <td>0.597641</td>\n",
       "      <td>-0.763934</td>\n",
       "      <td>-5.240336</td>\n",
       "      <td>-3.225249</td>\n",
       "      <td>2.191202</td>\n",
       "      <td>0.407173</td>\n",
       "      <td>-3.494500</td>\n",
       "      <td>-2.601804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>zr7530</td>\n",
       "      <td>0.9091</td>\n",
       "      <td>0.98385</td>\n",
       "      <td>0.96925</td>\n",
       "      <td>0.88890</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.65477</td>\n",
       "      <td>0.02129</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038498</td>\n",
       "      <td>1.574590</td>\n",
       "      <td>4.366079</td>\n",
       "      <td>-0.781208</td>\n",
       "      <td>-2.472102</td>\n",
       "      <td>5.054112</td>\n",
       "      <td>1.928232</td>\n",
       "      <td>5.681084</td>\n",
       "      <td>2.268159</td>\n",
       "      <td>1.959661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>543 rows Ã— 81048 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CELL_LINE_NAME  1:134999  1:135191;1:135218  1:135203;1:135208  \\\n",
       "0            22rv1    0.9821            1.00000            1.00000   \n",
       "1          2313287    0.8897            0.80360            0.99105   \n",
       "2           42mgba    0.9222            0.87500            1.00000   \n",
       "3             5637    1.0000            0.95915            1.00000   \n",
       "4             639v    1.0000            1.00000            1.00000   \n",
       "..             ...       ...                ...                ...   \n",
       "538       wsudlcl2    0.7586            0.76473            1.00000   \n",
       "539           yapc    0.9595            0.98215            1.00000   \n",
       "540           yh13    0.9211            1.00000            0.97220   \n",
       "541           ykg1    0.8617            0.92310            0.93395   \n",
       "542         zr7530    0.9091            0.98385            0.96925   \n",
       "\n",
       "     1:713376;1:713388;1:713400;1:713448;1:713450;1:713454  \\\n",
       "0                                              0.41803       \n",
       "1                                              0.65648       \n",
       "2                                              0.49984       \n",
       "3                                              0.75370       \n",
       "4                                              0.70308       \n",
       "..                                                 ...       \n",
       "538                                            0.70492       \n",
       "539                                            0.96413       \n",
       "540                                            0.59999       \n",
       "541                                            0.82007       \n",
       "542                                            0.88890       \n",
       "\n",
       "     1:713901;1:713921;1:714178;1:714182;1:714199;1:714254;1:714258;1:714261;1:714264;1:714277;1:714278;1:714293;1:714301;1:714491;1:714511;1:714566;1:714584  \\\n",
       "0                                              0.00000                                                                                                          \n",
       "1                                              0.00000                                                                                                          \n",
       "2                                              0.00897                                                                                                          \n",
       "3                                              0.00000                                                                                                          \n",
       "4                                              0.00000                                                                                                          \n",
       "..                                                 ...                                                                                                          \n",
       "538                                            0.00000                                                                                                          \n",
       "539                                            0.00000                                                                                                          \n",
       "540                                            0.00000                                                                                                          \n",
       "541                                            0.00000                                                                                                          \n",
       "542                                            0.00000                                                                                                          \n",
       "\n",
       "     1:715390;1:715392;1:715405;1:715415  \\\n",
       "0                                0.23610   \n",
       "1                                0.24032   \n",
       "2                                0.95695   \n",
       "3                                0.03540   \n",
       "4                                0.16765   \n",
       "..                                   ...   \n",
       "538                              0.72915   \n",
       "539                              0.86138   \n",
       "540                              0.94020   \n",
       "541                              0.99590   \n",
       "542                              0.65477   \n",
       "\n",
       "     1:804993;1:804999;1:805282;1:805284;1:805290;1:805327;1:805338;1:805341;1:805352;1:805445;1:805450;1:805467;1:805468  \\\n",
       "0                                              0.00000                                                                      \n",
       "1                                              0.00000                                                                      \n",
       "2                                              0.02262                                                                      \n",
       "3                                              0.07694                                                                      \n",
       "4                                              0.05868                                                                      \n",
       "..                                                 ...                                                                      \n",
       "538                                            0.00000                                                                      \n",
       "539                                            0.00357                                                                      \n",
       "540                                            0.01222                                                                      \n",
       "541                                            0.00000                                                                      \n",
       "542                                            0.02129                                                                      \n",
       "\n",
       "     1:805474;1:805477;1:805479  1:805484;1:805486  ...  Dactinomycin  \\\n",
       "0                       0.10000             0.0000  ...     -4.840963   \n",
       "1                       0.00000             0.0000  ...     -4.584971   \n",
       "2                       0.12001             0.1765  ...     -4.240733   \n",
       "3                       0.00000             0.0000  ...     -4.321517   \n",
       "4                       0.16667             0.3000  ...     -4.725271   \n",
       "..                          ...                ...  ...           ...   \n",
       "538                     0.00000             0.0000  ...     -5.120747   \n",
       "539                     0.00000             0.0000  ...     -1.932980   \n",
       "540                     0.00000             0.0000  ...     -3.595530   \n",
       "541                     0.00000             0.0000  ...     -5.262451   \n",
       "542                     0.00000             0.0000  ...     -0.038498   \n",
       "\n",
       "     Daporinad  Dasatinib  Rapamycin  Romidepsin     SN-38  Temsirolimus  \\\n",
       "0    -3.112784   4.203067  -4.382138   -4.917572 -4.972312     -0.974396   \n",
       "1    -3.336795   1.797167  -3.486065   -6.017003 -4.132899      3.279539   \n",
       "2    -3.336795  -2.233603  -3.671158   -6.415517 -4.737156     -0.074308   \n",
       "3    -3.336795  -1.829021  -0.116397   -5.559258 -4.537337      1.818908   \n",
       "4    -3.336795   1.391965  -1.822279   -4.955669 -6.639479      1.248546   \n",
       "..         ...        ...        ...         ...       ...           ...   \n",
       "538  -6.196767  -1.463565  -5.903482   -5.240336 -4.959568     -5.974346   \n",
       "539  -5.076682   1.407784   1.214362   -3.897758  5.073883      1.195720   \n",
       "540  -3.469021  -0.787238  -2.743620   -5.957548 -3.526360     -1.963500   \n",
       "541   3.115970   0.597641  -0.763934   -5.240336 -3.225249      2.191202   \n",
       "542   1.574590   4.366079  -0.781208   -2.472102  5.054112      1.928232   \n",
       "\n",
       "     Trametinib  Vinblastine  Vinorelbine  \n",
       "0      1.325603    -4.384381    -3.401996  \n",
       "1     -0.040150    -4.849422    -5.303091  \n",
       "2      1.621553    -4.784344    -3.927690  \n",
       "3     -0.275098    -3.494500    -5.252216  \n",
       "4      1.437469    -3.986408    -3.138535  \n",
       "..          ...          ...          ...  \n",
       "538    0.641326    -7.103903    -6.196508  \n",
       "539    3.598733    -0.095525     2.307072  \n",
       "540    1.153223    -5.412392    -4.207989  \n",
       "541    0.407173    -3.494500    -2.601804  \n",
       "542    5.681084     2.268159     1.959661  \n",
       "\n",
       "[543 rows x 81048 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna_meth = pd.read_csv('new_clean_data/consistent_dna_meth.csv')\n",
    "dna_meth.drop(columns='Unnamed: 0', inplace=True)\n",
    "dna_meth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns\n",
    "cell_line_cols = ['CELL_LINE_NAME']\n",
    "drug_resp_cols = ['Dactinomycin', 'Daporinad', 'Dasatinib',\t'Rapamycin', 'Romidepsin', 'SN-38',\t'Temsirolimus',\t'Trametinib', 'Vinblastine', 'Vinorelbine']\n",
    "dna_meth_cols = dna_meth.drop(columns=['CELL_LINE_NAME','Dactinomycin', 'Daporinad', 'Dasatinib',\t'Rapamycin', 'Romidepsin', 'SN-38',\t'Temsirolimus',\t'Trametinib', 'Vinblastine', 'Vinorelbine']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drug response labels to tensor\n",
    "drug_resp_labels = np.stack([dna_meth[col].values for col in drug_resp_cols], 1)\n",
    "drug_resp_labels = torch.tensor(drug_resp_labels, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dna_meth values to tensor\n",
    "dna_meth_values = np.stack([dna_meth[col].values for col in dna_meth_cols], 1)\n",
    "dna_meth_values = torch.tensor(dna_meth_values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell line values to list\n",
    "cell_line_list = []\n",
    "for col in cell_line_cols:\n",
    "    cell_line_list.append(dna_meth[col].values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "full_dataset = torch.utils.data.TensorDataset(dna_meth_values, drug_resp_labels)\n",
    "full_loader = DataLoader(full_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# dna_meth_train, dna_meth_test, drug_resp_labels_train, drug_resp_labels_test = train_test_split(dna_meth_values, drug_resp_labels, test_size= 0.1, random_state= 0, shuffle=False)\n",
    "\n",
    "# train_dataset = torch.utils.data.TensorDataset(dna_meth_train, drug_resp_labels_train)\n",
    "# test_dataset = torch.utils.data.TensorDataset(dna_meth_test, drug_resp_labels_test)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, shuffle= False, batch_size= batch_size)\n",
    "# test_loader = DataLoader(test_dataset, shuffle= False, batch_size= batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE model with Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_with_regressor(nn.Module):\n",
    "    def __init__(self, input_size, level_2, level_3, latent_dim):\n",
    "        super(VAE_with_regressor, self).__init__()\n",
    "        cat_features = 10\n",
    "        # Encoder layers\n",
    "        self.enc_fc1 = nn.Sequential(\n",
    "                        nn.Linear(input_size, level_2),\n",
    "                        nn.BatchNorm1d(level_2),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.enc_fc2 = nn.Sequential(\n",
    "                        nn.Linear(level_2, level_3),\n",
    "                        nn.BatchNorm1d(level_3),\n",
    "                        nn.ReLU())\n",
    "\n",
    "        self.enc_fc3_mean = nn.Sequential(\n",
    "                    nn.Linear(level_3, latent_dim),\n",
    "                    nn.BatchNorm1d(latent_dim))\n",
    "        \n",
    "        self.enc_fc3_log_var = nn.Sequential(\n",
    "                    nn.Linear(level_3, latent_dim),\n",
    "                    nn.BatchNorm1d(latent_dim))\n",
    "        \n",
    "        \n",
    "        # Decoder layers\n",
    "        self.dec_fc3 = nn.Sequential(\n",
    "                        nn.Linear(latent_dim, level_3),\n",
    "                        nn.BatchNorm1d(level_3),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.dec_fc2 = nn.Sequential(\n",
    "                        nn.Linear(level_3, level_2),\n",
    "                        nn.BatchNorm1d(level_2),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.dec_fc1 = nn.Sequential(\n",
    "                    nn.Linear(level_2, input_size),\n",
    "                    nn.BatchNorm1d(input_size),\n",
    "                    nn.Sigmoid())\n",
    "        \n",
    "        # Regression fc layers\n",
    "        self.r_fc1 = nn.Sequential(\n",
    "                    nn.Linear(latent_dim, 64),\n",
    "                    nn.BatchNorm1d(64),\n",
    "                    nn.ReLU())\n",
    "        self.r_fc2 = nn.Sequential(\n",
    "                    nn.Linear(64, 32),\n",
    "                    nn.BatchNorm1d(32),\n",
    "                    nn.ReLU())\n",
    "        \n",
    "        self.r_fc3 = nn.Sequential(\n",
    "                    nn.Linear(32, 10),\n",
    "                    nn.BatchNorm1d(10))\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        l2_layer = self.enc_fc1(x)\n",
    "        l3_layer = self.enc_fc2(l2_layer)\n",
    "        \n",
    "        mu = self.enc_fc3_mean(l3_layer)\n",
    "        logvar = self.enc_fc3_log_var(l3_layer)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        l3_layer = self.dec_fc3(z)\n",
    "        l2_layer = self.dec_fc2(l3_layer)\n",
    "        x_hat = self.dec_fc1(l2_layer)\n",
    "        return x_hat\n",
    "\n",
    "    def regressor(self, mean):\n",
    "        level_1_layer = self.r_fc1(mean)\n",
    "        level_2_layer = self.r_fc2(level_1_layer)\n",
    "        output_layer = self.r_fc3(level_2_layer)\n",
    "        return output_layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        pred_y = self.regressor(mu)\n",
    "        return x_hat, mu, logvar, z, pred_y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function(x_hat, x, mean, log_var): # recon loss and kld loss\n",
    "        bce = torch.nn.functional.binary_cross_entropy(x_hat, x, reduction = 'sum')\n",
    "        kld = 0.5 * torch.sum(log_var.exp() + mean.pow(2) - 1 - log_var)\n",
    "        loss = kld + bce\n",
    "        return loss, kld, bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss_function(y_pred, y):\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "    loss = torch.sqrt(loss_fn(y_pred, y))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_function(y_pred, y):\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_function_mean(y_pred, y):\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y_true, y_pred):\n",
    "    y_true_mean = torch.mean(y_true)\n",
    "    SS_res = torch.sum(torch.square(y_true - y_pred))\n",
    "    SS_tot = torch.sum(torch.square(y_true - y_true_mean))\n",
    "    r_squared = 1 - SS_res / (SS_tot + torch.finfo(torch.float32).eps)\n",
    "    return r_squared"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "lr = 0.001\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "k = 10\n",
    "splits = KFold(n_splits=k, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "input_size = 81037 #dimension of gene expressions\n",
    "level_2 = 2048\n",
    "level_3 = 1024\n",
    "latent_dim = 128 # target latent size\n",
    "model = VAE_with_regressor(input_size, level_2, level_3, latent_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (model, dataloader, epoch, optimizer, w_recon_loss, w_kl_loss, w_reg_loss):\n",
    "    model.train()\n",
    "    train_recon = 0\n",
    "    train_kl  = 0\n",
    "    train_reg = 0\n",
    "    train_total_loss = 0\n",
    "    batch_num = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        batch_num = batch_num + 1\n",
    "        # get data values\n",
    "        dna_meth_values, drug_resp_labels = batch\n",
    "        dna_meth_values = dna_meth_values.to(device)\n",
    "        drug_resp_labels = drug_resp_labels.to(device)\n",
    "\n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        # apply model\n",
    "        x_hat, mu, logvar, z, pred_y = model(dna_meth_values)\n",
    "\n",
    "        # losses\n",
    "        vae_loss, kld, bce = vae_loss_function(x_hat, dna_meth_values, mu, logvar)\n",
    "        mse_loss = mse_loss_function_mean(pred_y, drug_resp_labels)\n",
    "\n",
    "        #apply loss weights\n",
    "        loss = w_recon_loss * bce + w_kl_loss * kld + w_reg_loss * mse_loss\n",
    "\n",
    "        \n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            train_recon += bce.item()\n",
    "            train_kl += kld.item()\n",
    "            train_reg += mse_loss.item()\n",
    "            train_total_loss += loss.item()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    train_recon_ave = train_recon/ len(dataloader.sampler)\n",
    "    train_kl_ave = train_kl/ len(dataloader.sampler)\n",
    "    train_reg_ave = train_reg/ batch_num\n",
    "    train_total_ave = train_total_loss/ len(dataloader.sampler)\n",
    "\n",
    "    print('=====> Epoch {} \\n' \n",
    "          'Average Recon Loss: {:.3f} \\n'\n",
    "          'Average KL Loss: {:.3f} \\n'\n",
    "          'Average MSE Loss: {:.3f} \\n'.format(epoch, train_recon_ave, train_kl_ave, train_reg_ave))\n",
    "    \n",
    "    return train_recon_ave, train_kl_ave, train_reg_ave, train_total_ave\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model):\n",
    "    model.eval()\n",
    "    test_recon = 0\n",
    "    test_kl  = 0\n",
    "    test_reg = 0\n",
    "    test_rmse = 0\n",
    "    test_r2 = 0\n",
    "    batch_num = 0\n",
    "    with torch.no_grad():\n",
    "        mean_store = torch.zeros(1, latent_dim).to(device)\n",
    "        pred_store = torch.zeros(1,10).to(device)\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            batch_num = batch_num + 1\n",
    "            # get data values\n",
    "            dna_meth_values, drug_resp_labels = batch\n",
    "            dna_meth_values = dna_meth_values.to(device)\n",
    "            drug_resp_labels = drug_resp_labels.to(device)\n",
    "\n",
    "            # apply model\n",
    "            x_hat, mu, logvar, z, pred_y = model(dna_meth_values)\n",
    "\n",
    "            # loss functions\n",
    "            vae_loss, kld, bce = vae_loss_function(x_hat, dna_meth_values, mu, logvar)\n",
    "            mse_loss = mse_loss_function(pred_y, drug_resp_labels)\n",
    "            r2_value = r_squared(drug_resp_labels, pred_y)\n",
    "\n",
    "            test_recon += bce.item()\n",
    "            test_kl += kld.item()\n",
    "            test_reg += mse_loss.item()\n",
    "            test_r2 += r2_value.item()\n",
    "\n",
    "            # Get relevant values\n",
    "            mean_store = torch.cat((mean_store, mu), 0)\n",
    "            pred_store = torch.cat((pred_store, pred_y), 0)\n",
    "\n",
    "        # ====== export reduced dimensions and predictions ======\n",
    "        mean_final = mean_store[1:]\n",
    "        mean_final_np = mean_final.cpu().numpy()\n",
    "        mean_final_df = pd.DataFrame(mean_final_np)\n",
    "\n",
    "        pred_final = pred_store[1:]\n",
    "        pred_final_np = pred_final.cpu().numpy()\n",
    "        pred_final_df = pd.DataFrame(pred_final_np)\n",
    "\n",
    "        input_path = 'C:/Users/mdzak/Desktop/GitHub/FYP_Zaki/notebooks_new/output/'\n",
    "\n",
    "        mean_path =  input_path + str(latent_dim) + 'D_dna_meth_latent_code.tsv'\n",
    "        pred_path = input_path + str(latent_dim) + 'D_dna_meth_predictions.tsv'\n",
    "\n",
    "        mean_final_df.to_csv(mean_path, sep='\\t')\n",
    "        pred_final_df.to_csv(pred_path, sep='\\t')\n",
    "\n",
    "        # print loss\n",
    "        test_recon_ave = test_recon/ len(dataloader.sampler)\n",
    "        test_kl_ave = test_kl/ len(dataloader.sampler)\n",
    "        test_reg_ave = test_reg/ (len(dataloader.sampler) * 10)\n",
    "        test_rmse_ave = math.sqrt(test_reg_ave)\n",
    "        test_r2_ave = test_r2/ batch_num\n",
    "\n",
    "        print('Average Recon Loss: {:.3f} \\n'\n",
    "          'Average KL Loss: {:.3f} \\n'\n",
    "          'Average MSE Loss: {:.3f} \\n'\n",
    "          'Average RMSE Loss: {:.3f} \\n'\n",
    "          'Average R2: {:.3f} \\n'.format(test_recon_ave, test_kl_ave, test_reg_ave, test_rmse_ave, test_r2_ave))\n",
    "        \n",
    "        return test_recon_ave, test_kl_ave, test_reg_ave, test_rmse_ave, test_r2_ave"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Training (Cross Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 57831.100 \n",
      "Average KL Loss: 113.556 \n",
      "Average MSE Loss: 13.344 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 54712.327 \n",
      "Average KL Loss: 108.740 \n",
      "Average MSE Loss: 13.619 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 53732.118 \n",
      "Average KL Loss: 104.840 \n",
      "Average MSE Loss: 13.353 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 53263.044 \n",
      "Average KL Loss: 101.137 \n",
      "Average MSE Loss: 13.592 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 52748.168 \n",
      "Average KL Loss: 99.768 \n",
      "Average MSE Loss: 13.588 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 52295.067 \n",
      "Average KL Loss: 98.110 \n",
      "Average MSE Loss: 13.412 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 51696.341 \n",
      "Average KL Loss: 96.494 \n",
      "Average MSE Loss: 13.422 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 51331.447 \n",
      "Average KL Loss: 96.137 \n",
      "Average MSE Loss: 13.342 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 50883.970 \n",
      "Average KL Loss: 95.617 \n",
      "Average MSE Loss: 13.370 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 50508.332 \n",
      "Average KL Loss: 95.048 \n",
      "Average MSE Loss: 13.341 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 50030.676 \n",
      "Average KL Loss: 95.041 \n",
      "Average MSE Loss: 13.182 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 49720.509 \n",
      "Average KL Loss: 95.556 \n",
      "Average MSE Loss: 13.295 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 49336.148 \n",
      "Average KL Loss: 95.541 \n",
      "Average MSE Loss: 13.210 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 48960.567 \n",
      "Average KL Loss: 96.218 \n",
      "Average MSE Loss: 13.331 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 48573.511 \n",
      "Average KL Loss: 96.168 \n",
      "Average MSE Loss: 13.393 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 48272.325 \n",
      "Average KL Loss: 96.977 \n",
      "Average MSE Loss: 13.323 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 47938.244 \n",
      "Average KL Loss: 97.426 \n",
      "Average MSE Loss: 13.459 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 47583.349 \n",
      "Average KL Loss: 97.694 \n",
      "Average MSE Loss: 13.392 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 47337.354 \n",
      "Average KL Loss: 98.642 \n",
      "Average MSE Loss: 13.420 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 46897.929 \n",
      "Average KL Loss: 98.608 \n",
      "Average MSE Loss: 13.413 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 46503.103 \n",
      "Average KL Loss: 98.943 \n",
      "Average MSE Loss: 13.350 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 46170.609 \n",
      "Average KL Loss: 99.365 \n",
      "Average MSE Loss: 13.432 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 45855.418 \n",
      "Average KL Loss: 99.998 \n",
      "Average MSE Loss: 13.387 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 45541.134 \n",
      "Average KL Loss: 100.578 \n",
      "Average MSE Loss: 13.353 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 45314.732 \n",
      "Average KL Loss: 101.757 \n",
      "Average MSE Loss: 13.391 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 45024.816 \n",
      "Average KL Loss: 102.776 \n",
      "Average MSE Loss: 13.515 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 44660.280 \n",
      "Average KL Loss: 103.230 \n",
      "Average MSE Loss: 13.379 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 44289.804 \n",
      "Average KL Loss: 103.977 \n",
      "Average MSE Loss: 13.581 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 44052.413 \n",
      "Average KL Loss: 105.043 \n",
      "Average MSE Loss: 13.330 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 43698.384 \n",
      "Average KL Loss: 105.712 \n",
      "Average MSE Loss: 13.398 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 43343.082 \n",
      "Average KL Loss: 106.333 \n",
      "Average MSE Loss: 13.504 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 43156.015 \n",
      "Average KL Loss: 107.192 \n",
      "Average MSE Loss: 13.418 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 42852.138 \n",
      "Average KL Loss: 107.725 \n",
      "Average MSE Loss: 13.360 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 42574.371 \n",
      "Average KL Loss: 108.652 \n",
      "Average MSE Loss: 13.366 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 42258.708 \n",
      "Average KL Loss: 109.598 \n",
      "Average MSE Loss: 13.477 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 42021.530 \n",
      "Average KL Loss: 110.313 \n",
      "Average MSE Loss: 13.498 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 41830.722 \n",
      "Average KL Loss: 111.178 \n",
      "Average MSE Loss: 13.493 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 41565.341 \n",
      "Average KL Loss: 112.145 \n",
      "Average MSE Loss: 13.431 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 41336.721 \n",
      "Average KL Loss: 112.924 \n",
      "Average MSE Loss: 13.582 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 40976.404 \n",
      "Average KL Loss: 114.067 \n",
      "Average MSE Loss: 13.441 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 40661.123 \n",
      "Average KL Loss: 114.839 \n",
      "Average MSE Loss: 13.543 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 40660.769 \n",
      "Average KL Loss: 115.920 \n",
      "Average MSE Loss: 13.505 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 40367.754 \n",
      "Average KL Loss: 116.833 \n",
      "Average MSE Loss: 13.642 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 40005.429 \n",
      "Average KL Loss: 117.797 \n",
      "Average MSE Loss: 13.414 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 39928.075 \n",
      "Average KL Loss: 118.842 \n",
      "Average MSE Loss: 13.482 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 39494.940 \n",
      "Average KL Loss: 119.724 \n",
      "Average MSE Loss: 13.553 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 39228.701 \n",
      "Average KL Loss: 120.893 \n",
      "Average MSE Loss: 13.374 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 39032.000 \n",
      "Average KL Loss: 121.863 \n",
      "Average MSE Loss: 13.589 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 38817.705 \n",
      "Average KL Loss: 122.769 \n",
      "Average MSE Loss: 13.499 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 38471.376 \n",
      "Average KL Loss: 123.940 \n",
      "Average MSE Loss: 13.637 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 38676.280 \n",
      "Average KL Loss: 124.771 \n",
      "Average MSE Loss: 12.434 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 39140.101 \n",
      "Average KL Loss: 125.082 \n",
      "Average MSE Loss: 11.214 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 39319.791 \n",
      "Average KL Loss: 124.916 \n",
      "Average MSE Loss: 10.729 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 39437.616 \n",
      "Average KL Loss: 125.000 \n",
      "Average MSE Loss: 10.469 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 39435.557 \n",
      "Average KL Loss: 125.041 \n",
      "Average MSE Loss: 10.109 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 39381.997 \n",
      "Average KL Loss: 125.072 \n",
      "Average MSE Loss: 9.892 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 39487.899 \n",
      "Average KL Loss: 125.054 \n",
      "Average MSE Loss: 9.662 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 39399.414 \n",
      "Average KL Loss: 124.890 \n",
      "Average MSE Loss: 9.468 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 39514.895 \n",
      "Average KL Loss: 125.056 \n",
      "Average MSE Loss: 9.295 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 39397.752 \n",
      "Average KL Loss: 125.105 \n",
      "Average MSE Loss: 9.064 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 39442.111 \n",
      "Average KL Loss: 125.046 \n",
      "Average MSE Loss: 8.943 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 39297.471 \n",
      "Average KL Loss: 124.989 \n",
      "Average MSE Loss: 8.859 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 39367.028 \n",
      "Average KL Loss: 125.107 \n",
      "Average MSE Loss: 8.575 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 39373.388 \n",
      "Average KL Loss: 124.982 \n",
      "Average MSE Loss: 8.509 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 39600.173 \n",
      "Average KL Loss: 125.056 \n",
      "Average MSE Loss: 8.408 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 39431.078 \n",
      "Average KL Loss: 124.990 \n",
      "Average MSE Loss: 8.240 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 39437.720 \n",
      "Average KL Loss: 124.934 \n",
      "Average MSE Loss: 8.155 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 39340.264 \n",
      "Average KL Loss: 124.889 \n",
      "Average MSE Loss: 8.008 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 39476.179 \n",
      "Average KL Loss: 125.042 \n",
      "Average MSE Loss: 7.976 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 39384.780 \n",
      "Average KL Loss: 124.993 \n",
      "Average MSE Loss: 7.788 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 39310.566 \n",
      "Average KL Loss: 124.951 \n",
      "Average MSE Loss: 7.611 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 39377.438 \n",
      "Average KL Loss: 124.989 \n",
      "Average MSE Loss: 7.625 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 39371.784 \n",
      "Average KL Loss: 125.050 \n",
      "Average MSE Loss: 7.518 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 39475.151 \n",
      "Average KL Loss: 124.994 \n",
      "Average MSE Loss: 7.326 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 39449.603 \n",
      "Average KL Loss: 125.023 \n",
      "Average MSE Loss: 7.397 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 39304.829 \n",
      "Average KL Loss: 124.987 \n",
      "Average MSE Loss: 7.250 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 39465.817 \n",
      "Average KL Loss: 125.007 \n",
      "Average MSE Loss: 7.071 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 39411.278 \n",
      "Average KL Loss: 125.046 \n",
      "Average MSE Loss: 7.283 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 39334.245 \n",
      "Average KL Loss: 124.964 \n",
      "Average MSE Loss: 7.053 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 39491.611 \n",
      "Average KL Loss: 125.066 \n",
      "Average MSE Loss: 6.868 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 39480.130 \n",
      "Average KL Loss: 125.244 \n",
      "Average MSE Loss: 6.747 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 39296.381 \n",
      "Average KL Loss: 124.945 \n",
      "Average MSE Loss: 6.840 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 39465.603 \n",
      "Average KL Loss: 124.955 \n",
      "Average MSE Loss: 6.599 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 39403.276 \n",
      "Average KL Loss: 124.884 \n",
      "Average MSE Loss: 6.515 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 39553.365 \n",
      "Average KL Loss: 125.022 \n",
      "Average MSE Loss: 6.553 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 39485.120 \n",
      "Average KL Loss: 125.015 \n",
      "Average MSE Loss: 6.461 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 39349.806 \n",
      "Average KL Loss: 124.911 \n",
      "Average MSE Loss: 6.470 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 39332.730 \n",
      "Average KL Loss: 124.927 \n",
      "Average MSE Loss: 6.309 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 39415.203 \n",
      "Average KL Loss: 124.914 \n",
      "Average MSE Loss: 6.192 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 39336.948 \n",
      "Average KL Loss: 124.909 \n",
      "Average MSE Loss: 6.165 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 39438.866 \n",
      "Average KL Loss: 125.089 \n",
      "Average MSE Loss: 6.059 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 39362.091 \n",
      "Average KL Loss: 125.038 \n",
      "Average MSE Loss: 6.057 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 39367.751 \n",
      "Average KL Loss: 125.054 \n",
      "Average MSE Loss: 6.036 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 39390.901 \n",
      "Average KL Loss: 124.901 \n",
      "Average MSE Loss: 5.949 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 39502.702 \n",
      "Average KL Loss: 125.127 \n",
      "Average MSE Loss: 6.011 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 39410.472 \n",
      "Average KL Loss: 125.047 \n",
      "Average MSE Loss: 5.945 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 39352.951 \n",
      "Average KL Loss: 124.877 \n",
      "Average MSE Loss: 5.825 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 39406.144 \n",
      "Average KL Loss: 125.054 \n",
      "Average MSE Loss: 5.728 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 39439.343 \n",
      "Average KL Loss: 125.043 \n",
      "Average MSE Loss: 5.690 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 39378.882 \n",
      "Average KL Loss: 124.971 \n",
      "Average MSE Loss: 5.551 \n",
      "\n",
      "Average Recon Loss: 42014.889 \n",
      "Average KL Loss: 90.775 \n",
      "Average MSE Loss: 8.118 \n",
      "Average RMSE Loss: 2.849 \n",
      "Average R2: 0.084 \n",
      "\n",
      "Fold 2\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 39198.650 \n",
      "Average KL Loss: 125.703 \n",
      "Average MSE Loss: 6.046 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 38855.662 \n",
      "Average KL Loss: 126.581 \n",
      "Average MSE Loss: 6.305 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 38429.753 \n",
      "Average KL Loss: 127.962 \n",
      "Average MSE Loss: 6.063 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 37934.955 \n",
      "Average KL Loss: 129.187 \n",
      "Average MSE Loss: 6.366 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 37672.702 \n",
      "Average KL Loss: 130.506 \n",
      "Average MSE Loss: 6.371 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 37264.959 \n",
      "Average KL Loss: 131.903 \n",
      "Average MSE Loss: 6.315 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 36877.617 \n",
      "Average KL Loss: 133.154 \n",
      "Average MSE Loss: 6.179 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 36508.254 \n",
      "Average KL Loss: 134.480 \n",
      "Average MSE Loss: 6.251 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 36288.419 \n",
      "Average KL Loss: 135.660 \n",
      "Average MSE Loss: 6.203 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 36080.116 \n",
      "Average KL Loss: 137.074 \n",
      "Average MSE Loss: 6.282 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 35520.390 \n",
      "Average KL Loss: 138.397 \n",
      "Average MSE Loss: 6.163 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 35234.585 \n",
      "Average KL Loss: 139.724 \n",
      "Average MSE Loss: 6.163 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 35025.423 \n",
      "Average KL Loss: 141.068 \n",
      "Average MSE Loss: 6.384 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 34596.836 \n",
      "Average KL Loss: 142.371 \n",
      "Average MSE Loss: 6.288 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 34323.655 \n",
      "Average KL Loss: 143.621 \n",
      "Average MSE Loss: 6.288 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 34237.794 \n",
      "Average KL Loss: 144.843 \n",
      "Average MSE Loss: 6.348 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 34128.486 \n",
      "Average KL Loss: 146.162 \n",
      "Average MSE Loss: 6.313 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 33748.318 \n",
      "Average KL Loss: 147.464 \n",
      "Average MSE Loss: 6.137 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 33448.115 \n",
      "Average KL Loss: 148.734 \n",
      "Average MSE Loss: 6.199 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 33230.649 \n",
      "Average KL Loss: 149.919 \n",
      "Average MSE Loss: 6.238 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 32971.745 \n",
      "Average KL Loss: 151.132 \n",
      "Average MSE Loss: 6.369 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 32698.138 \n",
      "Average KL Loss: 152.340 \n",
      "Average MSE Loss: 6.234 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 32455.395 \n",
      "Average KL Loss: 153.582 \n",
      "Average MSE Loss: 6.266 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 32231.102 \n",
      "Average KL Loss: 154.816 \n",
      "Average MSE Loss: 6.320 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 32266.909 \n",
      "Average KL Loss: 156.046 \n",
      "Average MSE Loss: 6.234 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 31783.477 \n",
      "Average KL Loss: 157.243 \n",
      "Average MSE Loss: 6.219 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 31534.290 \n",
      "Average KL Loss: 158.467 \n",
      "Average MSE Loss: 6.303 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 31353.366 \n",
      "Average KL Loss: 159.703 \n",
      "Average MSE Loss: 6.271 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 31161.067 \n",
      "Average KL Loss: 160.884 \n",
      "Average MSE Loss: 6.263 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 31014.163 \n",
      "Average KL Loss: 161.954 \n",
      "Average MSE Loss: 6.258 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 31001.942 \n",
      "Average KL Loss: 163.094 \n",
      "Average MSE Loss: 6.330 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 30668.332 \n",
      "Average KL Loss: 164.155 \n",
      "Average MSE Loss: 6.306 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 30307.141 \n",
      "Average KL Loss: 165.226 \n",
      "Average MSE Loss: 6.399 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 30234.141 \n",
      "Average KL Loss: 166.409 \n",
      "Average MSE Loss: 6.282 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 30131.433 \n",
      "Average KL Loss: 167.542 \n",
      "Average MSE Loss: 6.266 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 29950.357 \n",
      "Average KL Loss: 168.632 \n",
      "Average MSE Loss: 6.321 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 29623.959 \n",
      "Average KL Loss: 169.693 \n",
      "Average MSE Loss: 6.324 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 29664.611 \n",
      "Average KL Loss: 170.657 \n",
      "Average MSE Loss: 6.320 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 29382.582 \n",
      "Average KL Loss: 171.669 \n",
      "Average MSE Loss: 6.210 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 29199.782 \n",
      "Average KL Loss: 172.636 \n",
      "Average MSE Loss: 6.303 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 29044.855 \n",
      "Average KL Loss: 173.588 \n",
      "Average MSE Loss: 6.288 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 28812.415 \n",
      "Average KL Loss: 174.514 \n",
      "Average MSE Loss: 6.298 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 28766.724 \n",
      "Average KL Loss: 175.445 \n",
      "Average MSE Loss: 6.361 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 28647.216 \n",
      "Average KL Loss: 176.446 \n",
      "Average MSE Loss: 6.331 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 28438.643 \n",
      "Average KL Loss: 177.449 \n",
      "Average MSE Loss: 6.356 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 28386.271 \n",
      "Average KL Loss: 178.369 \n",
      "Average MSE Loss: 6.472 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 28308.420 \n",
      "Average KL Loss: 179.252 \n",
      "Average MSE Loss: 6.356 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 28265.458 \n",
      "Average KL Loss: 180.132 \n",
      "Average MSE Loss: 6.426 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 27916.234 \n",
      "Average KL Loss: 181.002 \n",
      "Average MSE Loss: 6.392 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 27842.030 \n",
      "Average KL Loss: 181.887 \n",
      "Average MSE Loss: 6.467 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 28129.393 \n",
      "Average KL Loss: 182.534 \n",
      "Average MSE Loss: 6.419 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 29124.398 \n",
      "Average KL Loss: 182.700 \n",
      "Average MSE Loss: 5.911 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 29298.054 \n",
      "Average KL Loss: 182.741 \n",
      "Average MSE Loss: 5.668 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 29377.711 \n",
      "Average KL Loss: 182.750 \n",
      "Average MSE Loss: 5.540 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 29346.187 \n",
      "Average KL Loss: 182.753 \n",
      "Average MSE Loss: 5.394 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 29483.053 \n",
      "Average KL Loss: 182.749 \n",
      "Average MSE Loss: 5.450 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 29303.169 \n",
      "Average KL Loss: 182.752 \n",
      "Average MSE Loss: 5.427 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 29395.673 \n",
      "Average KL Loss: 182.752 \n",
      "Average MSE Loss: 5.188 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 29212.190 \n",
      "Average KL Loss: 182.757 \n",
      "Average MSE Loss: 5.018 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 29480.807 \n",
      "Average KL Loss: 182.754 \n",
      "Average MSE Loss: 5.143 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 29417.703 \n",
      "Average KL Loss: 182.754 \n",
      "Average MSE Loss: 5.053 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 29340.219 \n",
      "Average KL Loss: 182.754 \n",
      "Average MSE Loss: 4.925 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 29423.059 \n",
      "Average KL Loss: 182.747 \n",
      "Average MSE Loss: 4.838 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 29224.086 \n",
      "Average KL Loss: 182.747 \n",
      "Average MSE Loss: 4.628 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 29512.984 \n",
      "Average KL Loss: 182.762 \n",
      "Average MSE Loss: 4.750 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 29325.107 \n",
      "Average KL Loss: 182.756 \n",
      "Average MSE Loss: 4.618 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 29517.640 \n",
      "Average KL Loss: 182.746 \n",
      "Average MSE Loss: 4.471 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 29358.872 \n",
      "Average KL Loss: 182.752 \n",
      "Average MSE Loss: 4.475 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 29386.387 \n",
      "Average KL Loss: 182.746 \n",
      "Average MSE Loss: 4.388 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 29339.049 \n",
      "Average KL Loss: 182.750 \n",
      "Average MSE Loss: 4.392 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 29275.468 \n",
      "Average KL Loss: 182.740 \n",
      "Average MSE Loss: 4.265 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 29276.663 \n",
      "Average KL Loss: 182.750 \n",
      "Average MSE Loss: 4.133 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 29317.795 \n",
      "Average KL Loss: 182.754 \n",
      "Average MSE Loss: 4.227 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 29285.152 \n",
      "Average KL Loss: 182.742 \n",
      "Average MSE Loss: 4.153 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 29252.059 \n",
      "Average KL Loss: 182.750 \n",
      "Average MSE Loss: 3.963 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 29556.483 \n",
      "Average KL Loss: 182.755 \n",
      "Average MSE Loss: 4.219 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 29411.742 \n",
      "Average KL Loss: 182.755 \n",
      "Average MSE Loss: 4.084 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 29334.795 \n",
      "Average KL Loss: 182.748 \n",
      "Average MSE Loss: 4.027 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 29370.249 \n",
      "Average KL Loss: 182.751 \n",
      "Average MSE Loss: 3.892 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 29466.850 \n",
      "Average KL Loss: 182.756 \n",
      "Average MSE Loss: 3.808 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 29389.877 \n",
      "Average KL Loss: 182.750 \n",
      "Average MSE Loss: 3.794 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 29367.579 \n",
      "Average KL Loss: 182.755 \n",
      "Average MSE Loss: 3.816 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 29345.896 \n",
      "Average KL Loss: 182.753 \n",
      "Average MSE Loss: 3.671 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 29496.393 \n",
      "Average KL Loss: 182.760 \n",
      "Average MSE Loss: 3.893 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 29455.792 \n",
      "Average KL Loss: 182.755 \n",
      "Average MSE Loss: 3.664 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 29227.054 \n",
      "Average KL Loss: 182.752 \n",
      "Average MSE Loss: 3.644 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 29382.851 \n",
      "Average KL Loss: 182.751 \n",
      "Average MSE Loss: 3.730 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 29364.922 \n",
      "Average KL Loss: 182.750 \n",
      "Average MSE Loss: 3.519 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 29536.557 \n",
      "Average KL Loss: 182.754 \n",
      "Average MSE Loss: 3.619 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 29252.994 \n",
      "Average KL Loss: 182.746 \n",
      "Average MSE Loss: 3.572 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 29481.631 \n",
      "Average KL Loss: 182.749 \n",
      "Average MSE Loss: 3.626 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 29484.760 \n",
      "Average KL Loss: 182.749 \n",
      "Average MSE Loss: 3.616 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 29355.667 \n",
      "Average KL Loss: 182.747 \n",
      "Average MSE Loss: 3.435 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 29285.673 \n",
      "Average KL Loss: 182.744 \n",
      "Average MSE Loss: 3.308 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 29277.391 \n",
      "Average KL Loss: 182.750 \n",
      "Average MSE Loss: 3.379 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 29436.021 \n",
      "Average KL Loss: 182.747 \n",
      "Average MSE Loss: 3.454 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 29339.802 \n",
      "Average KL Loss: 182.753 \n",
      "Average MSE Loss: 3.525 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 29510.655 \n",
      "Average KL Loss: 182.746 \n",
      "Average MSE Loss: 3.355 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 29411.049 \n",
      "Average KL Loss: 182.750 \n",
      "Average MSE Loss: 3.437 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 29287.622 \n",
      "Average KL Loss: 182.755 \n",
      "Average MSE Loss: 3.260 \n",
      "\n",
      "Average Recon Loss: 36964.916 \n",
      "Average KL Loss: 145.532 \n",
      "Average MSE Loss: 5.082 \n",
      "Average RMSE Loss: 2.254 \n",
      "Average R2: 0.446 \n",
      "\n",
      "Fold 3\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 29717.502 \n",
      "Average KL Loss: 182.956 \n",
      "Average MSE Loss: 3.797 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 29091.312 \n",
      "Average KL Loss: 183.825 \n",
      "Average MSE Loss: 3.769 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 28781.583 \n",
      "Average KL Loss: 184.874 \n",
      "Average MSE Loss: 3.785 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 28372.594 \n",
      "Average KL Loss: 185.873 \n",
      "Average MSE Loss: 3.810 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 28086.174 \n",
      "Average KL Loss: 186.750 \n",
      "Average MSE Loss: 3.854 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 27938.637 \n",
      "Average KL Loss: 187.607 \n",
      "Average MSE Loss: 4.082 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 27820.973 \n",
      "Average KL Loss: 188.468 \n",
      "Average MSE Loss: 3.802 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 27503.237 \n",
      "Average KL Loss: 189.331 \n",
      "Average MSE Loss: 3.839 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 27231.196 \n",
      "Average KL Loss: 190.147 \n",
      "Average MSE Loss: 3.730 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 27170.093 \n",
      "Average KL Loss: 190.907 \n",
      "Average MSE Loss: 3.913 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 27016.250 \n",
      "Average KL Loss: 191.675 \n",
      "Average MSE Loss: 3.968 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 26740.429 \n",
      "Average KL Loss: 192.420 \n",
      "Average MSE Loss: 3.707 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 26473.719 \n",
      "Average KL Loss: 193.075 \n",
      "Average MSE Loss: 3.874 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 26329.675 \n",
      "Average KL Loss: 193.699 \n",
      "Average MSE Loss: 3.821 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 26293.476 \n",
      "Average KL Loss: 194.295 \n",
      "Average MSE Loss: 3.864 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 26244.172 \n",
      "Average KL Loss: 194.893 \n",
      "Average MSE Loss: 3.827 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 26118.492 \n",
      "Average KL Loss: 195.456 \n",
      "Average MSE Loss: 3.857 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 25992.853 \n",
      "Average KL Loss: 196.066 \n",
      "Average MSE Loss: 3.723 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 25939.840 \n",
      "Average KL Loss: 196.667 \n",
      "Average MSE Loss: 3.909 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 25819.390 \n",
      "Average KL Loss: 197.244 \n",
      "Average MSE Loss: 3.808 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 25748.975 \n",
      "Average KL Loss: 197.839 \n",
      "Average MSE Loss: 3.756 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 25591.118 \n",
      "Average KL Loss: 198.384 \n",
      "Average MSE Loss: 3.907 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 25533.553 \n",
      "Average KL Loss: 198.888 \n",
      "Average MSE Loss: 3.770 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 25321.087 \n",
      "Average KL Loss: 199.370 \n",
      "Average MSE Loss: 3.816 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 25297.888 \n",
      "Average KL Loss: 199.807 \n",
      "Average MSE Loss: 3.853 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 25266.454 \n",
      "Average KL Loss: 200.242 \n",
      "Average MSE Loss: 3.751 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 25270.238 \n",
      "Average KL Loss: 200.685 \n",
      "Average MSE Loss: 3.976 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 25194.894 \n",
      "Average KL Loss: 201.155 \n",
      "Average MSE Loss: 3.820 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 25077.641 \n",
      "Average KL Loss: 201.587 \n",
      "Average MSE Loss: 3.888 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 24952.697 \n",
      "Average KL Loss: 201.982 \n",
      "Average MSE Loss: 3.964 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 24798.883 \n",
      "Average KL Loss: 202.366 \n",
      "Average MSE Loss: 3.845 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 24775.559 \n",
      "Average KL Loss: 202.704 \n",
      "Average MSE Loss: 3.731 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 24741.868 \n",
      "Average KL Loss: 203.070 \n",
      "Average MSE Loss: 3.786 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 24572.517 \n",
      "Average KL Loss: 203.467 \n",
      "Average MSE Loss: 3.825 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 24594.667 \n",
      "Average KL Loss: 203.819 \n",
      "Average MSE Loss: 3.731 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 24521.128 \n",
      "Average KL Loss: 204.122 \n",
      "Average MSE Loss: 3.904 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 24547.127 \n",
      "Average KL Loss: 204.489 \n",
      "Average MSE Loss: 3.830 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 24346.752 \n",
      "Average KL Loss: 204.852 \n",
      "Average MSE Loss: 3.930 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 24362.286 \n",
      "Average KL Loss: 205.150 \n",
      "Average MSE Loss: 3.800 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 24307.275 \n",
      "Average KL Loss: 205.432 \n",
      "Average MSE Loss: 3.843 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 24273.769 \n",
      "Average KL Loss: 205.723 \n",
      "Average MSE Loss: 3.772 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 24161.008 \n",
      "Average KL Loss: 205.964 \n",
      "Average MSE Loss: 3.848 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 24159.538 \n",
      "Average KL Loss: 206.198 \n",
      "Average MSE Loss: 3.838 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 23962.598 \n",
      "Average KL Loss: 206.443 \n",
      "Average MSE Loss: 3.801 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 23889.229 \n",
      "Average KL Loss: 206.646 \n",
      "Average MSE Loss: 3.869 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 23980.385 \n",
      "Average KL Loss: 206.809 \n",
      "Average MSE Loss: 3.924 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 23800.522 \n",
      "Average KL Loss: 206.957 \n",
      "Average MSE Loss: 3.755 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 23822.173 \n",
      "Average KL Loss: 207.095 \n",
      "Average MSE Loss: 3.916 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 23882.127 \n",
      "Average KL Loss: 207.196 \n",
      "Average MSE Loss: 3.924 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 23935.042 \n",
      "Average KL Loss: 207.355 \n",
      "Average MSE Loss: 3.908 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 23903.848 \n",
      "Average KL Loss: 207.537 \n",
      "Average MSE Loss: 3.865 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 24346.663 \n",
      "Average KL Loss: 207.596 \n",
      "Average MSE Loss: 3.675 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 24483.234 \n",
      "Average KL Loss: 207.607 \n",
      "Average MSE Loss: 3.435 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 24622.150 \n",
      "Average KL Loss: 207.609 \n",
      "Average MSE Loss: 3.473 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 24674.160 \n",
      "Average KL Loss: 207.609 \n",
      "Average MSE Loss: 3.375 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 24700.671 \n",
      "Average KL Loss: 207.611 \n",
      "Average MSE Loss: 3.303 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 24641.251 \n",
      "Average KL Loss: 207.609 \n",
      "Average MSE Loss: 3.219 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 24528.566 \n",
      "Average KL Loss: 207.609 \n",
      "Average MSE Loss: 3.193 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 24606.985 \n",
      "Average KL Loss: 207.611 \n",
      "Average MSE Loss: 3.089 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 24574.061 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 3.057 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 24676.743 \n",
      "Average KL Loss: 207.611 \n",
      "Average MSE Loss: 2.891 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 24584.099 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 2.920 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 24598.014 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 2.861 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 24539.511 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 2.900 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 24630.333 \n",
      "Average KL Loss: 207.611 \n",
      "Average MSE Loss: 2.841 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 24636.399 \n",
      "Average KL Loss: 207.611 \n",
      "Average MSE Loss: 2.922 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 24723.757 \n",
      "Average KL Loss: 207.611 \n",
      "Average MSE Loss: 2.856 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 24708.806 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 2.759 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 24763.354 \n",
      "Average KL Loss: 207.611 \n",
      "Average MSE Loss: 2.962 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 24656.107 \n",
      "Average KL Loss: 207.611 \n",
      "Average MSE Loss: 2.809 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 24603.467 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 2.960 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 24833.358 \n",
      "Average KL Loss: 207.611 \n",
      "Average MSE Loss: 2.792 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 24700.564 \n",
      "Average KL Loss: 207.612 \n",
      "Average MSE Loss: 2.680 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 24713.949 \n",
      "Average KL Loss: 207.611 \n",
      "Average MSE Loss: 2.608 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 24561.938 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 2.559 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 24698.372 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 2.593 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 24620.690 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 2.641 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 24596.451 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 2.653 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 24525.030 \n",
      "Average KL Loss: 207.611 \n",
      "Average MSE Loss: 2.611 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 24521.017 \n",
      "Average KL Loss: 207.609 \n",
      "Average MSE Loss: 2.405 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 24508.103 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 2.404 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 24557.261 \n",
      "Average KL Loss: 207.611 \n",
      "Average MSE Loss: 2.486 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 24488.174 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 2.455 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 24549.232 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 2.360 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 24549.707 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 2.387 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 24669.184 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 2.403 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 24594.213 \n",
      "Average KL Loss: 207.612 \n",
      "Average MSE Loss: 2.407 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 24605.165 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 2.390 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 24679.041 \n",
      "Average KL Loss: 207.611 \n",
      "Average MSE Loss: 2.367 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 24757.557 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 2.369 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 24653.296 \n",
      "Average KL Loss: 207.609 \n",
      "Average MSE Loss: 2.289 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 24602.115 \n",
      "Average KL Loss: 207.612 \n",
      "Average MSE Loss: 2.294 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 24752.078 \n",
      "Average KL Loss: 207.611 \n",
      "Average MSE Loss: 2.339 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 24598.440 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 2.307 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 24658.343 \n",
      "Average KL Loss: 207.611 \n",
      "Average MSE Loss: 2.328 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 24743.342 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 2.338 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 24709.800 \n",
      "Average KL Loss: 207.610 \n",
      "Average MSE Loss: 2.292 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 24699.883 \n",
      "Average KL Loss: 207.611 \n",
      "Average MSE Loss: 2.146 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 24541.473 \n",
      "Average KL Loss: 207.611 \n",
      "Average MSE Loss: 2.106 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 24736.140 \n",
      "Average KL Loss: 207.611 \n",
      "Average MSE Loss: 2.161 \n",
      "\n",
      "Average Recon Loss: 32279.830 \n",
      "Average KL Loss: 185.668 \n",
      "Average MSE Loss: 3.342 \n",
      "Average RMSE Loss: 1.828 \n",
      "Average R2: 0.592 \n",
      "\n",
      "Fold 4\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 25228.690 \n",
      "Average KL Loss: 207.665 \n",
      "Average MSE Loss: 2.461 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 25259.709 \n",
      "Average KL Loss: 207.993 \n",
      "Average MSE Loss: 2.600 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 24886.182 \n",
      "Average KL Loss: 208.478 \n",
      "Average MSE Loss: 2.524 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 25040.590 \n",
      "Average KL Loss: 208.868 \n",
      "Average MSE Loss: 2.735 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 24753.466 \n",
      "Average KL Loss: 209.311 \n",
      "Average MSE Loss: 2.534 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 24547.481 \n",
      "Average KL Loss: 209.711 \n",
      "Average MSE Loss: 2.550 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 24719.451 \n",
      "Average KL Loss: 210.026 \n",
      "Average MSE Loss: 2.804 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 24366.707 \n",
      "Average KL Loss: 210.376 \n",
      "Average MSE Loss: 2.529 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 24234.493 \n",
      "Average KL Loss: 210.687 \n",
      "Average MSE Loss: 2.627 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 24108.460 \n",
      "Average KL Loss: 210.895 \n",
      "Average MSE Loss: 2.599 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 24008.690 \n",
      "Average KL Loss: 211.055 \n",
      "Average MSE Loss: 2.570 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 23950.486 \n",
      "Average KL Loss: 211.159 \n",
      "Average MSE Loss: 2.548 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 23812.636 \n",
      "Average KL Loss: 211.258 \n",
      "Average MSE Loss: 2.595 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 23762.887 \n",
      "Average KL Loss: 211.405 \n",
      "Average MSE Loss: 2.521 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 23708.581 \n",
      "Average KL Loss: 211.547 \n",
      "Average MSE Loss: 2.638 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 23673.621 \n",
      "Average KL Loss: 211.640 \n",
      "Average MSE Loss: 2.491 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 23739.006 \n",
      "Average KL Loss: 211.762 \n",
      "Average MSE Loss: 2.699 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 23627.197 \n",
      "Average KL Loss: 211.904 \n",
      "Average MSE Loss: 2.633 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 23489.592 \n",
      "Average KL Loss: 212.010 \n",
      "Average MSE Loss: 2.502 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 23475.125 \n",
      "Average KL Loss: 212.052 \n",
      "Average MSE Loss: 2.580 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 23379.434 \n",
      "Average KL Loss: 212.078 \n",
      "Average MSE Loss: 2.646 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 23372.023 \n",
      "Average KL Loss: 212.107 \n",
      "Average MSE Loss: 2.671 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 23334.236 \n",
      "Average KL Loss: 212.096 \n",
      "Average MSE Loss: 2.601 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 23272.085 \n",
      "Average KL Loss: 212.084 \n",
      "Average MSE Loss: 2.659 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 23214.152 \n",
      "Average KL Loss: 212.047 \n",
      "Average MSE Loss: 2.529 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 23169.776 \n",
      "Average KL Loss: 211.980 \n",
      "Average MSE Loss: 2.667 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 23109.878 \n",
      "Average KL Loss: 211.952 \n",
      "Average MSE Loss: 2.698 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 23097.324 \n",
      "Average KL Loss: 211.912 \n",
      "Average MSE Loss: 2.491 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 23046.945 \n",
      "Average KL Loss: 211.863 \n",
      "Average MSE Loss: 2.611 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 23107.957 \n",
      "Average KL Loss: 211.801 \n",
      "Average MSE Loss: 2.574 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 23146.815 \n",
      "Average KL Loss: 211.750 \n",
      "Average MSE Loss: 2.652 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 23029.383 \n",
      "Average KL Loss: 211.700 \n",
      "Average MSE Loss: 2.617 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 22967.388 \n",
      "Average KL Loss: 211.649 \n",
      "Average MSE Loss: 2.622 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 22824.229 \n",
      "Average KL Loss: 211.581 \n",
      "Average MSE Loss: 2.538 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 22993.996 \n",
      "Average KL Loss: 211.510 \n",
      "Average MSE Loss: 2.608 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 22931.628 \n",
      "Average KL Loss: 211.434 \n",
      "Average MSE Loss: 2.513 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 23045.242 \n",
      "Average KL Loss: 211.383 \n",
      "Average MSE Loss: 2.675 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 22768.110 \n",
      "Average KL Loss: 211.360 \n",
      "Average MSE Loss: 2.589 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 22805.587 \n",
      "Average KL Loss: 211.262 \n",
      "Average MSE Loss: 2.589 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 22795.457 \n",
      "Average KL Loss: 211.140 \n",
      "Average MSE Loss: 2.647 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 22791.128 \n",
      "Average KL Loss: 211.014 \n",
      "Average MSE Loss: 2.635 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 22807.967 \n",
      "Average KL Loss: 210.897 \n",
      "Average MSE Loss: 2.571 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 22761.715 \n",
      "Average KL Loss: 210.792 \n",
      "Average MSE Loss: 2.644 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 22722.851 \n",
      "Average KL Loss: 210.671 \n",
      "Average MSE Loss: 2.599 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 22720.694 \n",
      "Average KL Loss: 210.549 \n",
      "Average MSE Loss: 2.494 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 22762.668 \n",
      "Average KL Loss: 210.434 \n",
      "Average MSE Loss: 2.724 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 22825.994 \n",
      "Average KL Loss: 210.308 \n",
      "Average MSE Loss: 2.683 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 22784.348 \n",
      "Average KL Loss: 210.264 \n",
      "Average MSE Loss: 2.730 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 22804.441 \n",
      "Average KL Loss: 210.216 \n",
      "Average MSE Loss: 2.727 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 22750.935 \n",
      "Average KL Loss: 210.167 \n",
      "Average MSE Loss: 2.668 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 22809.546 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 2.540 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 23095.677 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 2.474 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 23297.458 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 2.594 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 23348.750 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 2.475 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 23049.456 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 2.217 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 23155.928 \n",
      "Average KL Loss: 210.141 \n",
      "Average MSE Loss: 2.167 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 23243.755 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 2.140 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 23211.454 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 2.159 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 23119.924 \n",
      "Average KL Loss: 210.141 \n",
      "Average MSE Loss: 2.039 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 23139.660 \n",
      "Average KL Loss: 210.141 \n",
      "Average MSE Loss: 2.191 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 23137.001 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 2.123 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 23239.015 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 2.088 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 23333.535 \n",
      "Average KL Loss: 210.141 \n",
      "Average MSE Loss: 2.101 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 23287.827 \n",
      "Average KL Loss: 210.141 \n",
      "Average MSE Loss: 2.004 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 23190.327 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 2.054 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 23075.874 \n",
      "Average KL Loss: 210.141 \n",
      "Average MSE Loss: 1.922 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 23102.823 \n",
      "Average KL Loss: 210.141 \n",
      "Average MSE Loss: 1.860 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 23148.138 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.973 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 23154.313 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.826 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 23222.063 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.920 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 23282.455 \n",
      "Average KL Loss: 210.141 \n",
      "Average MSE Loss: 1.916 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 23124.059 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.872 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 23201.352 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.895 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 23149.205 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.915 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 23150.584 \n",
      "Average KL Loss: 210.141 \n",
      "Average MSE Loss: 1.892 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 23287.730 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.961 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 23169.169 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.840 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 23207.329 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.921 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 23215.825 \n",
      "Average KL Loss: 210.141 \n",
      "Average MSE Loss: 1.807 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 23106.071 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.839 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 23220.186 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.868 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 23158.983 \n",
      "Average KL Loss: 210.141 \n",
      "Average MSE Loss: 1.870 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 23186.918 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.777 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 23099.181 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.862 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 23226.045 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.957 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 23227.173 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.806 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 23068.547 \n",
      "Average KL Loss: 210.141 \n",
      "Average MSE Loss: 1.705 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 23217.438 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.821 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 23208.997 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.830 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 23129.073 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.638 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 23127.374 \n",
      "Average KL Loss: 210.141 \n",
      "Average MSE Loss: 1.694 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 23299.963 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.720 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 23181.368 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.806 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 23392.832 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.819 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 23190.699 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.634 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 23053.663 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.587 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 23323.949 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.658 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 23200.799 \n",
      "Average KL Loss: 210.141 \n",
      "Average MSE Loss: 1.679 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 23129.924 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.709 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 23186.069 \n",
      "Average KL Loss: 210.142 \n",
      "Average MSE Loss: 1.609 \n",
      "\n",
      "Average Recon Loss: 26285.856 \n",
      "Average KL Loss: 202.615 \n",
      "Average MSE Loss: 2.679 \n",
      "Average RMSE Loss: 1.637 \n",
      "Average R2: 0.671 \n",
      "\n",
      "Fold 5\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 23724.177 \n",
      "Average KL Loss: 210.202 \n",
      "Average MSE Loss: 1.921 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 23828.832 \n",
      "Average KL Loss: 210.358 \n",
      "Average MSE Loss: 1.965 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 23579.021 \n",
      "Average KL Loss: 210.606 \n",
      "Average MSE Loss: 2.036 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 23393.685 \n",
      "Average KL Loss: 210.861 \n",
      "Average MSE Loss: 1.932 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 23203.921 \n",
      "Average KL Loss: 211.016 \n",
      "Average MSE Loss: 2.082 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 23170.664 \n",
      "Average KL Loss: 211.194 \n",
      "Average MSE Loss: 2.078 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 23066.614 \n",
      "Average KL Loss: 211.311 \n",
      "Average MSE Loss: 2.028 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 22903.996 \n",
      "Average KL Loss: 211.339 \n",
      "Average MSE Loss: 1.996 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 22856.112 \n",
      "Average KL Loss: 211.262 \n",
      "Average MSE Loss: 2.041 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 22748.313 \n",
      "Average KL Loss: 211.199 \n",
      "Average MSE Loss: 1.977 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 22717.149 \n",
      "Average KL Loss: 211.053 \n",
      "Average MSE Loss: 2.043 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 22751.081 \n",
      "Average KL Loss: 210.883 \n",
      "Average MSE Loss: 1.945 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 22709.679 \n",
      "Average KL Loss: 210.817 \n",
      "Average MSE Loss: 1.931 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 22661.263 \n",
      "Average KL Loss: 210.708 \n",
      "Average MSE Loss: 1.978 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 22649.334 \n",
      "Average KL Loss: 210.625 \n",
      "Average MSE Loss: 1.981 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 22599.958 \n",
      "Average KL Loss: 210.522 \n",
      "Average MSE Loss: 2.100 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 22626.108 \n",
      "Average KL Loss: 210.364 \n",
      "Average MSE Loss: 1.996 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 22659.117 \n",
      "Average KL Loss: 210.209 \n",
      "Average MSE Loss: 1.981 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 22631.752 \n",
      "Average KL Loss: 210.058 \n",
      "Average MSE Loss: 2.000 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 22615.115 \n",
      "Average KL Loss: 209.901 \n",
      "Average MSE Loss: 1.972 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 22465.173 \n",
      "Average KL Loss: 209.727 \n",
      "Average MSE Loss: 1.930 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 22404.554 \n",
      "Average KL Loss: 209.550 \n",
      "Average MSE Loss: 1.928 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 22411.705 \n",
      "Average KL Loss: 209.330 \n",
      "Average MSE Loss: 1.981 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 22350.810 \n",
      "Average KL Loss: 209.140 \n",
      "Average MSE Loss: 2.146 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 22387.182 \n",
      "Average KL Loss: 208.943 \n",
      "Average MSE Loss: 1.924 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 22423.393 \n",
      "Average KL Loss: 208.783 \n",
      "Average MSE Loss: 2.078 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 22503.408 \n",
      "Average KL Loss: 208.636 \n",
      "Average MSE Loss: 2.023 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 22351.781 \n",
      "Average KL Loss: 208.476 \n",
      "Average MSE Loss: 1.949 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 22548.650 \n",
      "Average KL Loss: 208.354 \n",
      "Average MSE Loss: 1.990 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 22311.989 \n",
      "Average KL Loss: 208.233 \n",
      "Average MSE Loss: 2.009 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 22242.582 \n",
      "Average KL Loss: 208.043 \n",
      "Average MSE Loss: 1.969 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 22164.473 \n",
      "Average KL Loss: 207.830 \n",
      "Average MSE Loss: 1.942 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 22265.869 \n",
      "Average KL Loss: 207.617 \n",
      "Average MSE Loss: 2.015 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 22212.616 \n",
      "Average KL Loss: 207.438 \n",
      "Average MSE Loss: 1.916 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 22300.961 \n",
      "Average KL Loss: 207.281 \n",
      "Average MSE Loss: 1.943 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 22274.861 \n",
      "Average KL Loss: 207.118 \n",
      "Average MSE Loss: 2.085 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 22218.185 \n",
      "Average KL Loss: 206.938 \n",
      "Average MSE Loss: 1.976 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 22225.547 \n",
      "Average KL Loss: 206.739 \n",
      "Average MSE Loss: 2.055 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 22201.222 \n",
      "Average KL Loss: 206.517 \n",
      "Average MSE Loss: 2.028 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 22177.994 \n",
      "Average KL Loss: 206.307 \n",
      "Average MSE Loss: 1.935 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 22259.335 \n",
      "Average KL Loss: 206.081 \n",
      "Average MSE Loss: 1.977 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 22266.784 \n",
      "Average KL Loss: 205.906 \n",
      "Average MSE Loss: 2.012 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 22245.916 \n",
      "Average KL Loss: 205.809 \n",
      "Average MSE Loss: 2.022 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 22049.479 \n",
      "Average KL Loss: 205.645 \n",
      "Average MSE Loss: 1.898 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 22244.091 \n",
      "Average KL Loss: 205.433 \n",
      "Average MSE Loss: 1.978 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 22077.979 \n",
      "Average KL Loss: 205.241 \n",
      "Average MSE Loss: 1.953 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 22128.077 \n",
      "Average KL Loss: 205.038 \n",
      "Average MSE Loss: 2.051 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 22126.665 \n",
      "Average KL Loss: 204.824 \n",
      "Average MSE Loss: 2.060 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 22215.258 \n",
      "Average KL Loss: 204.650 \n",
      "Average MSE Loss: 2.080 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 22057.685 \n",
      "Average KL Loss: 204.481 \n",
      "Average MSE Loss: 2.035 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 22139.746 \n",
      "Average KL Loss: 204.318 \n",
      "Average MSE Loss: 2.027 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 22472.016 \n",
      "Average KL Loss: 204.275 \n",
      "Average MSE Loss: 1.967 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 22564.433 \n",
      "Average KL Loss: 204.267 \n",
      "Average MSE Loss: 1.883 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 22451.863 \n",
      "Average KL Loss: 204.266 \n",
      "Average MSE Loss: 1.691 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 22410.655 \n",
      "Average KL Loss: 204.266 \n",
      "Average MSE Loss: 1.755 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 22495.027 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.833 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 22439.022 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.725 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 22524.573 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.713 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 22437.609 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.690 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 22685.715 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.951 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 22579.164 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.726 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 22451.388 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.601 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 22517.270 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.645 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 22378.152 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.627 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 22454.252 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.615 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 22578.279 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.701 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 22454.767 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.711 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 22658.130 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.746 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 22584.051 \n",
      "Average KL Loss: 204.266 \n",
      "Average MSE Loss: 1.647 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 22392.758 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.620 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 22498.776 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.584 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 22445.663 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.557 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 22462.526 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.574 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 22586.930 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.657 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 22552.197 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.539 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 22498.752 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.614 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 22517.215 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.554 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 22500.265 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.503 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 22503.980 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.614 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 22473.027 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.577 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 22428.314 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.468 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 22493.928 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.540 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 22433.984 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.446 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 22527.284 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.702 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 22570.497 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.545 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 22482.610 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.636 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 22431.842 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.443 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 22552.635 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.565 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 22480.144 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.437 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 22526.182 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.564 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 22516.422 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.432 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 22440.774 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.412 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 22424.183 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.389 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 22400.759 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.446 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 22605.758 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.471 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 22405.931 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.497 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 22419.179 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.492 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 22468.849 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.399 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 22481.550 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.474 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 22611.384 \n",
      "Average KL Loss: 204.265 \n",
      "Average MSE Loss: 1.484 \n",
      "\n",
      "Average Recon Loss: 24238.190 \n",
      "Average KL Loss: 198.315 \n",
      "Average MSE Loss: 2.433 \n",
      "Average RMSE Loss: 1.560 \n",
      "Average R2: 0.742 \n",
      "\n",
      "Fold 6\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 22988.936 \n",
      "Average KL Loss: 204.346 \n",
      "Average MSE Loss: 1.715 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 23091.489 \n",
      "Average KL Loss: 204.636 \n",
      "Average MSE Loss: 1.988 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 22823.759 \n",
      "Average KL Loss: 204.892 \n",
      "Average MSE Loss: 1.810 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 22711.313 \n",
      "Average KL Loss: 205.054 \n",
      "Average MSE Loss: 1.826 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 22557.400 \n",
      "Average KL Loss: 205.076 \n",
      "Average MSE Loss: 1.735 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 22459.518 \n",
      "Average KL Loss: 205.125 \n",
      "Average MSE Loss: 1.844 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 22342.307 \n",
      "Average KL Loss: 205.137 \n",
      "Average MSE Loss: 1.802 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 22184.421 \n",
      "Average KL Loss: 205.053 \n",
      "Average MSE Loss: 1.850 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 22200.035 \n",
      "Average KL Loss: 204.900 \n",
      "Average MSE Loss: 1.853 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 22132.589 \n",
      "Average KL Loss: 204.763 \n",
      "Average MSE Loss: 1.745 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 22199.557 \n",
      "Average KL Loss: 204.663 \n",
      "Average MSE Loss: 1.853 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 22219.499 \n",
      "Average KL Loss: 204.556 \n",
      "Average MSE Loss: 1.829 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 22121.897 \n",
      "Average KL Loss: 204.437 \n",
      "Average MSE Loss: 1.954 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 22043.567 \n",
      "Average KL Loss: 204.334 \n",
      "Average MSE Loss: 1.679 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 22019.631 \n",
      "Average KL Loss: 204.110 \n",
      "Average MSE Loss: 1.820 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 22104.551 \n",
      "Average KL Loss: 203.878 \n",
      "Average MSE Loss: 1.719 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 22085.630 \n",
      "Average KL Loss: 203.657 \n",
      "Average MSE Loss: 1.731 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 22085.070 \n",
      "Average KL Loss: 203.482 \n",
      "Average MSE Loss: 1.776 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 22082.254 \n",
      "Average KL Loss: 203.338 \n",
      "Average MSE Loss: 1.906 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 22045.610 \n",
      "Average KL Loss: 203.188 \n",
      "Average MSE Loss: 1.829 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 22067.467 \n",
      "Average KL Loss: 203.020 \n",
      "Average MSE Loss: 1.751 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 21987.470 \n",
      "Average KL Loss: 202.847 \n",
      "Average MSE Loss: 1.836 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 21959.751 \n",
      "Average KL Loss: 202.667 \n",
      "Average MSE Loss: 1.716 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 21962.842 \n",
      "Average KL Loss: 202.508 \n",
      "Average MSE Loss: 1.695 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 21925.752 \n",
      "Average KL Loss: 202.381 \n",
      "Average MSE Loss: 1.805 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 21891.761 \n",
      "Average KL Loss: 202.186 \n",
      "Average MSE Loss: 1.765 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 21993.419 \n",
      "Average KL Loss: 202.006 \n",
      "Average MSE Loss: 1.906 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 21978.859 \n",
      "Average KL Loss: 201.874 \n",
      "Average MSE Loss: 1.807 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 21879.293 \n",
      "Average KL Loss: 201.727 \n",
      "Average MSE Loss: 1.733 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 21854.957 \n",
      "Average KL Loss: 201.571 \n",
      "Average MSE Loss: 1.845 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 21996.117 \n",
      "Average KL Loss: 201.401 \n",
      "Average MSE Loss: 1.750 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 21870.636 \n",
      "Average KL Loss: 201.152 \n",
      "Average MSE Loss: 1.738 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 21897.499 \n",
      "Average KL Loss: 200.884 \n",
      "Average MSE Loss: 1.767 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 21844.229 \n",
      "Average KL Loss: 200.669 \n",
      "Average MSE Loss: 1.732 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 21810.528 \n",
      "Average KL Loss: 200.454 \n",
      "Average MSE Loss: 1.747 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 21878.032 \n",
      "Average KL Loss: 200.211 \n",
      "Average MSE Loss: 1.814 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 21747.216 \n",
      "Average KL Loss: 199.998 \n",
      "Average MSE Loss: 1.849 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 21727.145 \n",
      "Average KL Loss: 199.783 \n",
      "Average MSE Loss: 1.649 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 21736.929 \n",
      "Average KL Loss: 199.551 \n",
      "Average MSE Loss: 1.739 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 21787.300 \n",
      "Average KL Loss: 199.349 \n",
      "Average MSE Loss: 1.753 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 21793.465 \n",
      "Average KL Loss: 199.161 \n",
      "Average MSE Loss: 1.785 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 21797.545 \n",
      "Average KL Loss: 198.930 \n",
      "Average MSE Loss: 1.755 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 21890.053 \n",
      "Average KL Loss: 198.715 \n",
      "Average MSE Loss: 1.704 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 21859.661 \n",
      "Average KL Loss: 198.526 \n",
      "Average MSE Loss: 1.793 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 21834.383 \n",
      "Average KL Loss: 198.375 \n",
      "Average MSE Loss: 1.826 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 21744.660 \n",
      "Average KL Loss: 198.222 \n",
      "Average MSE Loss: 1.695 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 21830.305 \n",
      "Average KL Loss: 198.006 \n",
      "Average MSE Loss: 1.807 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 21668.938 \n",
      "Average KL Loss: 197.834 \n",
      "Average MSE Loss: 1.857 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 21798.445 \n",
      "Average KL Loss: 197.661 \n",
      "Average MSE Loss: 1.778 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 21738.084 \n",
      "Average KL Loss: 197.529 \n",
      "Average MSE Loss: 1.807 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 21728.379 \n",
      "Average KL Loss: 197.381 \n",
      "Average MSE Loss: 1.769 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 22071.769 \n",
      "Average KL Loss: 197.337 \n",
      "Average MSE Loss: 1.704 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 22087.656 \n",
      "Average KL Loss: 197.328 \n",
      "Average MSE Loss: 1.711 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 22112.545 \n",
      "Average KL Loss: 197.327 \n",
      "Average MSE Loss: 1.705 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 22053.236 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.609 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 22101.169 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.585 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 22192.774 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.653 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 22116.016 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.531 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 22188.855 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.632 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 22094.246 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.598 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 22062.563 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.587 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 21972.500 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.545 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 22144.688 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.644 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 22057.475 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.663 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 22118.834 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.632 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 22089.653 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.472 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 22037.372 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.579 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 22051.720 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.551 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 22118.760 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.530 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 22032.024 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.425 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 22024.262 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.455 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 22070.676 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.509 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 22096.307 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.538 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 22153.999 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.360 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 22084.391 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.409 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 22049.730 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.447 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 22084.881 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.502 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 21969.469 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.429 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 21986.397 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.477 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 22022.796 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.447 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 22072.625 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.315 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 22020.521 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.332 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 22047.913 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.456 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 22034.031 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.350 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 22093.276 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.400 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 22013.337 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.384 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 22032.070 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.394 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 22049.795 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.411 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 22146.093 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.372 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 22164.893 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.384 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 21986.468 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.314 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 22144.037 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.375 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 22147.001 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.304 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 22129.731 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.340 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 22032.155 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.376 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 22143.178 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.358 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 21987.554 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.439 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 21986.491 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.371 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 22108.229 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.513 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 22097.776 \n",
      "Average KL Loss: 197.326 \n",
      "Average MSE Loss: 1.365 \n",
      "\n",
      "Average Recon Loss: 22545.541 \n",
      "Average KL Loss: 192.190 \n",
      "Average MSE Loss: 1.909 \n",
      "Average RMSE Loss: 1.382 \n",
      "Average R2: 0.802 \n",
      "\n",
      "Fold 7\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 22178.701 \n",
      "Average KL Loss: 197.367 \n",
      "Average MSE Loss: 1.749 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 22161.908 \n",
      "Average KL Loss: 197.526 \n",
      "Average MSE Loss: 1.810 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 22158.995 \n",
      "Average KL Loss: 197.772 \n",
      "Average MSE Loss: 1.782 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 21998.323 \n",
      "Average KL Loss: 197.978 \n",
      "Average MSE Loss: 1.879 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 21897.287 \n",
      "Average KL Loss: 198.092 \n",
      "Average MSE Loss: 1.857 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 21701.115 \n",
      "Average KL Loss: 198.049 \n",
      "Average MSE Loss: 1.794 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 21816.866 \n",
      "Average KL Loss: 197.911 \n",
      "Average MSE Loss: 1.846 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 21630.829 \n",
      "Average KL Loss: 197.764 \n",
      "Average MSE Loss: 1.866 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 21827.173 \n",
      "Average KL Loss: 197.608 \n",
      "Average MSE Loss: 1.873 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 21717.078 \n",
      "Average KL Loss: 197.595 \n",
      "Average MSE Loss: 1.802 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 21788.861 \n",
      "Average KL Loss: 197.565 \n",
      "Average MSE Loss: 1.725 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 21698.794 \n",
      "Average KL Loss: 197.520 \n",
      "Average MSE Loss: 1.807 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 21615.864 \n",
      "Average KL Loss: 197.402 \n",
      "Average MSE Loss: 1.781 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 21630.516 \n",
      "Average KL Loss: 197.249 \n",
      "Average MSE Loss: 1.824 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 21751.312 \n",
      "Average KL Loss: 197.114 \n",
      "Average MSE Loss: 1.813 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 21688.122 \n",
      "Average KL Loss: 197.007 \n",
      "Average MSE Loss: 1.826 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 21701.518 \n",
      "Average KL Loss: 196.894 \n",
      "Average MSE Loss: 1.803 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 21569.675 \n",
      "Average KL Loss: 196.842 \n",
      "Average MSE Loss: 1.776 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 21478.652 \n",
      "Average KL Loss: 196.705 \n",
      "Average MSE Loss: 1.735 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 21537.651 \n",
      "Average KL Loss: 196.523 \n",
      "Average MSE Loss: 1.863 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 21507.719 \n",
      "Average KL Loss: 196.285 \n",
      "Average MSE Loss: 1.805 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 21657.058 \n",
      "Average KL Loss: 196.093 \n",
      "Average MSE Loss: 1.891 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 21579.366 \n",
      "Average KL Loss: 195.997 \n",
      "Average MSE Loss: 1.775 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 21627.436 \n",
      "Average KL Loss: 195.856 \n",
      "Average MSE Loss: 1.758 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 21466.214 \n",
      "Average KL Loss: 195.707 \n",
      "Average MSE Loss: 1.836 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 21431.036 \n",
      "Average KL Loss: 195.442 \n",
      "Average MSE Loss: 1.799 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 21498.601 \n",
      "Average KL Loss: 195.224 \n",
      "Average MSE Loss: 1.704 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 21567.753 \n",
      "Average KL Loss: 195.070 \n",
      "Average MSE Loss: 1.813 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 21481.280 \n",
      "Average KL Loss: 194.936 \n",
      "Average MSE Loss: 1.776 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 21449.905 \n",
      "Average KL Loss: 194.791 \n",
      "Average MSE Loss: 1.757 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 21439.095 \n",
      "Average KL Loss: 194.569 \n",
      "Average MSE Loss: 1.795 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 21486.806 \n",
      "Average KL Loss: 194.396 \n",
      "Average MSE Loss: 1.827 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 21430.135 \n",
      "Average KL Loss: 194.187 \n",
      "Average MSE Loss: 1.790 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 21440.193 \n",
      "Average KL Loss: 193.972 \n",
      "Average MSE Loss: 1.707 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 21338.483 \n",
      "Average KL Loss: 193.736 \n",
      "Average MSE Loss: 1.710 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 21449.698 \n",
      "Average KL Loss: 193.539 \n",
      "Average MSE Loss: 1.769 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 21485.807 \n",
      "Average KL Loss: 193.401 \n",
      "Average MSE Loss: 1.788 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 21509.166 \n",
      "Average KL Loss: 193.261 \n",
      "Average MSE Loss: 1.763 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 21546.658 \n",
      "Average KL Loss: 193.094 \n",
      "Average MSE Loss: 1.949 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 21469.858 \n",
      "Average KL Loss: 192.995 \n",
      "Average MSE Loss: 1.694 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 21557.201 \n",
      "Average KL Loss: 192.836 \n",
      "Average MSE Loss: 1.820 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 21486.083 \n",
      "Average KL Loss: 192.724 \n",
      "Average MSE Loss: 1.853 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 21362.467 \n",
      "Average KL Loss: 192.595 \n",
      "Average MSE Loss: 1.707 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 21463.596 \n",
      "Average KL Loss: 192.512 \n",
      "Average MSE Loss: 1.766 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 21374.274 \n",
      "Average KL Loss: 192.398 \n",
      "Average MSE Loss: 1.734 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 21344.486 \n",
      "Average KL Loss: 192.229 \n",
      "Average MSE Loss: 1.877 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 21355.981 \n",
      "Average KL Loss: 192.017 \n",
      "Average MSE Loss: 1.740 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 21395.561 \n",
      "Average KL Loss: 191.834 \n",
      "Average MSE Loss: 1.758 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 21346.669 \n",
      "Average KL Loss: 191.652 \n",
      "Average MSE Loss: 1.804 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 21496.957 \n",
      "Average KL Loss: 191.486 \n",
      "Average MSE Loss: 1.826 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 21561.297 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.704 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 21649.694 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.649 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 21682.729 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.656 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 21601.241 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.592 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 21789.564 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.502 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 21851.514 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.511 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 21693.463 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.513 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 21747.755 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.533 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 21726.462 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.467 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 21650.857 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.451 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 21747.823 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.497 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 21560.246 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.405 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 21703.252 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.522 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 21787.716 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.497 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 21734.389 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.498 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 21710.513 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.428 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 21732.680 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.419 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 21758.654 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.428 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 21720.804 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.458 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 21717.504 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.406 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 21750.185 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.327 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 21632.185 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.390 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 21760.987 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.404 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 21786.289 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.413 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 21742.553 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.354 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 21721.645 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.480 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 21651.134 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.383 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 21680.041 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.313 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 21689.424 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.567 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 21721.295 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.371 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 21738.067 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.365 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 21702.645 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.322 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 21904.600 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.408 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 21702.998 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.325 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 21635.291 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.401 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 21790.028 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.566 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 21801.135 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.439 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 21739.910 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.346 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 21702.409 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.370 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 21622.895 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.318 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 21644.880 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.329 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 21676.575 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.294 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 21843.829 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.336 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 21717.910 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.348 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 21728.601 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.390 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 21661.915 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.384 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 21761.995 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.372 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 21738.291 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.310 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 21740.543 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.287 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 21781.658 \n",
      "Average KL Loss: 191.449 \n",
      "Average MSE Loss: 1.393 \n",
      "\n",
      "Average Recon Loss: 23485.041 \n",
      "Average KL Loss: 182.476 \n",
      "Average MSE Loss: 2.013 \n",
      "Average RMSE Loss: 1.419 \n",
      "Average R2: 0.786 \n",
      "\n",
      "Fold 8\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 21851.667 \n",
      "Average KL Loss: 191.489 \n",
      "Average MSE Loss: 1.617 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 21791.575 \n",
      "Average KL Loss: 191.472 \n",
      "Average MSE Loss: 1.649 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 21786.078 \n",
      "Average KL Loss: 191.480 \n",
      "Average MSE Loss: 1.647 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 21753.339 \n",
      "Average KL Loss: 191.638 \n",
      "Average MSE Loss: 1.688 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 21637.886 \n",
      "Average KL Loss: 191.782 \n",
      "Average MSE Loss: 1.583 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 21635.967 \n",
      "Average KL Loss: 191.804 \n",
      "Average MSE Loss: 1.585 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 21705.650 \n",
      "Average KL Loss: 191.803 \n",
      "Average MSE Loss: 1.648 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 21555.102 \n",
      "Average KL Loss: 191.842 \n",
      "Average MSE Loss: 1.619 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 21475.822 \n",
      "Average KL Loss: 191.821 \n",
      "Average MSE Loss: 1.731 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 21487.781 \n",
      "Average KL Loss: 191.784 \n",
      "Average MSE Loss: 1.589 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 21559.230 \n",
      "Average KL Loss: 191.725 \n",
      "Average MSE Loss: 1.674 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 21578.909 \n",
      "Average KL Loss: 191.700 \n",
      "Average MSE Loss: 1.557 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 21480.563 \n",
      "Average KL Loss: 191.614 \n",
      "Average MSE Loss: 1.637 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 21536.103 \n",
      "Average KL Loss: 191.482 \n",
      "Average MSE Loss: 1.680 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 21507.514 \n",
      "Average KL Loss: 191.368 \n",
      "Average MSE Loss: 1.510 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 21476.501 \n",
      "Average KL Loss: 191.208 \n",
      "Average MSE Loss: 1.655 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 21493.830 \n",
      "Average KL Loss: 190.981 \n",
      "Average MSE Loss: 1.577 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 21504.028 \n",
      "Average KL Loss: 190.881 \n",
      "Average MSE Loss: 1.680 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 21571.923 \n",
      "Average KL Loss: 190.907 \n",
      "Average MSE Loss: 1.749 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 21575.031 \n",
      "Average KL Loss: 190.891 \n",
      "Average MSE Loss: 1.688 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 21522.641 \n",
      "Average KL Loss: 190.901 \n",
      "Average MSE Loss: 1.574 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 21580.982 \n",
      "Average KL Loss: 190.899 \n",
      "Average MSE Loss: 1.750 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 21430.690 \n",
      "Average KL Loss: 190.908 \n",
      "Average MSE Loss: 1.645 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 21373.070 \n",
      "Average KL Loss: 190.826 \n",
      "Average MSE Loss: 1.728 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 21421.472 \n",
      "Average KL Loss: 190.652 \n",
      "Average MSE Loss: 1.614 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 21378.997 \n",
      "Average KL Loss: 190.556 \n",
      "Average MSE Loss: 1.665 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 21413.038 \n",
      "Average KL Loss: 190.466 \n",
      "Average MSE Loss: 1.579 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 21516.310 \n",
      "Average KL Loss: 190.368 \n",
      "Average MSE Loss: 1.725 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 21391.523 \n",
      "Average KL Loss: 190.297 \n",
      "Average MSE Loss: 1.738 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 21422.997 \n",
      "Average KL Loss: 190.179 \n",
      "Average MSE Loss: 1.707 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 21312.209 \n",
      "Average KL Loss: 190.017 \n",
      "Average MSE Loss: 1.548 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 21360.890 \n",
      "Average KL Loss: 189.815 \n",
      "Average MSE Loss: 1.590 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 21336.079 \n",
      "Average KL Loss: 189.639 \n",
      "Average MSE Loss: 1.677 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 21345.233 \n",
      "Average KL Loss: 189.504 \n",
      "Average MSE Loss: 1.675 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 21274.935 \n",
      "Average KL Loss: 189.321 \n",
      "Average MSE Loss: 1.596 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 21295.423 \n",
      "Average KL Loss: 189.146 \n",
      "Average MSE Loss: 1.561 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 21338.969 \n",
      "Average KL Loss: 188.993 \n",
      "Average MSE Loss: 1.679 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 21357.857 \n",
      "Average KL Loss: 188.802 \n",
      "Average MSE Loss: 1.707 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 21213.442 \n",
      "Average KL Loss: 188.609 \n",
      "Average MSE Loss: 1.588 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 21274.205 \n",
      "Average KL Loss: 188.416 \n",
      "Average MSE Loss: 1.656 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 21296.698 \n",
      "Average KL Loss: 188.278 \n",
      "Average MSE Loss: 1.714 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 21251.685 \n",
      "Average KL Loss: 188.064 \n",
      "Average MSE Loss: 1.702 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 21321.695 \n",
      "Average KL Loss: 187.872 \n",
      "Average MSE Loss: 1.690 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 21228.824 \n",
      "Average KL Loss: 187.717 \n",
      "Average MSE Loss: 1.658 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 21257.286 \n",
      "Average KL Loss: 187.596 \n",
      "Average MSE Loss: 1.653 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 21142.318 \n",
      "Average KL Loss: 187.423 \n",
      "Average MSE Loss: 1.499 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 21199.600 \n",
      "Average KL Loss: 187.198 \n",
      "Average MSE Loss: 1.561 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 21264.383 \n",
      "Average KL Loss: 186.985 \n",
      "Average MSE Loss: 1.522 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 21211.192 \n",
      "Average KL Loss: 186.789 \n",
      "Average MSE Loss: 1.639 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 21234.208 \n",
      "Average KL Loss: 186.574 \n",
      "Average MSE Loss: 1.641 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 21418.077 \n",
      "Average KL Loss: 186.419 \n",
      "Average MSE Loss: 1.591 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 21403.432 \n",
      "Average KL Loss: 186.391 \n",
      "Average MSE Loss: 1.653 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 21414.712 \n",
      "Average KL Loss: 186.386 \n",
      "Average MSE Loss: 1.467 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 21384.648 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.487 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 21454.221 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.476 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 21431.886 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.399 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 21392.696 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.508 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 21454.614 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.448 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 21518.771 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.417 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 21485.391 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.457 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 21378.497 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.341 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 21493.271 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.440 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 21431.264 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.515 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 21364.728 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.331 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 21406.252 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.335 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 21423.271 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.359 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 21484.146 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.446 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 21421.685 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.388 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 21507.040 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.373 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 21359.851 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.336 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 21431.578 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.243 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 21374.369 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.319 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 21487.489 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.406 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 21411.356 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.312 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 21436.585 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.372 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 21498.972 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.475 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 21284.074 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.368 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 21350.711 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.398 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 21481.846 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.357 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 21481.724 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.398 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 21562.384 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.399 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 21499.118 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.398 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 21513.449 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.354 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 21442.781 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.356 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 21464.837 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.259 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 21373.215 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.350 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 21460.590 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.370 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 21457.941 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.407 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 21474.951 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.391 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 21350.596 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.282 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 21654.583 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.410 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 21406.702 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.295 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 21491.317 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.270 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 21279.726 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.291 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 21445.462 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.287 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 21462.851 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.320 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 21661.118 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.466 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 21482.597 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.297 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 21401.884 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.304 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 21448.990 \n",
      "Average KL Loss: 186.385 \n",
      "Average MSE Loss: 1.347 \n",
      "\n",
      "Average Recon Loss: 23068.071 \n",
      "Average KL Loss: 177.290 \n",
      "Average MSE Loss: 1.583 \n",
      "Average RMSE Loss: 1.258 \n",
      "Average R2: 0.803 \n",
      "\n",
      "Fold 9\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 21591.790 \n",
      "Average KL Loss: 186.429 \n",
      "Average MSE Loss: 1.502 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 21652.607 \n",
      "Average KL Loss: 186.565 \n",
      "Average MSE Loss: 1.812 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 21616.246 \n",
      "Average KL Loss: 186.843 \n",
      "Average MSE Loss: 1.743 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 21629.588 \n",
      "Average KL Loss: 187.133 \n",
      "Average MSE Loss: 1.853 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 21426.674 \n",
      "Average KL Loss: 187.343 \n",
      "Average MSE Loss: 1.784 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 21428.537 \n",
      "Average KL Loss: 187.523 \n",
      "Average MSE Loss: 1.742 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 21346.182 \n",
      "Average KL Loss: 187.533 \n",
      "Average MSE Loss: 1.694 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 21222.533 \n",
      "Average KL Loss: 187.468 \n",
      "Average MSE Loss: 1.660 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 21290.079 \n",
      "Average KL Loss: 187.371 \n",
      "Average MSE Loss: 1.705 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 21312.417 \n",
      "Average KL Loss: 187.287 \n",
      "Average MSE Loss: 1.823 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 21380.533 \n",
      "Average KL Loss: 187.219 \n",
      "Average MSE Loss: 1.743 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 21205.973 \n",
      "Average KL Loss: 187.129 \n",
      "Average MSE Loss: 1.690 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 21252.736 \n",
      "Average KL Loss: 186.999 \n",
      "Average MSE Loss: 1.835 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 21230.744 \n",
      "Average KL Loss: 186.893 \n",
      "Average MSE Loss: 1.814 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 21347.997 \n",
      "Average KL Loss: 186.799 \n",
      "Average MSE Loss: 1.785 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 21292.683 \n",
      "Average KL Loss: 186.770 \n",
      "Average MSE Loss: 1.800 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 21313.789 \n",
      "Average KL Loss: 186.667 \n",
      "Average MSE Loss: 1.860 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 21245.194 \n",
      "Average KL Loss: 186.597 \n",
      "Average MSE Loss: 1.837 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 21302.545 \n",
      "Average KL Loss: 186.502 \n",
      "Average MSE Loss: 1.779 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 21270.072 \n",
      "Average KL Loss: 186.412 \n",
      "Average MSE Loss: 1.728 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 21230.506 \n",
      "Average KL Loss: 186.289 \n",
      "Average MSE Loss: 1.810 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 21161.451 \n",
      "Average KL Loss: 186.158 \n",
      "Average MSE Loss: 1.700 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 21232.790 \n",
      "Average KL Loss: 186.053 \n",
      "Average MSE Loss: 1.846 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 21196.463 \n",
      "Average KL Loss: 185.987 \n",
      "Average MSE Loss: 1.762 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 21120.504 \n",
      "Average KL Loss: 185.847 \n",
      "Average MSE Loss: 1.742 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 21091.687 \n",
      "Average KL Loss: 185.694 \n",
      "Average MSE Loss: 1.798 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 21024.599 \n",
      "Average KL Loss: 185.525 \n",
      "Average MSE Loss: 1.720 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 21227.494 \n",
      "Average KL Loss: 185.314 \n",
      "Average MSE Loss: 1.776 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 21202.117 \n",
      "Average KL Loss: 185.248 \n",
      "Average MSE Loss: 1.730 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 21169.508 \n",
      "Average KL Loss: 185.175 \n",
      "Average MSE Loss: 1.850 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 21081.240 \n",
      "Average KL Loss: 185.005 \n",
      "Average MSE Loss: 1.729 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 21084.042 \n",
      "Average KL Loss: 184.807 \n",
      "Average MSE Loss: 1.744 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 21114.842 \n",
      "Average KL Loss: 184.573 \n",
      "Average MSE Loss: 1.865 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 21066.414 \n",
      "Average KL Loss: 184.436 \n",
      "Average MSE Loss: 1.627 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 21081.216 \n",
      "Average KL Loss: 184.243 \n",
      "Average MSE Loss: 1.737 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 21127.674 \n",
      "Average KL Loss: 184.052 \n",
      "Average MSE Loss: 1.670 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 21178.520 \n",
      "Average KL Loss: 183.904 \n",
      "Average MSE Loss: 1.805 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 21206.688 \n",
      "Average KL Loss: 183.746 \n",
      "Average MSE Loss: 1.739 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 21033.519 \n",
      "Average KL Loss: 183.623 \n",
      "Average MSE Loss: 1.717 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 21107.193 \n",
      "Average KL Loss: 183.463 \n",
      "Average MSE Loss: 1.971 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 21183.448 \n",
      "Average KL Loss: 183.322 \n",
      "Average MSE Loss: 1.632 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 21078.403 \n",
      "Average KL Loss: 183.160 \n",
      "Average MSE Loss: 1.703 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 21037.337 \n",
      "Average KL Loss: 183.026 \n",
      "Average MSE Loss: 1.747 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 21130.078 \n",
      "Average KL Loss: 182.856 \n",
      "Average MSE Loss: 1.658 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 21144.992 \n",
      "Average KL Loss: 182.673 \n",
      "Average MSE Loss: 1.713 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 21215.176 \n",
      "Average KL Loss: 182.563 \n",
      "Average MSE Loss: 1.642 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 21099.801 \n",
      "Average KL Loss: 182.452 \n",
      "Average MSE Loss: 1.631 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 21044.913 \n",
      "Average KL Loss: 182.318 \n",
      "Average MSE Loss: 1.748 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 21141.013 \n",
      "Average KL Loss: 182.175 \n",
      "Average MSE Loss: 1.761 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 21054.370 \n",
      "Average KL Loss: 182.051 \n",
      "Average MSE Loss: 1.744 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 21155.989 \n",
      "Average KL Loss: 181.922 \n",
      "Average MSE Loss: 1.778 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 21262.761 \n",
      "Average KL Loss: 181.893 \n",
      "Average MSE Loss: 1.623 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 21216.227 \n",
      "Average KL Loss: 181.887 \n",
      "Average MSE Loss: 1.508 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 21133.029 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.387 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 21261.159 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.545 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 21208.677 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.455 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 21214.929 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.425 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 21217.213 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.418 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 21405.881 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.465 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 21336.179 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.434 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 21176.600 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.436 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 21269.859 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.560 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 21218.629 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.408 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 21246.145 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.464 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 21181.275 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.272 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 21237.485 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.379 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 21185.183 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.298 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 21181.872 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.391 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 21161.259 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.320 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 21213.500 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.221 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 21303.733 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.295 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 21269.681 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.254 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 21266.535 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.350 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 21231.358 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.265 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 21227.476 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.454 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 21278.741 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.361 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 21176.208 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.270 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 21266.910 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.387 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 21151.725 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.327 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 21173.571 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.265 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 21216.853 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.369 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 21204.925 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.371 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 21224.589 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.228 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 21238.558 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.254 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 21358.355 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.253 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 21209.099 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.378 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 21289.254 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.256 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 21162.788 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.314 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 21207.300 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.262 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 21429.727 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.349 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 21270.733 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.244 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 21289.640 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.370 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 21219.482 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.264 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 21210.992 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.262 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 21144.967 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.341 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 21212.025 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.317 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 21295.839 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.196 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 21200.765 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.290 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 21330.131 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.366 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 21151.354 \n",
      "Average KL Loss: 181.886 \n",
      "Average MSE Loss: 1.189 \n",
      "\n",
      "Average Recon Loss: 23525.567 \n",
      "Average KL Loss: 173.683 \n",
      "Average MSE Loss: 1.604 \n",
      "Average RMSE Loss: 1.267 \n",
      "Average R2: 0.821 \n",
      "\n",
      "Fold 10\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 21455.710 \n",
      "Average KL Loss: 181.890 \n",
      "Average MSE Loss: 1.330 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 21476.855 \n",
      "Average KL Loss: 182.045 \n",
      "Average MSE Loss: 1.431 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 21380.939 \n",
      "Average KL Loss: 182.163 \n",
      "Average MSE Loss: 1.467 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 21351.017 \n",
      "Average KL Loss: 182.155 \n",
      "Average MSE Loss: 1.442 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 21396.623 \n",
      "Average KL Loss: 182.196 \n",
      "Average MSE Loss: 1.483 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 21326.078 \n",
      "Average KL Loss: 182.184 \n",
      "Average MSE Loss: 1.568 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 21386.586 \n",
      "Average KL Loss: 182.262 \n",
      "Average MSE Loss: 1.430 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 21366.358 \n",
      "Average KL Loss: 182.356 \n",
      "Average MSE Loss: 1.483 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 21510.517 \n",
      "Average KL Loss: 182.409 \n",
      "Average MSE Loss: 1.547 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 21280.232 \n",
      "Average KL Loss: 182.454 \n",
      "Average MSE Loss: 1.448 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 21239.569 \n",
      "Average KL Loss: 182.422 \n",
      "Average MSE Loss: 1.552 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 21554.098 \n",
      "Average KL Loss: 182.371 \n",
      "Average MSE Loss: 1.571 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 21234.840 \n",
      "Average KL Loss: 182.367 \n",
      "Average MSE Loss: 1.425 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 21293.501 \n",
      "Average KL Loss: 182.309 \n",
      "Average MSE Loss: 1.548 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 21171.067 \n",
      "Average KL Loss: 182.285 \n",
      "Average MSE Loss: 1.418 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 21222.521 \n",
      "Average KL Loss: 182.181 \n",
      "Average MSE Loss: 1.396 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 21236.570 \n",
      "Average KL Loss: 182.061 \n",
      "Average MSE Loss: 1.623 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 21201.892 \n",
      "Average KL Loss: 181.961 \n",
      "Average MSE Loss: 1.530 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 21172.329 \n",
      "Average KL Loss: 181.887 \n",
      "Average MSE Loss: 1.480 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 21167.207 \n",
      "Average KL Loss: 181.766 \n",
      "Average MSE Loss: 1.504 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 21176.638 \n",
      "Average KL Loss: 181.590 \n",
      "Average MSE Loss: 1.650 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 21304.511 \n",
      "Average KL Loss: 181.469 \n",
      "Average MSE Loss: 1.524 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 21181.033 \n",
      "Average KL Loss: 181.397 \n",
      "Average MSE Loss: 1.574 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 21175.384 \n",
      "Average KL Loss: 181.315 \n",
      "Average MSE Loss: 1.433 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 21108.398 \n",
      "Average KL Loss: 181.180 \n",
      "Average MSE Loss: 1.405 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 21192.923 \n",
      "Average KL Loss: 181.028 \n",
      "Average MSE Loss: 1.467 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 21106.233 \n",
      "Average KL Loss: 180.830 \n",
      "Average MSE Loss: 1.481 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 21256.301 \n",
      "Average KL Loss: 180.684 \n",
      "Average MSE Loss: 1.520 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 21229.393 \n",
      "Average KL Loss: 180.544 \n",
      "Average MSE Loss: 1.437 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 21152.020 \n",
      "Average KL Loss: 180.414 \n",
      "Average MSE Loss: 1.388 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 21160.296 \n",
      "Average KL Loss: 180.291 \n",
      "Average MSE Loss: 1.618 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 21111.579 \n",
      "Average KL Loss: 180.181 \n",
      "Average MSE Loss: 1.417 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 21121.471 \n",
      "Average KL Loss: 180.033 \n",
      "Average MSE Loss: 1.477 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 21075.703 \n",
      "Average KL Loss: 179.842 \n",
      "Average MSE Loss: 1.428 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 21142.794 \n",
      "Average KL Loss: 179.672 \n",
      "Average MSE Loss: 1.431 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 21269.480 \n",
      "Average KL Loss: 179.512 \n",
      "Average MSE Loss: 1.505 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 21119.744 \n",
      "Average KL Loss: 179.424 \n",
      "Average MSE Loss: 1.448 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 21178.764 \n",
      "Average KL Loss: 179.314 \n",
      "Average MSE Loss: 1.454 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 21197.803 \n",
      "Average KL Loss: 179.209 \n",
      "Average MSE Loss: 1.541 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 21195.032 \n",
      "Average KL Loss: 179.130 \n",
      "Average MSE Loss: 1.524 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 21134.015 \n",
      "Average KL Loss: 179.011 \n",
      "Average MSE Loss: 1.480 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 21080.612 \n",
      "Average KL Loss: 178.843 \n",
      "Average MSE Loss: 1.486 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 21200.195 \n",
      "Average KL Loss: 178.713 \n",
      "Average MSE Loss: 1.487 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 21237.791 \n",
      "Average KL Loss: 178.656 \n",
      "Average MSE Loss: 1.484 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 21159.429 \n",
      "Average KL Loss: 178.598 \n",
      "Average MSE Loss: 1.526 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 21134.400 \n",
      "Average KL Loss: 178.483 \n",
      "Average MSE Loss: 1.489 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 21107.900 \n",
      "Average KL Loss: 178.356 \n",
      "Average MSE Loss: 1.428 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 21122.212 \n",
      "Average KL Loss: 178.254 \n",
      "Average MSE Loss: 1.512 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 21138.210 \n",
      "Average KL Loss: 178.168 \n",
      "Average MSE Loss: 1.554 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 21120.272 \n",
      "Average KL Loss: 178.033 \n",
      "Average MSE Loss: 1.411 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 21150.825 \n",
      "Average KL Loss: 177.956 \n",
      "Average MSE Loss: 1.431 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 21153.483 \n",
      "Average KL Loss: 177.946 \n",
      "Average MSE Loss: 1.474 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 21307.146 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.404 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 21188.166 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.349 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 21234.901 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.336 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 21384.274 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.328 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 21326.422 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.515 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 21367.129 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.360 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 21244.116 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.444 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 21328.984 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.391 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 21316.156 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.410 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 21206.388 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.355 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 21271.568 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.350 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 21202.458 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.263 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 21177.772 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.262 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 21285.740 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.351 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 21216.171 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.343 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 21309.453 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.349 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 21309.332 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.350 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 21207.840 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.267 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 21313.796 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.418 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 21192.076 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.246 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 21155.861 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.301 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 21171.222 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.229 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 21355.500 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.385 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 21329.890 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.302 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 21304.153 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.442 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 21322.050 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.454 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 21242.831 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.280 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 21248.078 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.486 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 21302.033 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.346 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 21315.788 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.326 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 21223.285 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.350 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 21364.396 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.460 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 21429.276 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.423 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 21304.296 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.342 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 21329.399 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.487 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 21327.979 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.396 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 21304.815 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.400 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 21189.935 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.247 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 21217.302 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.233 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 21366.748 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.309 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 21321.091 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.193 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 21231.969 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.345 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 21292.187 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.298 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 21276.064 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.345 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 21442.012 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.548 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 21368.924 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.273 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 21334.574 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.272 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 21254.592 \n",
      "Average KL Loss: 177.944 \n",
      "Average MSE Loss: 1.205 \n",
      "\n",
      "Average Recon Loss: 22419.786 \n",
      "Average KL Loss: 170.949 \n",
      "Average MSE Loss: 1.663 \n",
      "Average RMSE Loss: 1.290 \n",
      "Average R2: 0.800 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_train = {'Train Recon Loss': [], 'Train KL Loss': [], 'Train MSE Loss': [], 'Train Total Loss': []}\n",
    "history_test = {'Test Recon Loss': [], 'Test KL Loss': [], 'Test MSE Loss': [], 'Test RMSE Loss': [], 'Test R2': []}\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(full_dataset)))):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(full_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(full_dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "    print(\"Start Training (Unsupervised Phase)\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "       train_recon_ave, train_kl_ave, train_reg_ave, train_total_ave = train(model,train_loader, epoch, optimizer, w_recon_loss=1, w_kl_loss=1, w_reg_loss=0)\n",
    "       history_train['Train Recon Loss'].append(train_recon_ave)\n",
    "       history_train['Train KL Loss'].append(train_kl_ave)\n",
    "       history_train['Train MSE Loss'].append(train_reg_ave)\n",
    "       history_train['Train Total Loss'].append(train_total_ave)\n",
    "\n",
    "    print(\"Start Training (Supervised Phase)\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_recon_ave, train_kl_ave, train_reg_ave, train_total_ave = train(model,train_loader, epoch, optimizer, w_recon_loss=0, w_kl_loss=0, w_reg_loss=1)\n",
    "        history_train['Train Recon Loss'].append(train_recon_ave)\n",
    "        history_train['Train KL Loss'].append(train_kl_ave)\n",
    "        history_train['Train MSE Loss'].append(train_reg_ave)\n",
    "        history_train['Train Total Loss'].append(train_total_ave)\n",
    "\n",
    "    test_recon_ave, test_kl_ave, test_reg_ave, test_rmse_ave, test_r2_ave = test(test_loader, model)\n",
    "    history_test['Test Recon Loss'].append(test_recon_ave)\n",
    "    history_test['Test KL Loss'].append(test_kl_ave)\n",
    "    history_test['Test MSE Loss'].append(test_reg_ave)\n",
    "    history_test['Test RMSE Loss'].append(test_rmse_ave)\n",
    "    history_test['Test R2'].append(test_r2_ave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Start Training (Unsupervised Phase)\")\n",
    "# for epoch in range(1, epochs + 1):\n",
    "#     train(model,train_loader, epoch, optimizer, w_vae_loss=1, w_reg_loss=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Start Training (Supervised Phase)\")\n",
    "# for epoch in range(1, epochs + 1):\n",
    "#     train(model,train_loader, epoch, optimizer, w_vae_loss=0, w_reg_loss=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_avg_cv_recon_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(history_train[\u001b[39m'\u001b[39m\u001b[39mTrain Recon Loss\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      2\u001b[0m train_avg_cv_kl_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(history_train[\u001b[39m'\u001b[39m\u001b[39mTrain KL Loss\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m train_avg_cv_mse_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(history_train[\u001b[39m'\u001b[39m\u001b[39mTrain MSE Loss\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "train_avg_cv_recon_loss = np.mean(history_train['Train Recon Loss'])\n",
    "train_avg_cv_kl_loss = np.mean(history_train['Train KL Loss'])\n",
    "train_avg_cv_mse_loss = np.mean(history_train['Train MSE Loss'])\n",
    "train_avg_cv_total_loss = np.mean(history_train['Train Total Loss'])\n",
    "\n",
    "test_avg_recon_loss =  np.mean(history_test['Test Recon Loss'])\n",
    "test_avg_kl_loss =  np.mean(history_test['Test KL Loss'])\n",
    "test_avg_mse_loss =  np.mean(history_test['Test MSE Loss'])\n",
    "test_avg_rmse_loss =  np.mean(history_test['Test RMSE Loss'])\n",
    "test_avg_r2_loss =  np.mean(history_test['Test R2'])\n",
    "\n",
    "print('Performance of {} fold cross validation {} dimension:'.format(k, latent_dim))\n",
    "print(\"\\nAverage Training Recon Loss: {:.4f} \\n Average Training KL Loss: {:.4f} \\n Average Training RMSE Loss: {:.3f} \\n Average Training Total Loss: {:.3f} \\n\".format(train_avg_cv_recon_loss, train_avg_cv_kl_loss, train_avg_cv_mse_loss, train_avg_cv_total_loss))\n",
    "print(\"\\nAverage Testing Recon Loss: {:.4f} \\n Average Testing KL Loss: {:.4f} \\n Average Testing MSE Loss: {:.3f} \\n Average Testing RMSE Loss: {:.3f} \\n Average Testing R2 Loss: {:.3f} \\n\".format(test_avg_recon_loss, test_avg_kl_loss, test_avg_mse_loss, test_avg_rmse_loss, test_avg_r2_loss))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.674289744094556"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(history_test['Test RMSE Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.849237377095305,\n",
       " 2.254316064346069,\n",
       " 1.828225612761639,\n",
       " 1.6366155399814923,\n",
       " 1.559920722707668,\n",
       " 1.3816201260239909,\n",
       " 1.4188095540851018,\n",
       " 1.258016153069804,\n",
       " 1.2665763331989643,\n",
       " 1.289559957675526]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_test['Test RMSE Loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.118153631036932,\n",
       " 5.08194091796875,\n",
       " 3.3424088911576706,\n",
       " 2.678510425708912,\n",
       " 2.4333526611328127,\n",
       " 1.9088741726345486,\n",
       " 2.0130205507631653,\n",
       " 1.5826046413845487,\n",
       " 1.6042156078197338,\n",
       " 1.6629648844401042]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_test['Test MSE Loss']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'trained_models/VAE_dna_meth' + str(latent_dim) + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
