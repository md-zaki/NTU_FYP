{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20993297330>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import matplotlib\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "matplotlib.style.use('ggplot')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightning.pytorch as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "torch.manual_seed(33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CELL_LINE_NAME</th>\n",
       "      <th>1:134999</th>\n",
       "      <th>1:135191;1:135218</th>\n",
       "      <th>1:135203;1:135208</th>\n",
       "      <th>1:713376;1:713388;1:713400;1:713448;1:713450;1:713454</th>\n",
       "      <th>1:713901;1:713921;1:714178;1:714182;1:714199;1:714254;1:714258;1:714261;1:714264;1:714277;1:714278;1:714293;1:714301;1:714491;1:714511;1:714566;1:714584</th>\n",
       "      <th>1:715390;1:715392;1:715405;1:715415</th>\n",
       "      <th>1:804993;1:804999;1:805282;1:805284;1:805290;1:805327;1:805338;1:805341;1:805352;1:805445;1:805450;1:805467;1:805468</th>\n",
       "      <th>1:805474;1:805477;1:805479</th>\n",
       "      <th>1:805484;1:805486</th>\n",
       "      <th>...</th>\n",
       "      <th>Dactinomycin</th>\n",
       "      <th>Daporinad</th>\n",
       "      <th>Dasatinib</th>\n",
       "      <th>Rapamycin</th>\n",
       "      <th>Romidepsin</th>\n",
       "      <th>SN-38</th>\n",
       "      <th>Temsirolimus</th>\n",
       "      <th>Trametinib</th>\n",
       "      <th>Vinblastine</th>\n",
       "      <th>Vinorelbine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22rv1</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.41803</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.23610</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.840963</td>\n",
       "      <td>-3.112784</td>\n",
       "      <td>4.203067</td>\n",
       "      <td>-4.382138</td>\n",
       "      <td>-4.917572</td>\n",
       "      <td>-4.972312</td>\n",
       "      <td>-0.974396</td>\n",
       "      <td>1.325603</td>\n",
       "      <td>-4.384381</td>\n",
       "      <td>-3.401996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2313287</td>\n",
       "      <td>0.8897</td>\n",
       "      <td>0.80360</td>\n",
       "      <td>0.99105</td>\n",
       "      <td>0.65648</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.24032</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.584971</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>1.797167</td>\n",
       "      <td>-3.486065</td>\n",
       "      <td>-6.017003</td>\n",
       "      <td>-4.132899</td>\n",
       "      <td>3.279539</td>\n",
       "      <td>-0.040150</td>\n",
       "      <td>-4.849422</td>\n",
       "      <td>-5.303091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42mgba</td>\n",
       "      <td>0.9222</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.49984</td>\n",
       "      <td>0.00897</td>\n",
       "      <td>0.95695</td>\n",
       "      <td>0.02262</td>\n",
       "      <td>0.12001</td>\n",
       "      <td>0.1765</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.240733</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>-2.233603</td>\n",
       "      <td>-3.671158</td>\n",
       "      <td>-6.415517</td>\n",
       "      <td>-4.737156</td>\n",
       "      <td>-0.074308</td>\n",
       "      <td>1.621553</td>\n",
       "      <td>-4.784344</td>\n",
       "      <td>-3.927690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5637</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.95915</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.75370</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03540</td>\n",
       "      <td>0.07694</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.321517</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>-1.829021</td>\n",
       "      <td>-0.116397</td>\n",
       "      <td>-5.559258</td>\n",
       "      <td>-4.537337</td>\n",
       "      <td>1.818908</td>\n",
       "      <td>-0.275098</td>\n",
       "      <td>-3.494500</td>\n",
       "      <td>-5.252216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>639v</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.70308</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.16765</td>\n",
       "      <td>0.05868</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.725271</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>1.391965</td>\n",
       "      <td>-1.822279</td>\n",
       "      <td>-4.955669</td>\n",
       "      <td>-6.639479</td>\n",
       "      <td>1.248546</td>\n",
       "      <td>1.437469</td>\n",
       "      <td>-3.986408</td>\n",
       "      <td>-3.138535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>wsudlcl2</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>0.76473</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.70492</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.72915</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.120747</td>\n",
       "      <td>-6.196767</td>\n",
       "      <td>-1.463565</td>\n",
       "      <td>-5.903482</td>\n",
       "      <td>-5.240336</td>\n",
       "      <td>-4.959568</td>\n",
       "      <td>-5.974346</td>\n",
       "      <td>0.641326</td>\n",
       "      <td>-7.103903</td>\n",
       "      <td>-6.196508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>yapc</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>0.98215</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.96413</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.86138</td>\n",
       "      <td>0.00357</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.932980</td>\n",
       "      <td>-5.076682</td>\n",
       "      <td>1.407784</td>\n",
       "      <td>1.214362</td>\n",
       "      <td>-3.897758</td>\n",
       "      <td>5.073883</td>\n",
       "      <td>1.195720</td>\n",
       "      <td>3.598733</td>\n",
       "      <td>-0.095525</td>\n",
       "      <td>2.307072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>yh13</td>\n",
       "      <td>0.9211</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.97220</td>\n",
       "      <td>0.59999</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.94020</td>\n",
       "      <td>0.01222</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.595530</td>\n",
       "      <td>-3.469021</td>\n",
       "      <td>-0.787238</td>\n",
       "      <td>-2.743620</td>\n",
       "      <td>-5.957548</td>\n",
       "      <td>-3.526360</td>\n",
       "      <td>-1.963500</td>\n",
       "      <td>1.153223</td>\n",
       "      <td>-5.412392</td>\n",
       "      <td>-4.207989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>ykg1</td>\n",
       "      <td>0.8617</td>\n",
       "      <td>0.92310</td>\n",
       "      <td>0.93395</td>\n",
       "      <td>0.82007</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.99590</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.262451</td>\n",
       "      <td>3.115970</td>\n",
       "      <td>0.597641</td>\n",
       "      <td>-0.763934</td>\n",
       "      <td>-5.240336</td>\n",
       "      <td>-3.225249</td>\n",
       "      <td>2.191202</td>\n",
       "      <td>0.407173</td>\n",
       "      <td>-3.494500</td>\n",
       "      <td>-2.601804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>zr7530</td>\n",
       "      <td>0.9091</td>\n",
       "      <td>0.98385</td>\n",
       "      <td>0.96925</td>\n",
       "      <td>0.88890</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.65477</td>\n",
       "      <td>0.02129</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038498</td>\n",
       "      <td>1.574590</td>\n",
       "      <td>4.366079</td>\n",
       "      <td>-0.781208</td>\n",
       "      <td>-2.472102</td>\n",
       "      <td>5.054112</td>\n",
       "      <td>1.928232</td>\n",
       "      <td>5.681084</td>\n",
       "      <td>2.268159</td>\n",
       "      <td>1.959661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>543 rows × 81048 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CELL_LINE_NAME  1:134999  1:135191;1:135218  1:135203;1:135208  \\\n",
       "0            22rv1    0.9821            1.00000            1.00000   \n",
       "1          2313287    0.8897            0.80360            0.99105   \n",
       "2           42mgba    0.9222            0.87500            1.00000   \n",
       "3             5637    1.0000            0.95915            1.00000   \n",
       "4             639v    1.0000            1.00000            1.00000   \n",
       "..             ...       ...                ...                ...   \n",
       "538       wsudlcl2    0.7586            0.76473            1.00000   \n",
       "539           yapc    0.9595            0.98215            1.00000   \n",
       "540           yh13    0.9211            1.00000            0.97220   \n",
       "541           ykg1    0.8617            0.92310            0.93395   \n",
       "542         zr7530    0.9091            0.98385            0.96925   \n",
       "\n",
       "     1:713376;1:713388;1:713400;1:713448;1:713450;1:713454  \\\n",
       "0                                              0.41803       \n",
       "1                                              0.65648       \n",
       "2                                              0.49984       \n",
       "3                                              0.75370       \n",
       "4                                              0.70308       \n",
       "..                                                 ...       \n",
       "538                                            0.70492       \n",
       "539                                            0.96413       \n",
       "540                                            0.59999       \n",
       "541                                            0.82007       \n",
       "542                                            0.88890       \n",
       "\n",
       "     1:713901;1:713921;1:714178;1:714182;1:714199;1:714254;1:714258;1:714261;1:714264;1:714277;1:714278;1:714293;1:714301;1:714491;1:714511;1:714566;1:714584  \\\n",
       "0                                              0.00000                                                                                                          \n",
       "1                                              0.00000                                                                                                          \n",
       "2                                              0.00897                                                                                                          \n",
       "3                                              0.00000                                                                                                          \n",
       "4                                              0.00000                                                                                                          \n",
       "..                                                 ...                                                                                                          \n",
       "538                                            0.00000                                                                                                          \n",
       "539                                            0.00000                                                                                                          \n",
       "540                                            0.00000                                                                                                          \n",
       "541                                            0.00000                                                                                                          \n",
       "542                                            0.00000                                                                                                          \n",
       "\n",
       "     1:715390;1:715392;1:715405;1:715415  \\\n",
       "0                                0.23610   \n",
       "1                                0.24032   \n",
       "2                                0.95695   \n",
       "3                                0.03540   \n",
       "4                                0.16765   \n",
       "..                                   ...   \n",
       "538                              0.72915   \n",
       "539                              0.86138   \n",
       "540                              0.94020   \n",
       "541                              0.99590   \n",
       "542                              0.65477   \n",
       "\n",
       "     1:804993;1:804999;1:805282;1:805284;1:805290;1:805327;1:805338;1:805341;1:805352;1:805445;1:805450;1:805467;1:805468  \\\n",
       "0                                              0.00000                                                                      \n",
       "1                                              0.00000                                                                      \n",
       "2                                              0.02262                                                                      \n",
       "3                                              0.07694                                                                      \n",
       "4                                              0.05868                                                                      \n",
       "..                                                 ...                                                                      \n",
       "538                                            0.00000                                                                      \n",
       "539                                            0.00357                                                                      \n",
       "540                                            0.01222                                                                      \n",
       "541                                            0.00000                                                                      \n",
       "542                                            0.02129                                                                      \n",
       "\n",
       "     1:805474;1:805477;1:805479  1:805484;1:805486  ...  Dactinomycin  \\\n",
       "0                       0.10000             0.0000  ...     -4.840963   \n",
       "1                       0.00000             0.0000  ...     -4.584971   \n",
       "2                       0.12001             0.1765  ...     -4.240733   \n",
       "3                       0.00000             0.0000  ...     -4.321517   \n",
       "4                       0.16667             0.3000  ...     -4.725271   \n",
       "..                          ...                ...  ...           ...   \n",
       "538                     0.00000             0.0000  ...     -5.120747   \n",
       "539                     0.00000             0.0000  ...     -1.932980   \n",
       "540                     0.00000             0.0000  ...     -3.595530   \n",
       "541                     0.00000             0.0000  ...     -5.262451   \n",
       "542                     0.00000             0.0000  ...     -0.038498   \n",
       "\n",
       "     Daporinad  Dasatinib  Rapamycin  Romidepsin     SN-38  Temsirolimus  \\\n",
       "0    -3.112784   4.203067  -4.382138   -4.917572 -4.972312     -0.974396   \n",
       "1    -3.336795   1.797167  -3.486065   -6.017003 -4.132899      3.279539   \n",
       "2    -3.336795  -2.233603  -3.671158   -6.415517 -4.737156     -0.074308   \n",
       "3    -3.336795  -1.829021  -0.116397   -5.559258 -4.537337      1.818908   \n",
       "4    -3.336795   1.391965  -1.822279   -4.955669 -6.639479      1.248546   \n",
       "..         ...        ...        ...         ...       ...           ...   \n",
       "538  -6.196767  -1.463565  -5.903482   -5.240336 -4.959568     -5.974346   \n",
       "539  -5.076682   1.407784   1.214362   -3.897758  5.073883      1.195720   \n",
       "540  -3.469021  -0.787238  -2.743620   -5.957548 -3.526360     -1.963500   \n",
       "541   3.115970   0.597641  -0.763934   -5.240336 -3.225249      2.191202   \n",
       "542   1.574590   4.366079  -0.781208   -2.472102  5.054112      1.928232   \n",
       "\n",
       "     Trametinib  Vinblastine  Vinorelbine  \n",
       "0      1.325603    -4.384381    -3.401996  \n",
       "1     -0.040150    -4.849422    -5.303091  \n",
       "2      1.621553    -4.784344    -3.927690  \n",
       "3     -0.275098    -3.494500    -5.252216  \n",
       "4      1.437469    -3.986408    -3.138535  \n",
       "..          ...          ...          ...  \n",
       "538    0.641326    -7.103903    -6.196508  \n",
       "539    3.598733    -0.095525     2.307072  \n",
       "540    1.153223    -5.412392    -4.207989  \n",
       "541    0.407173    -3.494500    -2.601804  \n",
       "542    5.681084     2.268159     1.959661  \n",
       "\n",
       "[543 rows x 81048 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna_meth = pd.read_csv('new_clean_data/consistent_dna_meth.csv')\n",
    "dna_meth.drop(columns='Unnamed: 0', inplace=True)\n",
    "dna_meth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CELL_LINE_NAME</th>\n",
       "      <th>ENSG00000000003.10</th>\n",
       "      <th>ENSG00000000005.5</th>\n",
       "      <th>ENSG00000000419.8</th>\n",
       "      <th>ENSG00000000457.9</th>\n",
       "      <th>ENSG00000000460.12</th>\n",
       "      <th>ENSG00000000938.8</th>\n",
       "      <th>ENSG00000000971.11</th>\n",
       "      <th>ENSG00000001036.9</th>\n",
       "      <th>ENSG00000001084.6</th>\n",
       "      <th>...</th>\n",
       "      <th>Dactinomycin</th>\n",
       "      <th>Daporinad</th>\n",
       "      <th>Dasatinib</th>\n",
       "      <th>Rapamycin</th>\n",
       "      <th>Romidepsin</th>\n",
       "      <th>SN-38</th>\n",
       "      <th>Temsirolimus</th>\n",
       "      <th>Trametinib</th>\n",
       "      <th>Vinblastine</th>\n",
       "      <th>Vinorelbine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22rv1</td>\n",
       "      <td>5.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.38</td>\n",
       "      <td>9.76</td>\n",
       "      <td>24.51</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>54.86</td>\n",
       "      <td>118.50</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.840963</td>\n",
       "      <td>-3.112784</td>\n",
       "      <td>4.203067</td>\n",
       "      <td>-4.382138</td>\n",
       "      <td>-4.917572</td>\n",
       "      <td>-4.972312</td>\n",
       "      <td>-0.974396</td>\n",
       "      <td>1.325603</td>\n",
       "      <td>-4.384381</td>\n",
       "      <td>-3.401996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2313287</td>\n",
       "      <td>7.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.99</td>\n",
       "      <td>16.76</td>\n",
       "      <td>13.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>170.91</td>\n",
       "      <td>93.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.584971</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>1.797167</td>\n",
       "      <td>-3.486065</td>\n",
       "      <td>-6.017003</td>\n",
       "      <td>-4.132899</td>\n",
       "      <td>3.279539</td>\n",
       "      <td>-0.040150</td>\n",
       "      <td>-4.849422</td>\n",
       "      <td>-5.303091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42mgba</td>\n",
       "      <td>23.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.28</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.27</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.45</td>\n",
       "      <td>53.29</td>\n",
       "      <td>8.36</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.240733</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>-2.233603</td>\n",
       "      <td>-3.671158</td>\n",
       "      <td>-6.415517</td>\n",
       "      <td>-4.737156</td>\n",
       "      <td>-0.074308</td>\n",
       "      <td>1.621553</td>\n",
       "      <td>-4.784344</td>\n",
       "      <td>-3.927690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5637</td>\n",
       "      <td>57.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.95</td>\n",
       "      <td>3.11</td>\n",
       "      <td>31.61</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.98</td>\n",
       "      <td>71.97</td>\n",
       "      <td>9.76</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.321517</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>-1.829021</td>\n",
       "      <td>-0.116397</td>\n",
       "      <td>-5.559258</td>\n",
       "      <td>-4.537337</td>\n",
       "      <td>1.818908</td>\n",
       "      <td>-0.275098</td>\n",
       "      <td>-3.494500</td>\n",
       "      <td>-5.252216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>639v</td>\n",
       "      <td>32.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125.99</td>\n",
       "      <td>2.69</td>\n",
       "      <td>10.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.89</td>\n",
       "      <td>82.50</td>\n",
       "      <td>24.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.725271</td>\n",
       "      <td>-3.336795</td>\n",
       "      <td>1.391965</td>\n",
       "      <td>-1.822279</td>\n",
       "      <td>-4.955669</td>\n",
       "      <td>-6.639479</td>\n",
       "      <td>1.248546</td>\n",
       "      <td>1.437469</td>\n",
       "      <td>-3.986408</td>\n",
       "      <td>-3.138535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>wsudlcl2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.71</td>\n",
       "      <td>5.60</td>\n",
       "      <td>20.21</td>\n",
       "      <td>30.57</td>\n",
       "      <td>0.46</td>\n",
       "      <td>11.78</td>\n",
       "      <td>8.32</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.120747</td>\n",
       "      <td>-6.196767</td>\n",
       "      <td>-1.463565</td>\n",
       "      <td>-5.903482</td>\n",
       "      <td>-5.240336</td>\n",
       "      <td>-4.959568</td>\n",
       "      <td>-5.974346</td>\n",
       "      <td>0.641326</td>\n",
       "      <td>-7.103903</td>\n",
       "      <td>-6.196508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>yapc</td>\n",
       "      <td>50.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103.70</td>\n",
       "      <td>7.76</td>\n",
       "      <td>12.84</td>\n",
       "      <td>0.17</td>\n",
       "      <td>13.58</td>\n",
       "      <td>71.13</td>\n",
       "      <td>29.67</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.932980</td>\n",
       "      <td>-5.076682</td>\n",
       "      <td>1.407784</td>\n",
       "      <td>1.214362</td>\n",
       "      <td>-3.897758</td>\n",
       "      <td>5.073883</td>\n",
       "      <td>1.195720</td>\n",
       "      <td>3.598733</td>\n",
       "      <td>-0.095525</td>\n",
       "      <td>2.307072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>yh13</td>\n",
       "      <td>28.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.09</td>\n",
       "      <td>4.94</td>\n",
       "      <td>13.35</td>\n",
       "      <td>0.22</td>\n",
       "      <td>139.44</td>\n",
       "      <td>118.19</td>\n",
       "      <td>13.53</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.595530</td>\n",
       "      <td>-3.469021</td>\n",
       "      <td>-0.787238</td>\n",
       "      <td>-2.743620</td>\n",
       "      <td>-5.957548</td>\n",
       "      <td>-3.526360</td>\n",
       "      <td>-1.963500</td>\n",
       "      <td>1.153223</td>\n",
       "      <td>-5.412392</td>\n",
       "      <td>-4.207989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>ykg1</td>\n",
       "      <td>61.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>109.13</td>\n",
       "      <td>5.91</td>\n",
       "      <td>17.40</td>\n",
       "      <td>0.13</td>\n",
       "      <td>53.25</td>\n",
       "      <td>92.96</td>\n",
       "      <td>23.09</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.262451</td>\n",
       "      <td>3.115970</td>\n",
       "      <td>0.597641</td>\n",
       "      <td>-0.763934</td>\n",
       "      <td>-5.240336</td>\n",
       "      <td>-3.225249</td>\n",
       "      <td>2.191202</td>\n",
       "      <td>0.407173</td>\n",
       "      <td>-3.494500</td>\n",
       "      <td>-2.601804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>zr7530</td>\n",
       "      <td>16.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.20</td>\n",
       "      <td>13.76</td>\n",
       "      <td>11.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>24.06</td>\n",
       "      <td>47.14</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038498</td>\n",
       "      <td>1.574590</td>\n",
       "      <td>4.366079</td>\n",
       "      <td>-0.781208</td>\n",
       "      <td>-2.472102</td>\n",
       "      <td>5.054112</td>\n",
       "      <td>1.928232</td>\n",
       "      <td>5.681084</td>\n",
       "      <td>2.268159</td>\n",
       "      <td>1.959661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>543 rows × 57831 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CELL_LINE_NAME  ENSG00000000003.10  ENSG00000000005.5  ENSG00000000419.8  \\\n",
       "0            22rv1                5.28                0.0              73.38   \n",
       "1          2313287                7.01                0.0             108.99   \n",
       "2           42mgba               23.09                0.0              99.28   \n",
       "3             5637               57.94                0.0              98.95   \n",
       "4             639v               32.02                0.0             125.99   \n",
       "..             ...                 ...                ...                ...   \n",
       "538       wsudlcl2                0.74                0.0              41.71   \n",
       "539           yapc               50.29                0.0             103.70   \n",
       "540           yh13               28.92                0.0              64.09   \n",
       "541           ykg1               61.08                0.0             109.13   \n",
       "542         zr7530               16.81                0.0              56.20   \n",
       "\n",
       "     ENSG00000000457.9  ENSG00000000460.12  ENSG00000000938.8  \\\n",
       "0                 9.76               24.51               0.01   \n",
       "1                16.76               13.32               0.00   \n",
       "2                 2.73                9.27               0.02   \n",
       "3                 3.11               31.61               0.14   \n",
       "4                 2.69               10.45               0.00   \n",
       "..                 ...                 ...                ...   \n",
       "538               5.60               20.21              30.57   \n",
       "539               7.76               12.84               0.17   \n",
       "540               4.94               13.35               0.22   \n",
       "541               5.91               17.40               0.13   \n",
       "542              13.76               11.53               0.00   \n",
       "\n",
       "     ENSG00000000971.11  ENSG00000001036.9  ENSG00000001084.6  ...  \\\n",
       "0                  0.08              54.86             118.50  ...   \n",
       "1                  0.23             170.91              93.00  ...   \n",
       "2                  0.45              53.29               8.36  ...   \n",
       "3                  1.98              71.97               9.76  ...   \n",
       "4                 12.89              82.50              24.13  ...   \n",
       "..                  ...                ...                ...  ...   \n",
       "538                0.46              11.78               8.32  ...   \n",
       "539               13.58              71.13              29.67  ...   \n",
       "540              139.44             118.19              13.53  ...   \n",
       "541               53.25              92.96              23.09  ...   \n",
       "542                0.03              24.06              47.14  ...   \n",
       "\n",
       "     Dactinomycin  Daporinad  Dasatinib  Rapamycin  Romidepsin     SN-38  \\\n",
       "0       -4.840963  -3.112784   4.203067  -4.382138   -4.917572 -4.972312   \n",
       "1       -4.584971  -3.336795   1.797167  -3.486065   -6.017003 -4.132899   \n",
       "2       -4.240733  -3.336795  -2.233603  -3.671158   -6.415517 -4.737156   \n",
       "3       -4.321517  -3.336795  -1.829021  -0.116397   -5.559258 -4.537337   \n",
       "4       -4.725271  -3.336795   1.391965  -1.822279   -4.955669 -6.639479   \n",
       "..            ...        ...        ...        ...         ...       ...   \n",
       "538     -5.120747  -6.196767  -1.463565  -5.903482   -5.240336 -4.959568   \n",
       "539     -1.932980  -5.076682   1.407784   1.214362   -3.897758  5.073883   \n",
       "540     -3.595530  -3.469021  -0.787238  -2.743620   -5.957548 -3.526360   \n",
       "541     -5.262451   3.115970   0.597641  -0.763934   -5.240336 -3.225249   \n",
       "542     -0.038498   1.574590   4.366079  -0.781208   -2.472102  5.054112   \n",
       "\n",
       "     Temsirolimus  Trametinib  Vinblastine  Vinorelbine  \n",
       "0       -0.974396    1.325603    -4.384381    -3.401996  \n",
       "1        3.279539   -0.040150    -4.849422    -5.303091  \n",
       "2       -0.074308    1.621553    -4.784344    -3.927690  \n",
       "3        1.818908   -0.275098    -3.494500    -5.252216  \n",
       "4        1.248546    1.437469    -3.986408    -3.138535  \n",
       "..            ...         ...          ...          ...  \n",
       "538     -5.974346    0.641326    -7.103903    -6.196508  \n",
       "539      1.195720    3.598733    -0.095525     2.307072  \n",
       "540     -1.963500    1.153223    -5.412392    -4.207989  \n",
       "541      2.191202    0.407173    -3.494500    -2.601804  \n",
       "542      1.928232    5.681084     2.268159     1.959661  \n",
       "\n",
       "[543 rows x 57831 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_exp = pd.read_csv('new_clean_data/consistent_gene_exp.csv')\n",
    "gene_exp.drop(columns='Unnamed: 0', inplace=True)\n",
    "gene_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns\n",
    "cell_line_cols = ['CELL_LINE_NAME']\n",
    "drug_resp_cols = ['Dactinomycin', 'Daporinad', 'Dasatinib',\t'Rapamycin', 'Romidepsin', 'SN-38',\t'Temsirolimus',\t'Trametinib', 'Vinblastine', 'Vinorelbine']\n",
    "dna_meth_cols = dna_meth.drop(columns=['CELL_LINE_NAME','Dactinomycin', 'Daporinad', 'Dasatinib',\t'Rapamycin', 'Romidepsin', 'SN-38',\t'Temsirolimus',\t'Trametinib', 'Vinblastine', 'Vinorelbine']).columns\n",
    "gene_exp_cols = gene_exp.drop(columns=['CELL_LINE_NAME','Dactinomycin', 'Daporinad', 'Dasatinib',\t'Rapamycin', 'Romidepsin', 'SN-38',\t'Temsirolimus',\t'Trametinib', 'Vinblastine', 'Vinorelbine']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drug response labels to tensor\n",
    "drug_resp_dna = np.stack([dna_meth[col].values for col in drug_resp_cols], 1)\n",
    "drug_resp_dna = torch.tensor(drug_resp_dna, dtype=torch.float)\n",
    "drug_resp_gene = np.stack([gene_exp[col].values for col in drug_resp_cols], 1)\n",
    "drug_resp_gene = torch.tensor(drug_resp_gene, dtype=torch.float)\n",
    "drug_resp = drug_resp_dna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dna_meth values to tensor\n",
    "dna_meth_values = np.stack([dna_meth[col].values for col in dna_meth_cols], 1)\n",
    "dna_meth_values = torch.tensor(dna_meth_values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_exp values to tensor and scale from 0 to 1\n",
    "gene_exp_values = np.stack([gene_exp[col].values for col in gene_exp_cols], 1)\n",
    "scaler = MinMaxScaler()\n",
    "gene_exp_values_t = gene_exp_values.T\n",
    "scaled_gene_exp_values_t = scaler.fit_transform(gene_exp_values_t)\n",
    "scaled_gene_exp_values = scaled_gene_exp_values_t.T\n",
    "scaled_gene_exp_values = torch.tensor(scaled_gene_exp_values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "full_dataset = torch.utils.data.TensorDataset(dna_meth_values, scaled_gene_exp_values, drug_resp)\n",
    "full_loader = DataLoader(full_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE model with Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_with_regressor(nn.Module):\n",
    "    def __init__(self, input_size_dna, input_size_gene, level_2, level_3, latent_dim, shared_size, independent_size):\n",
    "        super(VAE_with_regressor, self).__init__()\n",
    "\n",
    "        self.input_size_dna = input_size_dna\n",
    "        self.input_size_gene = input_size_gene\n",
    "        self.level_2 = level_2\n",
    "        self.level_3 = level_3\n",
    "        self.latent_dim = latent_dim\n",
    "        self.shared_size = shared_size\n",
    "        self.independent_size = independent_size\n",
    "\n",
    "        # Encoder DNA\n",
    "        self.enc_fc1_dna = nn.Sequential(\n",
    "                        nn.Linear(input_size_dna, level_2),\n",
    "                        nn.BatchNorm1d(level_2),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.enc_fc2_dna = nn.Sequential(\n",
    "                        nn.Linear(level_2, level_3),\n",
    "                        nn.BatchNorm1d(level_3),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.enc_fc3_dna_mean = nn.Sequential(\n",
    "                    nn.Linear(level_3, latent_dim),\n",
    "                    nn.BatchNorm1d(latent_dim))\n",
    "        \n",
    "        self.enc_fc3_dna_log_var = nn.Sequential(\n",
    "                    nn.Linear(level_3, latent_dim),\n",
    "                    nn.BatchNorm1d(latent_dim))\n",
    "        \n",
    "        # Decoder DNA\n",
    "        self.dec_fc3_dna = nn.Sequential(\n",
    "                        nn.Linear(latent_dim, level_3),\n",
    "                        nn.BatchNorm1d(level_3),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.dec_fc2_dna = nn.Sequential(\n",
    "                        nn.Linear(level_3, level_2),\n",
    "                        nn.BatchNorm1d(level_2),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.dec_fc1_dna = nn.Sequential(\n",
    "                    nn.Linear(level_2, input_size_dna),\n",
    "                    nn.BatchNorm1d(input_size_dna),\n",
    "                    nn.Sigmoid())\n",
    "        \n",
    "        # Encoder Gene\n",
    "        self.enc_fc1_gene = nn.Sequential(\n",
    "                        nn.Linear(input_size_gene, level_2),\n",
    "                        nn.BatchNorm1d(level_2),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.enc_fc2_gene = nn.Sequential(\n",
    "                        nn.Linear(level_2, level_3),\n",
    "                        nn.BatchNorm1d(level_3),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.enc_fc3_gene_mean = nn.Sequential(\n",
    "                    nn.Linear(level_3, latent_dim),\n",
    "                    nn.BatchNorm1d(latent_dim))\n",
    "        \n",
    "        self.enc_fc3_gene_log_var = nn.Sequential(\n",
    "                    nn.Linear(level_3, latent_dim),\n",
    "                    nn.BatchNorm1d(latent_dim))\n",
    "        \n",
    "        # Decoder Gene\n",
    "        self.dec_fc3_gene = nn.Sequential(\n",
    "                        nn.Linear(latent_dim, level_3),\n",
    "                        nn.BatchNorm1d(level_3),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.dec_fc2_gene = nn.Sequential(\n",
    "                        nn.Linear(level_3, level_2),\n",
    "                        nn.BatchNorm1d(level_2),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.dec_fc1_gene = nn.Sequential(\n",
    "                    nn.Linear(level_2, input_size_gene),\n",
    "                    nn.BatchNorm1d(input_size_gene),\n",
    "                    nn.Sigmoid())\n",
    "        \n",
    "        # attention layer\n",
    "        self.attention = MultiheadAttention(embed_dim=(independent_size * 2 + shared_size), num_heads=int((independent_size * 2 + shared_size)))\n",
    "        \n",
    "        # Regression fc layers\n",
    "        self.r_fc1 = nn.Sequential(\n",
    "                    nn.Linear(independent_size * 2 + shared_size, 64),\n",
    "                    nn.BatchNorm1d(64),\n",
    "                    nn.ReLU())\n",
    "        self.r_fc2 = nn.Sequential(\n",
    "                    nn.Linear(64, 32),\n",
    "                    nn.BatchNorm1d(32),\n",
    "                    nn.ReLU())\n",
    "        \n",
    "        self.r_fc3 = nn.Sequential(\n",
    "                    nn.Linear(32, 10),\n",
    "                    nn.BatchNorm1d(10))\n",
    "        \n",
    "    def encode(self, x):\n",
    "        dna_l2_layer = self.enc_fc1_dna(x[0])\n",
    "        dna_l3_layer = self.enc_fc2_dna(dna_l2_layer)\n",
    "        mu_dna = self.enc_fc3_dna_mean(dna_l3_layer)\n",
    "        logvar_dna = self.enc_fc3_dna_log_var(dna_l3_layer)\n",
    "\n",
    "        gene_l2_layer = self.enc_fc1_gene(x[1])\n",
    "        gene_l3_layer = self.enc_fc2_gene(gene_l2_layer)\n",
    "        mu_gene = self.enc_fc3_gene_mean(gene_l3_layer)\n",
    "        logvar_gene = self.enc_fc3_gene_log_var(gene_l3_layer)\n",
    "\n",
    "        u_dna, s_dna = torch.split(mu_dna, [self.independent_size, self.shared_size], -1)\n",
    "        u_gene, s_gene = torch.split(mu_gene, [self.independent_size, self.shared_size], -1)\n",
    "        shared_mean = torch.mean(torch.stack([s_dna, s_gene]), dim=0)\n",
    "        for_pred = torch.cat((u_dna, shared_mean, u_gene), dim=1)\n",
    "\n",
    "        mu_for_pred = for_pred.unsqueeze(0).transpose(0, 1)\n",
    "        attended_mean, _ = self.attention(mu_for_pred,mu_for_pred,mu_for_pred)\n",
    "        attended_mean = attended_mean.transpose(0, 1).squeeze(0)\n",
    "        # attended_mean = for_pred\n",
    "\n",
    "        return mu_dna, logvar_dna, mu_gene, logvar_gene, attended_mean, u_dna, s_dna, u_gene, s_gene\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def decode_dna(self, z):\n",
    "        l3_dna_layer = self.dec_fc3_dna(z)\n",
    "        l2_dna_layer = self.dec_fc2_gene(l3_dna_layer)\n",
    "        recon_dna = self.dec_fc1_dna(l2_dna_layer)\n",
    "        return recon_dna\n",
    "    \n",
    "    def decode_gene(self,z):\n",
    "        l3_gene_layer = self.dec_fc3_gene(z)\n",
    "        l2_gene_layer = self.dec_fc2_gene(l3_gene_layer)\n",
    "        recon_gene = self. dec_fc1_gene(l2_gene_layer)\n",
    "        return recon_gene\n",
    "    \n",
    "    def regressor(self, mean):\n",
    "        level_1_layer = self.r_fc1(mean)\n",
    "        level_2_layer = self.r_fc2(level_1_layer)\n",
    "        output_layer = self.r_fc3(level_2_layer)\n",
    "        return output_layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu_dna, logvar_dna, mu_gene, logvar_gene, attended_mean, u_dna, s_dna, u_gene, s_gene = self.encode(x)\n",
    "        z_dna = self.reparameterize(mu_dna, logvar_dna)\n",
    "        z_gene = self.reparameterize(mu_gene, logvar_gene)\n",
    "        recon_dna = self.decode_dna(z_dna)\n",
    "        recon_gene = self.decode_gene(z_gene)\n",
    "\n",
    "        \n",
    "        y_pred = self.regressor(attended_mean)\n",
    "\n",
    "        return recon_dna, recon_gene, mu_dna, logvar_dna, mu_gene, logvar_gene, u_dna, s_dna, u_gene, s_gene, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function(x_hat, x, mean, log_var): # recon loss and kld loss\n",
    "        bce = torch.nn.functional.binary_cross_entropy(x_hat, x, reduction = 'sum')\n",
    "        kld = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "        loss = kld + bce\n",
    "        return loss, kld, bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss_function(y_pred, y):\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "    loss = torch.sqrt(loss_fn(y_pred, y))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_function(y_pred, y):\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_function_mean(y_pred, y):\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndependenceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IndependenceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, batch1, batch2):\n",
    "        # Compute the covariance matrix for each pair of rows\n",
    "        cov_matrix = torch.matmul(batch1.unsqueeze(2), batch2.unsqueeze(1))\n",
    "\n",
    "        # Compute the Frobenius norm of each covariance matrix\n",
    "        loss = torch.norm(cov_matrix, dim=(1, 2), p='fro')\n",
    "\n",
    "        loss_mean = loss.mean()\n",
    "\n",
    "        return loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "independence_loss = IndependenceLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y_true, y_pred):\n",
    "    y_true_mean = torch.mean(y_true)\n",
    "    SS_res = torch.sum(torch.square(y_true - y_pred))\n",
    "    SS_tot = torch.sum(torch.square(y_true - y_true_mean))\n",
    "    r_squared = 1 - SS_res / (SS_tot + torch.finfo(torch.float32).eps)\n",
    "    return r_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "lr = 0.001\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda')\n",
    "\n",
    "k = 10\n",
    "splits = KFold(n_splits=k, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "input_size_dna = 81037\n",
    "input_size_gene = 57820 #dimension of gene expressions\n",
    "level_2 = 2048\n",
    "level_3 = 1024\n",
    "latent_dim = 128 # target latent size\n",
    "\n",
    "shared_size = 10\n",
    "independent_size = 118\n",
    "\n",
    "model = VAE_with_regressor(input_size_dna, input_size_gene, level_2, level_3, latent_dim, shared_size, independent_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (model, dataloader, epoch, optimizer, w_recon_loss, w_kl_loss, w_reg_loss, w_shared_loss, w_ind_loss):\n",
    "    model.train()\n",
    "    train_recon = 0\n",
    "    train_kl  = 0\n",
    "    train_reg = 0\n",
    "    train_shared = 0\n",
    "    train_ind = 0\n",
    "    batch_num = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        batch_num = batch_num + 1\n",
    "        # get values\n",
    "        dna_meth_values, gene_exp_values, drug_resp = batch\n",
    "        dna_meth_values = dna_meth_values.to(device)\n",
    "        gene_exp_values = gene_exp_values.to(device)\n",
    "        drug_resp = drug_resp.to(device)\n",
    "\n",
    "        dna_plus_gene = [dna_meth_values, gene_exp_values]\n",
    "        dna_plus_gene = dna_plus_gene\n",
    "\n",
    "        \n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        # apply model\n",
    "        recon_dna, recon_gene, mu_dna, logvar_dna, mu_gene, logvar_gene, u_dna, s_dna, u_gene, s_gene, y_pred = model(dna_plus_gene)\n",
    "\n",
    "        # losses\n",
    "        dna_vae_loss, dna_kld, dna_bce = vae_loss_function(recon_dna, dna_meth_values, mu_dna, logvar_dna)\n",
    "        gene_vae_loss, gene_kld, gene_bce = vae_loss_function(recon_gene, gene_exp_values, mu_gene, logvar_gene)\n",
    "\n",
    "        shared_loss = mse_loss_function_mean(s_dna, s_gene)\n",
    "        ind_loss = independence_loss(u_dna, u_gene)\n",
    "\n",
    "        reg_loss = mse_loss_function_mean(y_pred, drug_resp)\n",
    "\n",
    "        recon_loss = 0.5 * (dna_bce + gene_bce)\n",
    "        kld_loss = 0.5 * (dna_kld + gene_kld)\n",
    "\n",
    "        loss = w_recon_loss * recon_loss + w_kl_loss * kld_loss + w_reg_loss * reg_loss + w_shared_loss * shared_loss + w_ind_loss * ind_loss\n",
    "\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            train_recon += recon_loss.item()\n",
    "            train_kl += kld_loss.item()\n",
    "            train_reg += reg_loss.item()\n",
    "            train_shared += shared_loss.item()\n",
    "            train_ind += ind_loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_recon_ave = train_recon/ len(dataloader.sampler)\n",
    "    train_kl_ave = train_kl/ len(dataloader.sampler)\n",
    "    train_reg_ave = train_reg/ batch_num\n",
    "    train_shared_ave = train_shared/ batch_num\n",
    "    train_ind_ave = train_ind/batch_num\n",
    "\n",
    "    print('=====> Epoch {} \\n' \n",
    "          'Average Recon Loss: {:.3f} \\n'\n",
    "          'Average KL Loss: {:.3f} \\n'\n",
    "          'Average Regression Loss: {:.3f} \\n'\n",
    "          'Average Shared Loss: {:.3f} \\n'\n",
    "          'Average Independence Loss: {:.3f} \\n'.format(epoch, train_recon_ave, train_kl_ave, train_reg_ave, train_shared_ave, train_ind_ave))\n",
    "    return train_recon_ave, train_kl_ave, train_reg_ave, train_shared_ave, train_ind_ave\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model):\n",
    "    model.eval()\n",
    "\n",
    "    test_recon_dna = 0\n",
    "    test_recon_gene = 0\n",
    "    test_kl_dna = 0\n",
    "    test_kl_gene = 0\n",
    "\n",
    "    test_reg = 0\n",
    "    test_rmse = 0\n",
    "    test_shared = 0\n",
    "    test_shared_rmse = 0\n",
    "    test_r2 = 0\n",
    "    test_ind_loss = 0\n",
    "    \n",
    "\n",
    "    batch_num = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            batch_num = batch_num + 1\n",
    "            \n",
    "            # get data values\n",
    "            dna_meth_values, gene_exp_values, drug_resp = batch\n",
    "            dna_meth_values = dna_meth_values.to(device)\n",
    "            gene_exp_values = gene_exp_values.to(device)\n",
    "            drug_resp = drug_resp.to(device)\n",
    "\n",
    "            dna_plus_gene = [dna_meth_values, gene_exp_values]\n",
    "\n",
    "            recon_dna, recon_gene, mu_dna, logvar_dna, mu_gene, logvar_gene, u_dna, s_dna, u_gene, s_gene, y_pred = model(dna_plus_gene)\n",
    "\n",
    "            # losses\n",
    "            dna_vae_loss, dna_kld, dna_bce = vae_loss_function(recon_dna, dna_meth_values, mu_dna, logvar_dna)\n",
    "            gene_vae_loss, gene_kld, gene_bce = vae_loss_function(recon_gene, gene_exp_values, mu_gene, logvar_gene)\n",
    "\n",
    "            shared_loss = mse_loss_function(s_dna, s_gene)\n",
    "            ind_loss = independence_loss(u_dna, u_gene)\n",
    "\n",
    "            reg_loss = mse_loss_function(y_pred, drug_resp)\n",
    "            r2_value = r_squared(drug_resp, y_pred)\n",
    "\n",
    "            test_recon_dna += dna_bce.item()\n",
    "            test_recon_gene += gene_bce.item()\n",
    "            test_kl_dna += dna_kld.item()\n",
    "            test_kl_gene += gene_kld.item()\n",
    "\n",
    "            test_reg += reg_loss.item()\n",
    "            test_shared += shared_loss.item()\n",
    "            test_r2 += r2_value.item()\n",
    "            test_ind_loss += ind_loss.item()\n",
    "\n",
    "        # print loss\n",
    "        test_recon_dna_ave = test_recon_dna/len(dataloader.sampler)\n",
    "        test_recon_gene_ave = test_recon_gene/len(dataloader.sampler)\n",
    "        test_kl_dna_ave = test_kl_dna/ len(dataloader.sampler)\n",
    "        test_kl_gene_ave = test_kl_gene/ len(dataloader.sampler)\n",
    "\n",
    "        test_reg_ave = test_reg/ (len(dataloader.sampler) * 10)\n",
    "        test_rmse_ave = math.sqrt(test_reg_ave)\n",
    "        test_shared_ave = test_shared/ (len(dataloader.sampler) * 10)\n",
    "        test_shared_rmse_ave = math.sqrt(test_shared_ave)\n",
    "        test_r2_ave = test_r2/ batch_num\n",
    "        test_ind_ave = test_ind_loss/ batch_num\n",
    "\n",
    "        print('Average DNA Recon Loss: {:.3f} \\n'\n",
    "              'Average Gene Recon Loss: {:.3f} \\n'\n",
    "          'Average DNA KL Loss: {:.3f} \\n'\n",
    "          'Average Gene KL Loss: {:.3f} \\n'\n",
    "          'Average Regressor Loss: {:.3f} \\n'\n",
    "          'Average RMSE Loss: {:.3f} \\n'\n",
    "          'Average Shared MSE Loss: {:.3f} \\n'\n",
    "          'Average Shared RMSE Loss: {:.3f} \\n'\n",
    "          'Average R2: {:.3f} \\n'\n",
    "          'Average Indepence Loss: {:.3f} \\n'.format(test_recon_dna_ave, test_recon_gene_ave, test_kl_dna_ave, test_kl_gene_ave, test_reg_ave, test_rmse_ave, test_shared_ave, test_shared_rmse_ave, test_r2_ave, test_ind_ave))\n",
    "        \n",
    "        return test_recon_dna_ave, test_recon_gene_ave, test_kl_dna_ave, test_kl_gene_ave, test_reg_ave, test_rmse_ave, test_shared_ave, test_shared_rmse_ave, test_r2_ave, test_ind_ave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Training and K fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 51375.049 \n",
      "Average KL Loss: 125.651 \n",
      "Average Regression Loss: 13.661 \n",
      "Average Shared Loss: 2.104 \n",
      "Average Independence Loss: 85.986 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 49439.650 \n",
      "Average KL Loss: 117.515 \n",
      "Average Regression Loss: 13.620 \n",
      "Average Shared Loss: 2.045 \n",
      "Average Independence Loss: 84.044 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 48644.972 \n",
      "Average KL Loss: 113.918 \n",
      "Average Regression Loss: 13.531 \n",
      "Average Shared Loss: 2.122 \n",
      "Average Independence Loss: 81.823 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 47987.071 \n",
      "Average KL Loss: 108.671 \n",
      "Average Regression Loss: 13.484 \n",
      "Average Shared Loss: 2.045 \n",
      "Average Independence Loss: 86.728 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 47458.685 \n",
      "Average KL Loss: 105.755 \n",
      "Average Regression Loss: 13.642 \n",
      "Average Shared Loss: 2.095 \n",
      "Average Independence Loss: 86.389 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 46956.104 \n",
      "Average KL Loss: 108.448 \n",
      "Average Regression Loss: 13.716 \n",
      "Average Shared Loss: 2.101 \n",
      "Average Independence Loss: 85.179 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 46389.095 \n",
      "Average KL Loss: 105.722 \n",
      "Average Regression Loss: 13.560 \n",
      "Average Shared Loss: 2.078 \n",
      "Average Independence Loss: 85.166 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 45910.019 \n",
      "Average KL Loss: 106.599 \n",
      "Average Regression Loss: 13.656 \n",
      "Average Shared Loss: 2.107 \n",
      "Average Independence Loss: 85.171 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 45503.692 \n",
      "Average KL Loss: 106.451 \n",
      "Average Regression Loss: 13.568 \n",
      "Average Shared Loss: 2.108 \n",
      "Average Independence Loss: 87.631 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 45028.536 \n",
      "Average KL Loss: 107.906 \n",
      "Average Regression Loss: 13.499 \n",
      "Average Shared Loss: 2.137 \n",
      "Average Independence Loss: 85.648 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 44667.222 \n",
      "Average KL Loss: 102.144 \n",
      "Average Regression Loss: 13.700 \n",
      "Average Shared Loss: 2.164 \n",
      "Average Independence Loss: 90.684 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 44072.090 \n",
      "Average KL Loss: 102.065 \n",
      "Average Regression Loss: 13.642 \n",
      "Average Shared Loss: 2.149 \n",
      "Average Independence Loss: 89.939 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 43638.037 \n",
      "Average KL Loss: 102.535 \n",
      "Average Regression Loss: 13.598 \n",
      "Average Shared Loss: 2.173 \n",
      "Average Independence Loss: 90.923 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 43254.362 \n",
      "Average KL Loss: 101.270 \n",
      "Average Regression Loss: 13.496 \n",
      "Average Shared Loss: 2.131 \n",
      "Average Independence Loss: 92.420 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 42904.426 \n",
      "Average KL Loss: 100.505 \n",
      "Average Regression Loss: 13.430 \n",
      "Average Shared Loss: 2.161 \n",
      "Average Independence Loss: 95.109 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 42423.505 \n",
      "Average KL Loss: 98.465 \n",
      "Average Regression Loss: 13.469 \n",
      "Average Shared Loss: 2.165 \n",
      "Average Independence Loss: 95.123 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 41996.642 \n",
      "Average KL Loss: 99.152 \n",
      "Average Regression Loss: 13.643 \n",
      "Average Shared Loss: 2.165 \n",
      "Average Independence Loss: 90.385 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 41586.032 \n",
      "Average KL Loss: 98.028 \n",
      "Average Regression Loss: 13.723 \n",
      "Average Shared Loss: 2.178 \n",
      "Average Independence Loss: 93.351 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 41172.439 \n",
      "Average KL Loss: 95.631 \n",
      "Average Regression Loss: 13.605 \n",
      "Average Shared Loss: 2.163 \n",
      "Average Independence Loss: 98.085 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 40769.164 \n",
      "Average KL Loss: 96.415 \n",
      "Average Regression Loss: 13.590 \n",
      "Average Shared Loss: 2.172 \n",
      "Average Independence Loss: 94.614 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 40448.604 \n",
      "Average KL Loss: 95.972 \n",
      "Average Regression Loss: 13.736 \n",
      "Average Shared Loss: 2.199 \n",
      "Average Independence Loss: 96.403 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 39949.256 \n",
      "Average KL Loss: 93.800 \n",
      "Average Regression Loss: 13.697 \n",
      "Average Shared Loss: 2.223 \n",
      "Average Independence Loss: 98.372 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 39682.954 \n",
      "Average KL Loss: 93.251 \n",
      "Average Regression Loss: 13.807 \n",
      "Average Shared Loss: 2.261 \n",
      "Average Independence Loss: 103.463 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 39259.820 \n",
      "Average KL Loss: 93.437 \n",
      "Average Regression Loss: 13.660 \n",
      "Average Shared Loss: 2.211 \n",
      "Average Independence Loss: 98.189 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 38971.371 \n",
      "Average KL Loss: 93.457 \n",
      "Average Regression Loss: 13.904 \n",
      "Average Shared Loss: 2.272 \n",
      "Average Independence Loss: 99.079 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 38589.116 \n",
      "Average KL Loss: 93.076 \n",
      "Average Regression Loss: 13.652 \n",
      "Average Shared Loss: 2.264 \n",
      "Average Independence Loss: 101.199 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 38158.793 \n",
      "Average KL Loss: 92.835 \n",
      "Average Regression Loss: 13.492 \n",
      "Average Shared Loss: 2.313 \n",
      "Average Independence Loss: 102.105 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 37869.045 \n",
      "Average KL Loss: 92.868 \n",
      "Average Regression Loss: 13.625 \n",
      "Average Shared Loss: 2.324 \n",
      "Average Independence Loss: 100.935 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 37476.562 \n",
      "Average KL Loss: 92.864 \n",
      "Average Regression Loss: 13.656 \n",
      "Average Shared Loss: 2.361 \n",
      "Average Independence Loss: 101.590 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 37131.834 \n",
      "Average KL Loss: 92.638 \n",
      "Average Regression Loss: 13.680 \n",
      "Average Shared Loss: 2.321 \n",
      "Average Independence Loss: 103.556 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 36765.382 \n",
      "Average KL Loss: 93.083 \n",
      "Average Regression Loss: 13.613 \n",
      "Average Shared Loss: 2.357 \n",
      "Average Independence Loss: 100.476 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 36480.984 \n",
      "Average KL Loss: 92.534 \n",
      "Average Regression Loss: 13.655 \n",
      "Average Shared Loss: 2.394 \n",
      "Average Independence Loss: 102.231 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 36155.226 \n",
      "Average KL Loss: 93.026 \n",
      "Average Regression Loss: 13.649 \n",
      "Average Shared Loss: 2.393 \n",
      "Average Independence Loss: 99.734 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 35851.249 \n",
      "Average KL Loss: 93.171 \n",
      "Average Regression Loss: 13.429 \n",
      "Average Shared Loss: 2.379 \n",
      "Average Independence Loss: 104.529 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 35511.588 \n",
      "Average KL Loss: 93.151 \n",
      "Average Regression Loss: 13.584 \n",
      "Average Shared Loss: 2.362 \n",
      "Average Independence Loss: 100.921 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 35173.551 \n",
      "Average KL Loss: 93.241 \n",
      "Average Regression Loss: 13.545 \n",
      "Average Shared Loss: 2.389 \n",
      "Average Independence Loss: 103.038 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 34819.777 \n",
      "Average KL Loss: 93.196 \n",
      "Average Regression Loss: 13.587 \n",
      "Average Shared Loss: 2.406 \n",
      "Average Independence Loss: 98.438 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 34489.056 \n",
      "Average KL Loss: 93.112 \n",
      "Average Regression Loss: 13.559 \n",
      "Average Shared Loss: 2.422 \n",
      "Average Independence Loss: 99.821 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 34173.316 \n",
      "Average KL Loss: 93.052 \n",
      "Average Regression Loss: 13.468 \n",
      "Average Shared Loss: 2.403 \n",
      "Average Independence Loss: 100.373 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 33897.666 \n",
      "Average KL Loss: 93.068 \n",
      "Average Regression Loss: 13.470 \n",
      "Average Shared Loss: 2.423 \n",
      "Average Independence Loss: 101.493 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 33672.308 \n",
      "Average KL Loss: 92.965 \n",
      "Average Regression Loss: 13.419 \n",
      "Average Shared Loss: 2.457 \n",
      "Average Independence Loss: 96.991 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 33358.741 \n",
      "Average KL Loss: 93.262 \n",
      "Average Regression Loss: 13.861 \n",
      "Average Shared Loss: 2.477 \n",
      "Average Independence Loss: 101.242 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 33075.097 \n",
      "Average KL Loss: 93.029 \n",
      "Average Regression Loss: 13.551 \n",
      "Average Shared Loss: 2.506 \n",
      "Average Independence Loss: 101.391 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 32759.457 \n",
      "Average KL Loss: 93.202 \n",
      "Average Regression Loss: 13.545 \n",
      "Average Shared Loss: 2.497 \n",
      "Average Independence Loss: 101.059 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 32487.187 \n",
      "Average KL Loss: 93.392 \n",
      "Average Regression Loss: 13.332 \n",
      "Average Shared Loss: 2.479 \n",
      "Average Independence Loss: 101.104 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 32248.980 \n",
      "Average KL Loss: 93.780 \n",
      "Average Regression Loss: 13.491 \n",
      "Average Shared Loss: 2.471 \n",
      "Average Independence Loss: 101.903 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 31971.238 \n",
      "Average KL Loss: 93.842 \n",
      "Average Regression Loss: 13.778 \n",
      "Average Shared Loss: 2.495 \n",
      "Average Independence Loss: 100.181 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 31668.714 \n",
      "Average KL Loss: 94.134 \n",
      "Average Regression Loss: 13.640 \n",
      "Average Shared Loss: 2.529 \n",
      "Average Independence Loss: 101.609 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 31404.036 \n",
      "Average KL Loss: 93.968 \n",
      "Average Regression Loss: 13.468 \n",
      "Average Shared Loss: 2.496 \n",
      "Average Independence Loss: 101.960 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 31131.331 \n",
      "Average KL Loss: 94.499 \n",
      "Average Regression Loss: 13.477 \n",
      "Average Shared Loss: 2.549 \n",
      "Average Independence Loss: 103.387 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 31245.141 \n",
      "Average KL Loss: 94.641 \n",
      "Average Regression Loss: 12.427 \n",
      "Average Shared Loss: 2.512 \n",
      "Average Independence Loss: 100.613 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 31642.451 \n",
      "Average KL Loss: 94.786 \n",
      "Average Regression Loss: 11.481 \n",
      "Average Shared Loss: 2.518 \n",
      "Average Independence Loss: 97.169 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 31765.485 \n",
      "Average KL Loss: 95.124 \n",
      "Average Regression Loss: 10.979 \n",
      "Average Shared Loss: 2.482 \n",
      "Average Independence Loss: 94.248 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 31796.080 \n",
      "Average KL Loss: 95.193 \n",
      "Average Regression Loss: 10.515 \n",
      "Average Shared Loss: 2.501 \n",
      "Average Independence Loss: 90.965 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 31845.618 \n",
      "Average KL Loss: 95.821 \n",
      "Average Regression Loss: 10.310 \n",
      "Average Shared Loss: 2.494 \n",
      "Average Independence Loss: 91.932 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 31828.003 \n",
      "Average KL Loss: 96.401 \n",
      "Average Regression Loss: 9.935 \n",
      "Average Shared Loss: 2.518 \n",
      "Average Independence Loss: 94.069 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 31792.800 \n",
      "Average KL Loss: 96.320 \n",
      "Average Regression Loss: 9.875 \n",
      "Average Shared Loss: 2.522 \n",
      "Average Independence Loss: 92.067 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 31822.553 \n",
      "Average KL Loss: 96.352 \n",
      "Average Regression Loss: 9.550 \n",
      "Average Shared Loss: 2.513 \n",
      "Average Independence Loss: 87.858 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 31847.728 \n",
      "Average KL Loss: 96.619 \n",
      "Average Regression Loss: 9.254 \n",
      "Average Shared Loss: 2.501 \n",
      "Average Independence Loss: 90.147 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 31927.541 \n",
      "Average KL Loss: 96.995 \n",
      "Average Regression Loss: 9.229 \n",
      "Average Shared Loss: 2.508 \n",
      "Average Independence Loss: 90.858 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 31805.884 \n",
      "Average KL Loss: 96.830 \n",
      "Average Regression Loss: 9.008 \n",
      "Average Shared Loss: 2.469 \n",
      "Average Independence Loss: 89.536 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 31809.342 \n",
      "Average KL Loss: 96.960 \n",
      "Average Regression Loss: 8.855 \n",
      "Average Shared Loss: 2.497 \n",
      "Average Independence Loss: 88.080 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 31828.547 \n",
      "Average KL Loss: 96.967 \n",
      "Average Regression Loss: 8.892 \n",
      "Average Shared Loss: 2.510 \n",
      "Average Independence Loss: 86.339 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 31865.627 \n",
      "Average KL Loss: 97.131 \n",
      "Average Regression Loss: 8.724 \n",
      "Average Shared Loss: 2.474 \n",
      "Average Independence Loss: 85.885 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 31811.325 \n",
      "Average KL Loss: 96.717 \n",
      "Average Regression Loss: 8.479 \n",
      "Average Shared Loss: 2.444 \n",
      "Average Independence Loss: 83.682 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 31849.595 \n",
      "Average KL Loss: 96.894 \n",
      "Average Regression Loss: 8.443 \n",
      "Average Shared Loss: 2.498 \n",
      "Average Independence Loss: 83.497 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 31883.002 \n",
      "Average KL Loss: 97.271 \n",
      "Average Regression Loss: 8.444 \n",
      "Average Shared Loss: 2.500 \n",
      "Average Independence Loss: 86.210 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 31846.252 \n",
      "Average KL Loss: 97.215 \n",
      "Average Regression Loss: 8.363 \n",
      "Average Shared Loss: 2.450 \n",
      "Average Independence Loss: 83.611 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 31797.853 \n",
      "Average KL Loss: 97.016 \n",
      "Average Regression Loss: 8.077 \n",
      "Average Shared Loss: 2.503 \n",
      "Average Independence Loss: 80.988 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 31806.479 \n",
      "Average KL Loss: 97.107 \n",
      "Average Regression Loss: 7.883 \n",
      "Average Shared Loss: 2.484 \n",
      "Average Independence Loss: 79.315 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 31851.139 \n",
      "Average KL Loss: 97.151 \n",
      "Average Regression Loss: 7.830 \n",
      "Average Shared Loss: 2.549 \n",
      "Average Independence Loss: 80.456 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 31855.228 \n",
      "Average KL Loss: 97.294 \n",
      "Average Regression Loss: 7.718 \n",
      "Average Shared Loss: 2.504 \n",
      "Average Independence Loss: 81.244 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 31833.472 \n",
      "Average KL Loss: 97.171 \n",
      "Average Regression Loss: 7.621 \n",
      "Average Shared Loss: 2.450 \n",
      "Average Independence Loss: 78.146 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 31772.995 \n",
      "Average KL Loss: 97.074 \n",
      "Average Regression Loss: 7.510 \n",
      "Average Shared Loss: 2.504 \n",
      "Average Independence Loss: 75.385 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 31844.006 \n",
      "Average KL Loss: 97.301 \n",
      "Average Regression Loss: 7.438 \n",
      "Average Shared Loss: 2.498 \n",
      "Average Independence Loss: 77.141 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 31873.576 \n",
      "Average KL Loss: 97.519 \n",
      "Average Regression Loss: 7.334 \n",
      "Average Shared Loss: 2.475 \n",
      "Average Independence Loss: 77.938 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 31844.269 \n",
      "Average KL Loss: 97.119 \n",
      "Average Regression Loss: 7.215 \n",
      "Average Shared Loss: 2.480 \n",
      "Average Independence Loss: 74.369 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 31886.817 \n",
      "Average KL Loss: 97.168 \n",
      "Average Regression Loss: 7.228 \n",
      "Average Shared Loss: 2.482 \n",
      "Average Independence Loss: 71.130 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 31800.383 \n",
      "Average KL Loss: 97.307 \n",
      "Average Regression Loss: 7.003 \n",
      "Average Shared Loss: 2.467 \n",
      "Average Independence Loss: 76.843 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 31846.818 \n",
      "Average KL Loss: 97.376 \n",
      "Average Regression Loss: 6.874 \n",
      "Average Shared Loss: 2.481 \n",
      "Average Independence Loss: 72.840 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 31831.051 \n",
      "Average KL Loss: 97.086 \n",
      "Average Regression Loss: 6.973 \n",
      "Average Shared Loss: 2.441 \n",
      "Average Independence Loss: 72.581 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 31837.383 \n",
      "Average KL Loss: 97.106 \n",
      "Average Regression Loss: 6.820 \n",
      "Average Shared Loss: 2.514 \n",
      "Average Independence Loss: 70.359 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 31860.517 \n",
      "Average KL Loss: 97.008 \n",
      "Average Regression Loss: 6.786 \n",
      "Average Shared Loss: 2.469 \n",
      "Average Independence Loss: 69.063 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 31865.904 \n",
      "Average KL Loss: 97.247 \n",
      "Average Regression Loss: 6.818 \n",
      "Average Shared Loss: 2.491 \n",
      "Average Independence Loss: 71.041 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 31808.408 \n",
      "Average KL Loss: 97.091 \n",
      "Average Regression Loss: 6.698 \n",
      "Average Shared Loss: 2.456 \n",
      "Average Independence Loss: 66.076 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 31841.143 \n",
      "Average KL Loss: 97.348 \n",
      "Average Regression Loss: 6.657 \n",
      "Average Shared Loss: 2.472 \n",
      "Average Independence Loss: 69.340 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 31825.606 \n",
      "Average KL Loss: 96.867 \n",
      "Average Regression Loss: 6.591 \n",
      "Average Shared Loss: 2.497 \n",
      "Average Independence Loss: 63.207 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 31816.340 \n",
      "Average KL Loss: 97.100 \n",
      "Average Regression Loss: 6.310 \n",
      "Average Shared Loss: 2.469 \n",
      "Average Independence Loss: 65.257 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 31796.492 \n",
      "Average KL Loss: 97.058 \n",
      "Average Regression Loss: 6.382 \n",
      "Average Shared Loss: 2.504 \n",
      "Average Independence Loss: 65.769 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 31860.886 \n",
      "Average KL Loss: 97.088 \n",
      "Average Regression Loss: 6.346 \n",
      "Average Shared Loss: 2.508 \n",
      "Average Independence Loss: 63.565 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 31847.571 \n",
      "Average KL Loss: 97.323 \n",
      "Average Regression Loss: 6.162 \n",
      "Average Shared Loss: 2.478 \n",
      "Average Independence Loss: 66.997 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 31850.587 \n",
      "Average KL Loss: 97.157 \n",
      "Average Regression Loss: 6.253 \n",
      "Average Shared Loss: 2.448 \n",
      "Average Independence Loss: 65.216 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 31882.524 \n",
      "Average KL Loss: 96.749 \n",
      "Average Regression Loss: 6.131 \n",
      "Average Shared Loss: 2.469 \n",
      "Average Independence Loss: 62.029 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 31837.300 \n",
      "Average KL Loss: 96.667 \n",
      "Average Regression Loss: 6.042 \n",
      "Average Shared Loss: 2.439 \n",
      "Average Independence Loss: 61.650 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 31842.228 \n",
      "Average KL Loss: 96.888 \n",
      "Average Regression Loss: 5.990 \n",
      "Average Shared Loss: 2.454 \n",
      "Average Independence Loss: 62.393 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 31884.959 \n",
      "Average KL Loss: 96.666 \n",
      "Average Regression Loss: 6.065 \n",
      "Average Shared Loss: 2.467 \n",
      "Average Independence Loss: 60.005 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 31837.157 \n",
      "Average KL Loss: 96.491 \n",
      "Average Regression Loss: 5.828 \n",
      "Average Shared Loss: 2.470 \n",
      "Average Independence Loss: 55.085 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 31836.252 \n",
      "Average KL Loss: 96.884 \n",
      "Average Regression Loss: 5.785 \n",
      "Average Shared Loss: 2.462 \n",
      "Average Independence Loss: 59.906 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 31864.631 \n",
      "Average KL Loss: 96.866 \n",
      "Average Regression Loss: 5.773 \n",
      "Average Shared Loss: 2.470 \n",
      "Average Independence Loss: 59.885 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 31903.123 \n",
      "Average KL Loss: 96.803 \n",
      "Average Regression Loss: 5.645 \n",
      "Average Shared Loss: 2.443 \n",
      "Average Independence Loss: 54.768 \n",
      "\n",
      "Average DNA Recon Loss: 43841.795 \n",
      "Average Gene Recon Loss: 25491.410 \n",
      "Average DNA KL Loss: 96.873 \n",
      "Average Gene KL Loss: 44.768 \n",
      "Average Regressor Loss: 7.138 \n",
      "Average RMSE Loss: 2.672 \n",
      "Average Shared MSE Loss: 1.799 \n",
      "Average Shared RMSE Loss: 1.341 \n",
      "Average R2: 0.172 \n",
      "Average Indepence Loss: 40.792 \n",
      "\n",
      "Fold 2\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 31431.948 \n",
      "Average KL Loss: 96.111 \n",
      "Average Regression Loss: 6.633 \n",
      "Average Shared Loss: 2.481 \n",
      "Average Independence Loss: 58.540 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 31013.106 \n",
      "Average KL Loss: 94.767 \n",
      "Average Regression Loss: 6.910 \n",
      "Average Shared Loss: 2.520 \n",
      "Average Independence Loss: 66.601 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 30603.193 \n",
      "Average KL Loss: 94.531 \n",
      "Average Regression Loss: 6.845 \n",
      "Average Shared Loss: 2.454 \n",
      "Average Independence Loss: 74.802 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 30072.891 \n",
      "Average KL Loss: 94.209 \n",
      "Average Regression Loss: 6.968 \n",
      "Average Shared Loss: 2.491 \n",
      "Average Independence Loss: 77.844 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 29685.526 \n",
      "Average KL Loss: 94.187 \n",
      "Average Regression Loss: 6.732 \n",
      "Average Shared Loss: 2.542 \n",
      "Average Independence Loss: 76.578 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 29296.534 \n",
      "Average KL Loss: 94.362 \n",
      "Average Regression Loss: 6.818 \n",
      "Average Shared Loss: 2.537 \n",
      "Average Independence Loss: 80.204 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 28876.942 \n",
      "Average KL Loss: 94.383 \n",
      "Average Regression Loss: 6.758 \n",
      "Average Shared Loss: 2.585 \n",
      "Average Independence Loss: 81.270 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 28585.436 \n",
      "Average KL Loss: 94.503 \n",
      "Average Regression Loss: 6.842 \n",
      "Average Shared Loss: 2.576 \n",
      "Average Independence Loss: 79.850 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 28328.191 \n",
      "Average KL Loss: 94.712 \n",
      "Average Regression Loss: 6.784 \n",
      "Average Shared Loss: 2.595 \n",
      "Average Independence Loss: 82.671 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 27941.550 \n",
      "Average KL Loss: 94.810 \n",
      "Average Regression Loss: 6.743 \n",
      "Average Shared Loss: 2.592 \n",
      "Average Independence Loss: 81.786 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 27582.941 \n",
      "Average KL Loss: 95.088 \n",
      "Average Regression Loss: 6.687 \n",
      "Average Shared Loss: 2.606 \n",
      "Average Independence Loss: 81.743 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 27325.688 \n",
      "Average KL Loss: 95.367 \n",
      "Average Regression Loss: 6.799 \n",
      "Average Shared Loss: 2.623 \n",
      "Average Independence Loss: 82.197 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 26978.737 \n",
      "Average KL Loss: 95.677 \n",
      "Average Regression Loss: 6.903 \n",
      "Average Shared Loss: 2.619 \n",
      "Average Independence Loss: 83.794 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 26668.194 \n",
      "Average KL Loss: 95.767 \n",
      "Average Regression Loss: 6.805 \n",
      "Average Shared Loss: 2.702 \n",
      "Average Independence Loss: 82.443 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 26459.372 \n",
      "Average KL Loss: 95.750 \n",
      "Average Regression Loss: 6.827 \n",
      "Average Shared Loss: 2.664 \n",
      "Average Independence Loss: 78.297 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 26137.088 \n",
      "Average KL Loss: 96.411 \n",
      "Average Regression Loss: 6.779 \n",
      "Average Shared Loss: 2.676 \n",
      "Average Independence Loss: 83.222 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 25892.999 \n",
      "Average KL Loss: 96.481 \n",
      "Average Regression Loss: 6.663 \n",
      "Average Shared Loss: 2.651 \n",
      "Average Independence Loss: 80.929 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 25686.022 \n",
      "Average KL Loss: 96.673 \n",
      "Average Regression Loss: 6.796 \n",
      "Average Shared Loss: 2.676 \n",
      "Average Independence Loss: 79.502 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 25391.879 \n",
      "Average KL Loss: 97.056 \n",
      "Average Regression Loss: 6.820 \n",
      "Average Shared Loss: 2.696 \n",
      "Average Independence Loss: 80.693 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 25188.168 \n",
      "Average KL Loss: 97.471 \n",
      "Average Regression Loss: 6.694 \n",
      "Average Shared Loss: 2.700 \n",
      "Average Independence Loss: 81.283 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 24902.799 \n",
      "Average KL Loss: 97.477 \n",
      "Average Regression Loss: 6.981 \n",
      "Average Shared Loss: 2.687 \n",
      "Average Independence Loss: 77.875 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 24663.064 \n",
      "Average KL Loss: 98.023 \n",
      "Average Regression Loss: 6.752 \n",
      "Average Shared Loss: 2.730 \n",
      "Average Independence Loss: 81.577 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 24470.575 \n",
      "Average KL Loss: 98.093 \n",
      "Average Regression Loss: 6.731 \n",
      "Average Shared Loss: 2.731 \n",
      "Average Independence Loss: 79.476 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 24218.759 \n",
      "Average KL Loss: 98.158 \n",
      "Average Regression Loss: 6.663 \n",
      "Average Shared Loss: 2.743 \n",
      "Average Independence Loss: 76.951 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 23949.550 \n",
      "Average KL Loss: 98.398 \n",
      "Average Regression Loss: 6.744 \n",
      "Average Shared Loss: 2.722 \n",
      "Average Independence Loss: 76.593 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 23769.274 \n",
      "Average KL Loss: 98.794 \n",
      "Average Regression Loss: 6.736 \n",
      "Average Shared Loss: 2.768 \n",
      "Average Independence Loss: 78.854 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 23514.110 \n",
      "Average KL Loss: 98.969 \n",
      "Average Regression Loss: 6.559 \n",
      "Average Shared Loss: 2.724 \n",
      "Average Independence Loss: 77.665 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 23351.853 \n",
      "Average KL Loss: 99.146 \n",
      "Average Regression Loss: 6.582 \n",
      "Average Shared Loss: 2.767 \n",
      "Average Independence Loss: 75.919 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 23189.305 \n",
      "Average KL Loss: 99.615 \n",
      "Average Regression Loss: 6.758 \n",
      "Average Shared Loss: 2.805 \n",
      "Average Independence Loss: 76.617 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 22934.124 \n",
      "Average KL Loss: 99.753 \n",
      "Average Regression Loss: 6.749 \n",
      "Average Shared Loss: 2.760 \n",
      "Average Independence Loss: 76.145 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 22736.531 \n",
      "Average KL Loss: 100.023 \n",
      "Average Regression Loss: 6.739 \n",
      "Average Shared Loss: 2.760 \n",
      "Average Independence Loss: 75.461 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 22571.176 \n",
      "Average KL Loss: 100.233 \n",
      "Average Regression Loss: 6.700 \n",
      "Average Shared Loss: 2.797 \n",
      "Average Independence Loss: 74.731 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 22433.507 \n",
      "Average KL Loss: 100.613 \n",
      "Average Regression Loss: 6.748 \n",
      "Average Shared Loss: 2.803 \n",
      "Average Independence Loss: 75.695 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 22254.383 \n",
      "Average KL Loss: 100.853 \n",
      "Average Regression Loss: 6.679 \n",
      "Average Shared Loss: 2.817 \n",
      "Average Independence Loss: 74.948 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 22023.834 \n",
      "Average KL Loss: 101.113 \n",
      "Average Regression Loss: 6.717 \n",
      "Average Shared Loss: 2.797 \n",
      "Average Independence Loss: 73.205 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 21897.997 \n",
      "Average KL Loss: 101.351 \n",
      "Average Regression Loss: 6.678 \n",
      "Average Shared Loss: 2.810 \n",
      "Average Independence Loss: 73.727 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 21738.864 \n",
      "Average KL Loss: 101.559 \n",
      "Average Regression Loss: 6.708 \n",
      "Average Shared Loss: 2.835 \n",
      "Average Independence Loss: 73.450 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 21542.289 \n",
      "Average KL Loss: 101.712 \n",
      "Average Regression Loss: 6.747 \n",
      "Average Shared Loss: 2.820 \n",
      "Average Independence Loss: 72.338 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 21417.516 \n",
      "Average KL Loss: 102.137 \n",
      "Average Regression Loss: 6.694 \n",
      "Average Shared Loss: 2.819 \n",
      "Average Independence Loss: 73.996 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 21179.221 \n",
      "Average KL Loss: 102.242 \n",
      "Average Regression Loss: 6.718 \n",
      "Average Shared Loss: 2.836 \n",
      "Average Independence Loss: 70.814 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 21092.535 \n",
      "Average KL Loss: 102.456 \n",
      "Average Regression Loss: 6.723 \n",
      "Average Shared Loss: 2.824 \n",
      "Average Independence Loss: 72.515 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 20865.176 \n",
      "Average KL Loss: 102.783 \n",
      "Average Regression Loss: 6.689 \n",
      "Average Shared Loss: 2.831 \n",
      "Average Independence Loss: 72.482 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 20783.618 \n",
      "Average KL Loss: 102.597 \n",
      "Average Regression Loss: 6.687 \n",
      "Average Shared Loss: 2.845 \n",
      "Average Independence Loss: 67.596 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 20620.427 \n",
      "Average KL Loss: 103.212 \n",
      "Average Regression Loss: 6.793 \n",
      "Average Shared Loss: 2.817 \n",
      "Average Independence Loss: 70.346 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 20452.996 \n",
      "Average KL Loss: 103.490 \n",
      "Average Regression Loss: 6.728 \n",
      "Average Shared Loss: 2.867 \n",
      "Average Independence Loss: 70.988 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 20338.513 \n",
      "Average KL Loss: 103.561 \n",
      "Average Regression Loss: 6.721 \n",
      "Average Shared Loss: 2.840 \n",
      "Average Independence Loss: 68.303 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 20209.841 \n",
      "Average KL Loss: 103.806 \n",
      "Average Regression Loss: 6.794 \n",
      "Average Shared Loss: 2.817 \n",
      "Average Independence Loss: 68.966 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 20055.463 \n",
      "Average KL Loss: 103.676 \n",
      "Average Regression Loss: 6.670 \n",
      "Average Shared Loss: 2.838 \n",
      "Average Independence Loss: 65.685 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 19867.525 \n",
      "Average KL Loss: 104.085 \n",
      "Average Regression Loss: 7.013 \n",
      "Average Shared Loss: 2.857 \n",
      "Average Independence Loss: 66.398 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 19895.614 \n",
      "Average KL Loss: 103.967 \n",
      "Average Regression Loss: 6.742 \n",
      "Average Shared Loss: 2.851 \n",
      "Average Independence Loss: 63.352 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 19880.107 \n",
      "Average KL Loss: 104.360 \n",
      "Average Regression Loss: 6.528 \n",
      "Average Shared Loss: 2.826 \n",
      "Average Independence Loss: 64.746 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 20473.137 \n",
      "Average KL Loss: 104.194 \n",
      "Average Regression Loss: 6.094 \n",
      "Average Shared Loss: 2.851 \n",
      "Average Independence Loss: 61.356 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 20611.289 \n",
      "Average KL Loss: 104.518 \n",
      "Average Regression Loss: 5.855 \n",
      "Average Shared Loss: 2.845 \n",
      "Average Independence Loss: 62.028 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 20624.610 \n",
      "Average KL Loss: 104.280 \n",
      "Average Regression Loss: 6.006 \n",
      "Average Shared Loss: 2.825 \n",
      "Average Independence Loss: 57.595 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 20671.825 \n",
      "Average KL Loss: 104.456 \n",
      "Average Regression Loss: 5.626 \n",
      "Average Shared Loss: 2.793 \n",
      "Average Independence Loss: 58.498 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 20586.708 \n",
      "Average KL Loss: 104.543 \n",
      "Average Regression Loss: 5.552 \n",
      "Average Shared Loss: 2.839 \n",
      "Average Independence Loss: 56.070 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 20644.237 \n",
      "Average KL Loss: 104.507 \n",
      "Average Regression Loss: 5.298 \n",
      "Average Shared Loss: 2.829 \n",
      "Average Independence Loss: 55.719 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 20608.860 \n",
      "Average KL Loss: 104.465 \n",
      "Average Regression Loss: 5.249 \n",
      "Average Shared Loss: 2.825 \n",
      "Average Independence Loss: 53.363 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 20582.064 \n",
      "Average KL Loss: 104.757 \n",
      "Average Regression Loss: 5.093 \n",
      "Average Shared Loss: 2.831 \n",
      "Average Independence Loss: 52.483 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 20682.682 \n",
      "Average KL Loss: 104.727 \n",
      "Average Regression Loss: 5.078 \n",
      "Average Shared Loss: 2.807 \n",
      "Average Independence Loss: 51.952 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 20649.001 \n",
      "Average KL Loss: 104.723 \n",
      "Average Regression Loss: 5.150 \n",
      "Average Shared Loss: 2.834 \n",
      "Average Independence Loss: 49.885 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 20633.582 \n",
      "Average KL Loss: 104.771 \n",
      "Average Regression Loss: 4.977 \n",
      "Average Shared Loss: 2.824 \n",
      "Average Independence Loss: 48.862 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 20626.989 \n",
      "Average KL Loss: 104.631 \n",
      "Average Regression Loss: 4.844 \n",
      "Average Shared Loss: 2.818 \n",
      "Average Independence Loss: 46.541 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 20752.187 \n",
      "Average KL Loss: 104.955 \n",
      "Average Regression Loss: 4.849 \n",
      "Average Shared Loss: 2.833 \n",
      "Average Independence Loss: 50.698 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 20608.436 \n",
      "Average KL Loss: 104.805 \n",
      "Average Regression Loss: 4.743 \n",
      "Average Shared Loss: 2.789 \n",
      "Average Independence Loss: 46.406 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 20623.870 \n",
      "Average KL Loss: 104.736 \n",
      "Average Regression Loss: 4.740 \n",
      "Average Shared Loss: 2.852 \n",
      "Average Independence Loss: 44.497 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 20596.575 \n",
      "Average KL Loss: 104.602 \n",
      "Average Regression Loss: 4.654 \n",
      "Average Shared Loss: 2.778 \n",
      "Average Independence Loss: 44.808 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 20656.422 \n",
      "Average KL Loss: 104.961 \n",
      "Average Regression Loss: 4.488 \n",
      "Average Shared Loss: 2.800 \n",
      "Average Independence Loss: 46.426 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 20680.349 \n",
      "Average KL Loss: 104.746 \n",
      "Average Regression Loss: 4.541 \n",
      "Average Shared Loss: 2.798 \n",
      "Average Independence Loss: 45.615 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 20671.907 \n",
      "Average KL Loss: 104.855 \n",
      "Average Regression Loss: 4.422 \n",
      "Average Shared Loss: 2.811 \n",
      "Average Independence Loss: 46.659 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 20548.405 \n",
      "Average KL Loss: 104.976 \n",
      "Average Regression Loss: 4.360 \n",
      "Average Shared Loss: 2.841 \n",
      "Average Independence Loss: 46.517 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 20658.440 \n",
      "Average KL Loss: 104.541 \n",
      "Average Regression Loss: 4.325 \n",
      "Average Shared Loss: 2.838 \n",
      "Average Independence Loss: 41.980 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 20685.119 \n",
      "Average KL Loss: 104.684 \n",
      "Average Regression Loss: 4.338 \n",
      "Average Shared Loss: 2.844 \n",
      "Average Independence Loss: 42.260 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 20672.888 \n",
      "Average KL Loss: 104.852 \n",
      "Average Regression Loss: 4.225 \n",
      "Average Shared Loss: 2.813 \n",
      "Average Independence Loss: 45.056 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 20693.432 \n",
      "Average KL Loss: 104.864 \n",
      "Average Regression Loss: 4.103 \n",
      "Average Shared Loss: 2.847 \n",
      "Average Independence Loss: 44.481 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 20646.222 \n",
      "Average KL Loss: 104.723 \n",
      "Average Regression Loss: 4.008 \n",
      "Average Shared Loss: 2.798 \n",
      "Average Independence Loss: 41.207 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 20695.418 \n",
      "Average KL Loss: 104.633 \n",
      "Average Regression Loss: 4.133 \n",
      "Average Shared Loss: 2.804 \n",
      "Average Independence Loss: 41.041 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 20688.159 \n",
      "Average KL Loss: 104.873 \n",
      "Average Regression Loss: 4.176 \n",
      "Average Shared Loss: 2.820 \n",
      "Average Independence Loss: 43.425 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 20609.942 \n",
      "Average KL Loss: 104.654 \n",
      "Average Regression Loss: 4.088 \n",
      "Average Shared Loss: 2.782 \n",
      "Average Independence Loss: 39.943 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 20697.560 \n",
      "Average KL Loss: 104.511 \n",
      "Average Regression Loss: 3.987 \n",
      "Average Shared Loss: 2.820 \n",
      "Average Independence Loss: 39.031 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 20730.590 \n",
      "Average KL Loss: 104.281 \n",
      "Average Regression Loss: 3.756 \n",
      "Average Shared Loss: 2.802 \n",
      "Average Independence Loss: 35.877 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 20703.828 \n",
      "Average KL Loss: 104.439 \n",
      "Average Regression Loss: 3.856 \n",
      "Average Shared Loss: 2.791 \n",
      "Average Independence Loss: 38.152 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 20536.639 \n",
      "Average KL Loss: 104.300 \n",
      "Average Regression Loss: 3.794 \n",
      "Average Shared Loss: 2.834 \n",
      "Average Independence Loss: 36.209 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 20653.644 \n",
      "Average KL Loss: 104.420 \n",
      "Average Regression Loss: 3.688 \n",
      "Average Shared Loss: 2.794 \n",
      "Average Independence Loss: 37.929 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 20610.345 \n",
      "Average KL Loss: 104.597 \n",
      "Average Regression Loss: 3.729 \n",
      "Average Shared Loss: 2.803 \n",
      "Average Independence Loss: 39.341 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 20608.789 \n",
      "Average KL Loss: 104.543 \n",
      "Average Regression Loss: 3.606 \n",
      "Average Shared Loss: 2.793 \n",
      "Average Independence Loss: 38.874 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 20622.725 \n",
      "Average KL Loss: 104.427 \n",
      "Average Regression Loss: 3.473 \n",
      "Average Shared Loss: 2.812 \n",
      "Average Independence Loss: 36.593 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 20651.804 \n",
      "Average KL Loss: 104.525 \n",
      "Average Regression Loss: 3.675 \n",
      "Average Shared Loss: 2.824 \n",
      "Average Independence Loss: 39.255 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 20661.689 \n",
      "Average KL Loss: 104.470 \n",
      "Average Regression Loss: 3.538 \n",
      "Average Shared Loss: 2.822 \n",
      "Average Independence Loss: 39.241 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 20674.520 \n",
      "Average KL Loss: 104.378 \n",
      "Average Regression Loss: 3.515 \n",
      "Average Shared Loss: 2.813 \n",
      "Average Independence Loss: 37.024 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 20617.948 \n",
      "Average KL Loss: 104.379 \n",
      "Average Regression Loss: 3.332 \n",
      "Average Shared Loss: 2.795 \n",
      "Average Independence Loss: 36.823 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 20670.297 \n",
      "Average KL Loss: 104.304 \n",
      "Average Regression Loss: 3.475 \n",
      "Average Shared Loss: 2.806 \n",
      "Average Independence Loss: 35.141 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 20632.594 \n",
      "Average KL Loss: 104.248 \n",
      "Average Regression Loss: 3.360 \n",
      "Average Shared Loss: 2.784 \n",
      "Average Independence Loss: 36.237 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 20654.785 \n",
      "Average KL Loss: 104.412 \n",
      "Average Regression Loss: 3.233 \n",
      "Average Shared Loss: 2.794 \n",
      "Average Independence Loss: 36.369 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 20608.549 \n",
      "Average KL Loss: 104.190 \n",
      "Average Regression Loss: 3.258 \n",
      "Average Shared Loss: 2.799 \n",
      "Average Independence Loss: 35.649 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 20687.704 \n",
      "Average KL Loss: 104.253 \n",
      "Average Regression Loss: 3.316 \n",
      "Average Shared Loss: 2.787 \n",
      "Average Independence Loss: 35.195 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 20647.795 \n",
      "Average KL Loss: 104.221 \n",
      "Average Regression Loss: 3.178 \n",
      "Average Shared Loss: 2.827 \n",
      "Average Independence Loss: 35.573 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 20639.054 \n",
      "Average KL Loss: 104.350 \n",
      "Average Regression Loss: 3.288 \n",
      "Average Shared Loss: 2.815 \n",
      "Average Independence Loss: 35.693 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 20647.692 \n",
      "Average KL Loss: 104.158 \n",
      "Average Regression Loss: 3.133 \n",
      "Average Shared Loss: 2.802 \n",
      "Average Independence Loss: 33.951 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 20651.275 \n",
      "Average KL Loss: 104.232 \n",
      "Average Regression Loss: 3.177 \n",
      "Average Shared Loss: 2.808 \n",
      "Average Independence Loss: 34.600 \n",
      "\n",
      "Average DNA Recon Loss: 39621.834 \n",
      "Average Gene Recon Loss: 11605.265 \n",
      "Average DNA KL Loss: 151.494 \n",
      "Average Gene KL Loss: 16.904 \n",
      "Average Regressor Loss: 5.097 \n",
      "Average RMSE Loss: 2.258 \n",
      "Average Shared MSE Loss: 2.317 \n",
      "Average Shared RMSE Loss: 1.522 \n",
      "Average R2: 0.458 \n",
      "Average Indepence Loss: 16.222 \n",
      "\n",
      "Fold 3\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 20624.713 \n",
      "Average KL Loss: 104.004 \n",
      "Average Regression Loss: 3.805 \n",
      "Average Shared Loss: 2.801 \n",
      "Average Independence Loss: 34.864 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 20263.689 \n",
      "Average KL Loss: 103.957 \n",
      "Average Regression Loss: 3.933 \n",
      "Average Shared Loss: 2.812 \n",
      "Average Independence Loss: 39.271 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 20038.326 \n",
      "Average KL Loss: 103.900 \n",
      "Average Regression Loss: 4.320 \n",
      "Average Shared Loss: 2.804 \n",
      "Average Independence Loss: 40.992 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 19761.942 \n",
      "Average KL Loss: 104.077 \n",
      "Average Regression Loss: 4.271 \n",
      "Average Shared Loss: 2.809 \n",
      "Average Independence Loss: 45.369 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 19522.546 \n",
      "Average KL Loss: 104.129 \n",
      "Average Regression Loss: 4.107 \n",
      "Average Shared Loss: 2.839 \n",
      "Average Independence Loss: 43.472 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 19322.128 \n",
      "Average KL Loss: 104.353 \n",
      "Average Regression Loss: 4.191 \n",
      "Average Shared Loss: 2.797 \n",
      "Average Independence Loss: 45.626 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 19123.895 \n",
      "Average KL Loss: 104.535 \n",
      "Average Regression Loss: 4.043 \n",
      "Average Shared Loss: 2.823 \n",
      "Average Independence Loss: 45.317 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 18967.110 \n",
      "Average KL Loss: 104.493 \n",
      "Average Regression Loss: 4.274 \n",
      "Average Shared Loss: 2.793 \n",
      "Average Independence Loss: 44.172 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 18907.283 \n",
      "Average KL Loss: 104.729 \n",
      "Average Regression Loss: 4.259 \n",
      "Average Shared Loss: 2.805 \n",
      "Average Independence Loss: 44.048 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 18521.584 \n",
      "Average KL Loss: 104.868 \n",
      "Average Regression Loss: 4.076 \n",
      "Average Shared Loss: 2.818 \n",
      "Average Independence Loss: 43.316 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 18441.090 \n",
      "Average KL Loss: 105.095 \n",
      "Average Regression Loss: 4.234 \n",
      "Average Shared Loss: 2.850 \n",
      "Average Independence Loss: 43.807 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 18327.550 \n",
      "Average KL Loss: 105.049 \n",
      "Average Regression Loss: 4.049 \n",
      "Average Shared Loss: 2.809 \n",
      "Average Independence Loss: 40.996 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 18170.007 \n",
      "Average KL Loss: 105.157 \n",
      "Average Regression Loss: 4.176 \n",
      "Average Shared Loss: 2.856 \n",
      "Average Independence Loss: 41.507 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 18052.123 \n",
      "Average KL Loss: 105.303 \n",
      "Average Regression Loss: 4.159 \n",
      "Average Shared Loss: 2.779 \n",
      "Average Independence Loss: 41.761 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 17894.131 \n",
      "Average KL Loss: 105.486 \n",
      "Average Regression Loss: 4.219 \n",
      "Average Shared Loss: 2.815 \n",
      "Average Independence Loss: 41.472 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 17784.064 \n",
      "Average KL Loss: 105.517 \n",
      "Average Regression Loss: 4.087 \n",
      "Average Shared Loss: 2.812 \n",
      "Average Independence Loss: 39.930 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 17650.212 \n",
      "Average KL Loss: 105.712 \n",
      "Average Regression Loss: 4.120 \n",
      "Average Shared Loss: 2.779 \n",
      "Average Independence Loss: 40.384 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 17558.159 \n",
      "Average KL Loss: 105.739 \n",
      "Average Regression Loss: 4.283 \n",
      "Average Shared Loss: 2.798 \n",
      "Average Independence Loss: 39.393 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 17465.658 \n",
      "Average KL Loss: 105.920 \n",
      "Average Regression Loss: 4.053 \n",
      "Average Shared Loss: 2.805 \n",
      "Average Independence Loss: 39.554 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 17299.997 \n",
      "Average KL Loss: 105.878 \n",
      "Average Regression Loss: 4.150 \n",
      "Average Shared Loss: 2.805 \n",
      "Average Independence Loss: 37.704 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 17281.373 \n",
      "Average KL Loss: 106.147 \n",
      "Average Regression Loss: 4.368 \n",
      "Average Shared Loss: 2.805 \n",
      "Average Independence Loss: 38.754 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 17017.800 \n",
      "Average KL Loss: 106.201 \n",
      "Average Regression Loss: 4.145 \n",
      "Average Shared Loss: 2.809 \n",
      "Average Independence Loss: 37.410 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 16997.682 \n",
      "Average KL Loss: 106.268 \n",
      "Average Regression Loss: 4.192 \n",
      "Average Shared Loss: 2.797 \n",
      "Average Independence Loss: 37.772 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 16883.363 \n",
      "Average KL Loss: 106.297 \n",
      "Average Regression Loss: 4.269 \n",
      "Average Shared Loss: 2.788 \n",
      "Average Independence Loss: 35.510 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 16797.847 \n",
      "Average KL Loss: 106.349 \n",
      "Average Regression Loss: 4.183 \n",
      "Average Shared Loss: 2.754 \n",
      "Average Independence Loss: 34.462 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 16697.602 \n",
      "Average KL Loss: 106.448 \n",
      "Average Regression Loss: 4.326 \n",
      "Average Shared Loss: 2.782 \n",
      "Average Independence Loss: 34.338 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 16607.080 \n",
      "Average KL Loss: 106.649 \n",
      "Average Regression Loss: 4.104 \n",
      "Average Shared Loss: 2.792 \n",
      "Average Independence Loss: 35.127 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 16528.522 \n",
      "Average KL Loss: 106.701 \n",
      "Average Regression Loss: 4.276 \n",
      "Average Shared Loss: 2.779 \n",
      "Average Independence Loss: 34.139 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 16523.365 \n",
      "Average KL Loss: 106.706 \n",
      "Average Regression Loss: 4.132 \n",
      "Average Shared Loss: 2.775 \n",
      "Average Independence Loss: 32.941 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 16326.699 \n",
      "Average KL Loss: 106.947 \n",
      "Average Regression Loss: 4.135 \n",
      "Average Shared Loss: 2.768 \n",
      "Average Independence Loss: 34.115 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 16268.659 \n",
      "Average KL Loss: 107.016 \n",
      "Average Regression Loss: 4.163 \n",
      "Average Shared Loss: 2.762 \n",
      "Average Independence Loss: 33.009 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 16166.574 \n",
      "Average KL Loss: 107.058 \n",
      "Average Regression Loss: 4.329 \n",
      "Average Shared Loss: 2.769 \n",
      "Average Independence Loss: 32.544 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 16193.824 \n",
      "Average KL Loss: 107.075 \n",
      "Average Regression Loss: 4.173 \n",
      "Average Shared Loss: 2.754 \n",
      "Average Independence Loss: 31.531 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 15981.765 \n",
      "Average KL Loss: 107.084 \n",
      "Average Regression Loss: 4.198 \n",
      "Average Shared Loss: 2.747 \n",
      "Average Independence Loss: 29.920 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 15895.057 \n",
      "Average KL Loss: 107.289 \n",
      "Average Regression Loss: 4.158 \n",
      "Average Shared Loss: 2.741 \n",
      "Average Independence Loss: 30.975 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 15848.422 \n",
      "Average KL Loss: 107.379 \n",
      "Average Regression Loss: 4.448 \n",
      "Average Shared Loss: 2.742 \n",
      "Average Independence Loss: 30.562 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 15704.691 \n",
      "Average KL Loss: 107.328 \n",
      "Average Regression Loss: 4.167 \n",
      "Average Shared Loss: 2.750 \n",
      "Average Independence Loss: 30.024 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 15686.332 \n",
      "Average KL Loss: 107.321 \n",
      "Average Regression Loss: 4.215 \n",
      "Average Shared Loss: 2.741 \n",
      "Average Independence Loss: 28.955 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 15626.770 \n",
      "Average KL Loss: 107.327 \n",
      "Average Regression Loss: 4.126 \n",
      "Average Shared Loss: 2.731 \n",
      "Average Independence Loss: 27.960 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 15515.050 \n",
      "Average KL Loss: 107.428 \n",
      "Average Regression Loss: 4.267 \n",
      "Average Shared Loss: 2.722 \n",
      "Average Independence Loss: 28.424 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 15540.343 \n",
      "Average KL Loss: 107.290 \n",
      "Average Regression Loss: 4.346 \n",
      "Average Shared Loss: 2.732 \n",
      "Average Independence Loss: 27.143 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 15406.161 \n",
      "Average KL Loss: 107.665 \n",
      "Average Regression Loss: 4.211 \n",
      "Average Shared Loss: 2.737 \n",
      "Average Independence Loss: 28.078 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 15359.170 \n",
      "Average KL Loss: 107.478 \n",
      "Average Regression Loss: 4.226 \n",
      "Average Shared Loss: 2.717 \n",
      "Average Independence Loss: 26.741 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 15326.332 \n",
      "Average KL Loss: 107.482 \n",
      "Average Regression Loss: 4.213 \n",
      "Average Shared Loss: 2.711 \n",
      "Average Independence Loss: 25.737 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 15228.145 \n",
      "Average KL Loss: 107.535 \n",
      "Average Regression Loss: 4.328 \n",
      "Average Shared Loss: 2.692 \n",
      "Average Independence Loss: 25.034 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 15184.428 \n",
      "Average KL Loss: 107.689 \n",
      "Average Regression Loss: 4.240 \n",
      "Average Shared Loss: 2.702 \n",
      "Average Independence Loss: 26.639 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 15124.873 \n",
      "Average KL Loss: 107.633 \n",
      "Average Regression Loss: 4.321 \n",
      "Average Shared Loss: 2.703 \n",
      "Average Independence Loss: 25.711 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 15183.718 \n",
      "Average KL Loss: 107.521 \n",
      "Average Regression Loss: 4.358 \n",
      "Average Shared Loss: 2.679 \n",
      "Average Independence Loss: 24.232 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 15041.947 \n",
      "Average KL Loss: 107.691 \n",
      "Average Regression Loss: 4.321 \n",
      "Average Shared Loss: 2.687 \n",
      "Average Independence Loss: 24.700 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 15013.687 \n",
      "Average KL Loss: 107.731 \n",
      "Average Regression Loss: 4.438 \n",
      "Average Shared Loss: 2.695 \n",
      "Average Independence Loss: 25.245 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 15091.258 \n",
      "Average KL Loss: 107.827 \n",
      "Average Regression Loss: 4.261 \n",
      "Average Shared Loss: 2.670 \n",
      "Average Independence Loss: 24.094 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 15289.288 \n",
      "Average KL Loss: 107.935 \n",
      "Average Regression Loss: 3.629 \n",
      "Average Shared Loss: 2.691 \n",
      "Average Independence Loss: 22.728 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 15403.148 \n",
      "Average KL Loss: 107.898 \n",
      "Average Regression Loss: 3.406 \n",
      "Average Shared Loss: 2.672 \n",
      "Average Independence Loss: 20.719 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 15380.473 \n",
      "Average KL Loss: 107.841 \n",
      "Average Regression Loss: 3.278 \n",
      "Average Shared Loss: 2.677 \n",
      "Average Independence Loss: 19.611 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 15386.592 \n",
      "Average KL Loss: 107.932 \n",
      "Average Regression Loss: 3.281 \n",
      "Average Shared Loss: 2.681 \n",
      "Average Independence Loss: 18.770 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 15455.199 \n",
      "Average KL Loss: 107.923 \n",
      "Average Regression Loss: 3.249 \n",
      "Average Shared Loss: 2.675 \n",
      "Average Independence Loss: 18.386 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 15445.081 \n",
      "Average KL Loss: 107.832 \n",
      "Average Regression Loss: 3.267 \n",
      "Average Shared Loss: 2.692 \n",
      "Average Independence Loss: 16.372 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 15342.753 \n",
      "Average KL Loss: 107.915 \n",
      "Average Regression Loss: 3.073 \n",
      "Average Shared Loss: 2.690 \n",
      "Average Independence Loss: 16.561 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 15399.456 \n",
      "Average KL Loss: 107.870 \n",
      "Average Regression Loss: 2.967 \n",
      "Average Shared Loss: 2.675 \n",
      "Average Independence Loss: 16.632 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 15321.183 \n",
      "Average KL Loss: 107.910 \n",
      "Average Regression Loss: 2.902 \n",
      "Average Shared Loss: 2.664 \n",
      "Average Independence Loss: 15.795 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 15332.104 \n",
      "Average KL Loss: 107.960 \n",
      "Average Regression Loss: 2.845 \n",
      "Average Shared Loss: 2.676 \n",
      "Average Independence Loss: 16.103 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 15398.778 \n",
      "Average KL Loss: 107.956 \n",
      "Average Regression Loss: 2.799 \n",
      "Average Shared Loss: 2.673 \n",
      "Average Independence Loss: 15.363 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 15416.113 \n",
      "Average KL Loss: 107.945 \n",
      "Average Regression Loss: 2.764 \n",
      "Average Shared Loss: 2.674 \n",
      "Average Independence Loss: 14.975 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 15418.433 \n",
      "Average KL Loss: 107.853 \n",
      "Average Regression Loss: 2.709 \n",
      "Average Shared Loss: 2.680 \n",
      "Average Independence Loss: 13.739 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 15360.959 \n",
      "Average KL Loss: 107.885 \n",
      "Average Regression Loss: 2.856 \n",
      "Average Shared Loss: 2.668 \n",
      "Average Independence Loss: 14.670 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 15387.829 \n",
      "Average KL Loss: 107.988 \n",
      "Average Regression Loss: 2.537 \n",
      "Average Shared Loss: 2.659 \n",
      "Average Independence Loss: 14.101 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 15410.952 \n",
      "Average KL Loss: 107.951 \n",
      "Average Regression Loss: 2.560 \n",
      "Average Shared Loss: 2.661 \n",
      "Average Independence Loss: 14.430 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 15433.362 \n",
      "Average KL Loss: 107.933 \n",
      "Average Regression Loss: 2.511 \n",
      "Average Shared Loss: 2.678 \n",
      "Average Independence Loss: 14.164 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 15460.993 \n",
      "Average KL Loss: 107.989 \n",
      "Average Regression Loss: 2.509 \n",
      "Average Shared Loss: 2.666 \n",
      "Average Independence Loss: 14.050 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 15361.634 \n",
      "Average KL Loss: 108.040 \n",
      "Average Regression Loss: 2.517 \n",
      "Average Shared Loss: 2.667 \n",
      "Average Independence Loss: 13.686 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 15414.330 \n",
      "Average KL Loss: 107.909 \n",
      "Average Regression Loss: 2.503 \n",
      "Average Shared Loss: 2.682 \n",
      "Average Independence Loss: 13.295 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 15401.670 \n",
      "Average KL Loss: 107.817 \n",
      "Average Regression Loss: 2.398 \n",
      "Average Shared Loss: 2.666 \n",
      "Average Independence Loss: 12.826 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 15404.228 \n",
      "Average KL Loss: 108.030 \n",
      "Average Regression Loss: 2.474 \n",
      "Average Shared Loss: 2.668 \n",
      "Average Independence Loss: 13.679 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 15327.389 \n",
      "Average KL Loss: 107.856 \n",
      "Average Regression Loss: 2.343 \n",
      "Average Shared Loss: 2.664 \n",
      "Average Independence Loss: 12.287 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 15477.707 \n",
      "Average KL Loss: 107.913 \n",
      "Average Regression Loss: 2.414 \n",
      "Average Shared Loss: 2.663 \n",
      "Average Independence Loss: 13.518 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 15370.571 \n",
      "Average KL Loss: 107.936 \n",
      "Average Regression Loss: 2.335 \n",
      "Average Shared Loss: 2.666 \n",
      "Average Independence Loss: 12.322 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 15377.429 \n",
      "Average KL Loss: 107.790 \n",
      "Average Regression Loss: 2.308 \n",
      "Average Shared Loss: 2.659 \n",
      "Average Independence Loss: 12.011 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 15449.261 \n",
      "Average KL Loss: 107.927 \n",
      "Average Regression Loss: 2.324 \n",
      "Average Shared Loss: 2.656 \n",
      "Average Independence Loss: 12.878 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 15419.613 \n",
      "Average KL Loss: 107.883 \n",
      "Average Regression Loss: 2.384 \n",
      "Average Shared Loss: 2.657 \n",
      "Average Independence Loss: 11.944 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 15484.391 \n",
      "Average KL Loss: 107.896 \n",
      "Average Regression Loss: 2.285 \n",
      "Average Shared Loss: 2.671 \n",
      "Average Independence Loss: 11.813 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 15326.526 \n",
      "Average KL Loss: 107.861 \n",
      "Average Regression Loss: 2.215 \n",
      "Average Shared Loss: 2.665 \n",
      "Average Independence Loss: 11.794 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 15479.351 \n",
      "Average KL Loss: 107.860 \n",
      "Average Regression Loss: 2.292 \n",
      "Average Shared Loss: 2.666 \n",
      "Average Independence Loss: 12.784 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 15382.871 \n",
      "Average KL Loss: 107.736 \n",
      "Average Regression Loss: 2.149 \n",
      "Average Shared Loss: 2.667 \n",
      "Average Independence Loss: 11.163 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 15360.869 \n",
      "Average KL Loss: 107.814 \n",
      "Average Regression Loss: 2.111 \n",
      "Average Shared Loss: 2.669 \n",
      "Average Independence Loss: 11.713 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 15359.299 \n",
      "Average KL Loss: 107.882 \n",
      "Average Regression Loss: 1.994 \n",
      "Average Shared Loss: 2.650 \n",
      "Average Independence Loss: 12.004 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 15368.214 \n",
      "Average KL Loss: 107.769 \n",
      "Average Regression Loss: 2.060 \n",
      "Average Shared Loss: 2.666 \n",
      "Average Independence Loss: 10.725 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 15389.984 \n",
      "Average KL Loss: 107.723 \n",
      "Average Regression Loss: 2.072 \n",
      "Average Shared Loss: 2.659 \n",
      "Average Independence Loss: 10.581 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 15441.130 \n",
      "Average KL Loss: 107.745 \n",
      "Average Regression Loss: 2.086 \n",
      "Average Shared Loss: 2.662 \n",
      "Average Independence Loss: 10.997 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 15406.293 \n",
      "Average KL Loss: 107.735 \n",
      "Average Regression Loss: 2.022 \n",
      "Average Shared Loss: 2.664 \n",
      "Average Independence Loss: 11.363 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 15433.876 \n",
      "Average KL Loss: 107.796 \n",
      "Average Regression Loss: 2.113 \n",
      "Average Shared Loss: 2.664 \n",
      "Average Independence Loss: 11.393 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 15383.908 \n",
      "Average KL Loss: 107.583 \n",
      "Average Regression Loss: 1.940 \n",
      "Average Shared Loss: 2.647 \n",
      "Average Independence Loss: 9.762 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 15366.333 \n",
      "Average KL Loss: 107.723 \n",
      "Average Regression Loss: 1.950 \n",
      "Average Shared Loss: 2.666 \n",
      "Average Independence Loss: 10.534 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 15422.842 \n",
      "Average KL Loss: 107.650 \n",
      "Average Regression Loss: 2.019 \n",
      "Average Shared Loss: 2.659 \n",
      "Average Independence Loss: 9.641 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 15475.848 \n",
      "Average KL Loss: 107.661 \n",
      "Average Regression Loss: 1.953 \n",
      "Average Shared Loss: 2.661 \n",
      "Average Independence Loss: 10.314 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 15409.405 \n",
      "Average KL Loss: 107.700 \n",
      "Average Regression Loss: 1.900 \n",
      "Average Shared Loss: 2.662 \n",
      "Average Independence Loss: 9.876 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 15341.343 \n",
      "Average KL Loss: 107.817 \n",
      "Average Regression Loss: 1.962 \n",
      "Average Shared Loss: 2.661 \n",
      "Average Independence Loss: 10.474 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 15359.577 \n",
      "Average KL Loss: 107.649 \n",
      "Average Regression Loss: 1.840 \n",
      "Average Shared Loss: 2.658 \n",
      "Average Independence Loss: 9.470 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 15374.462 \n",
      "Average KL Loss: 107.594 \n",
      "Average Regression Loss: 1.960 \n",
      "Average Shared Loss: 2.654 \n",
      "Average Independence Loss: 10.403 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 15436.846 \n",
      "Average KL Loss: 107.702 \n",
      "Average Regression Loss: 1.831 \n",
      "Average Shared Loss: 2.657 \n",
      "Average Independence Loss: 10.256 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 15333.887 \n",
      "Average KL Loss: 107.671 \n",
      "Average Regression Loss: 1.864 \n",
      "Average Shared Loss: 2.668 \n",
      "Average Independence Loss: 9.494 \n",
      "\n",
      "Average DNA Recon Loss: 35467.252 \n",
      "Average Gene Recon Loss: 6054.280 \n",
      "Average DNA KL Loss: 183.099 \n",
      "Average Gene KL Loss: 17.847 \n",
      "Average Regressor Loss: 3.407 \n",
      "Average RMSE Loss: 1.846 \n",
      "Average Shared MSE Loss: 2.177 \n",
      "Average Shared RMSE Loss: 1.475 \n",
      "Average R2: 0.591 \n",
      "Average Indepence Loss: 10.419 \n",
      "\n",
      "Fold 4\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 15881.512 \n",
      "Average KL Loss: 107.540 \n",
      "Average Regression Loss: 2.385 \n",
      "Average Shared Loss: 2.658 \n",
      "Average Independence Loss: 9.968 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 15623.558 \n",
      "Average KL Loss: 107.566 \n",
      "Average Regression Loss: 2.576 \n",
      "Average Shared Loss: 2.658 \n",
      "Average Independence Loss: 10.718 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 15538.580 \n",
      "Average KL Loss: 107.431 \n",
      "Average Regression Loss: 2.544 \n",
      "Average Shared Loss: 2.663 \n",
      "Average Independence Loss: 11.647 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 15370.040 \n",
      "Average KL Loss: 107.472 \n",
      "Average Regression Loss: 2.641 \n",
      "Average Shared Loss: 2.647 \n",
      "Average Independence Loss: 12.052 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 15137.939 \n",
      "Average KL Loss: 107.487 \n",
      "Average Regression Loss: 2.563 \n",
      "Average Shared Loss: 2.665 \n",
      "Average Independence Loss: 12.049 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 15142.629 \n",
      "Average KL Loss: 107.556 \n",
      "Average Regression Loss: 2.740 \n",
      "Average Shared Loss: 2.651 \n",
      "Average Independence Loss: 11.545 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 15045.281 \n",
      "Average KL Loss: 107.807 \n",
      "Average Regression Loss: 2.632 \n",
      "Average Shared Loss: 2.660 \n",
      "Average Independence Loss: 12.177 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 14951.460 \n",
      "Average KL Loss: 107.760 \n",
      "Average Regression Loss: 2.617 \n",
      "Average Shared Loss: 2.646 \n",
      "Average Independence Loss: 11.178 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 14863.668 \n",
      "Average KL Loss: 107.796 \n",
      "Average Regression Loss: 2.692 \n",
      "Average Shared Loss: 2.635 \n",
      "Average Independence Loss: 10.838 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 14729.269 \n",
      "Average KL Loss: 108.144 \n",
      "Average Regression Loss: 2.580 \n",
      "Average Shared Loss: 2.640 \n",
      "Average Independence Loss: 12.192 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 14662.038 \n",
      "Average KL Loss: 108.107 \n",
      "Average Regression Loss: 2.588 \n",
      "Average Shared Loss: 2.626 \n",
      "Average Independence Loss: 11.607 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 14514.841 \n",
      "Average KL Loss: 108.076 \n",
      "Average Regression Loss: 2.575 \n",
      "Average Shared Loss: 2.626 \n",
      "Average Independence Loss: 10.806 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 14424.311 \n",
      "Average KL Loss: 108.136 \n",
      "Average Regression Loss: 2.609 \n",
      "Average Shared Loss: 2.622 \n",
      "Average Independence Loss: 10.912 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 14536.211 \n",
      "Average KL Loss: 108.188 \n",
      "Average Regression Loss: 2.688 \n",
      "Average Shared Loss: 2.619 \n",
      "Average Independence Loss: 10.886 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 14413.179 \n",
      "Average KL Loss: 108.103 \n",
      "Average Regression Loss: 2.629 \n",
      "Average Shared Loss: 2.604 \n",
      "Average Independence Loss: 10.788 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 14386.169 \n",
      "Average KL Loss: 108.237 \n",
      "Average Regression Loss: 2.626 \n",
      "Average Shared Loss: 2.600 \n",
      "Average Independence Loss: 10.886 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 14228.638 \n",
      "Average KL Loss: 108.212 \n",
      "Average Regression Loss: 2.664 \n",
      "Average Shared Loss: 2.612 \n",
      "Average Independence Loss: 10.270 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 14184.577 \n",
      "Average KL Loss: 108.121 \n",
      "Average Regression Loss: 2.664 \n",
      "Average Shared Loss: 2.600 \n",
      "Average Independence Loss: 10.151 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 14203.870 \n",
      "Average KL Loss: 108.195 \n",
      "Average Regression Loss: 2.652 \n",
      "Average Shared Loss: 2.584 \n",
      "Average Independence Loss: 9.470 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 14086.746 \n",
      "Average KL Loss: 108.226 \n",
      "Average Regression Loss: 2.464 \n",
      "Average Shared Loss: 2.576 \n",
      "Average Independence Loss: 9.525 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 14014.376 \n",
      "Average KL Loss: 108.173 \n",
      "Average Regression Loss: 2.606 \n",
      "Average Shared Loss: 2.580 \n",
      "Average Independence Loss: 9.211 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 13953.940 \n",
      "Average KL Loss: 108.087 \n",
      "Average Regression Loss: 2.633 \n",
      "Average Shared Loss: 2.581 \n",
      "Average Independence Loss: 8.705 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 13883.820 \n",
      "Average KL Loss: 108.170 \n",
      "Average Regression Loss: 2.695 \n",
      "Average Shared Loss: 2.569 \n",
      "Average Independence Loss: 8.664 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 13805.084 \n",
      "Average KL Loss: 108.201 \n",
      "Average Regression Loss: 2.502 \n",
      "Average Shared Loss: 2.562 \n",
      "Average Independence Loss: 8.841 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 13865.306 \n",
      "Average KL Loss: 108.115 \n",
      "Average Regression Loss: 2.695 \n",
      "Average Shared Loss: 2.554 \n",
      "Average Independence Loss: 8.132 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 13796.498 \n",
      "Average KL Loss: 108.126 \n",
      "Average Regression Loss: 2.664 \n",
      "Average Shared Loss: 2.558 \n",
      "Average Independence Loss: 8.086 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 13767.968 \n",
      "Average KL Loss: 108.088 \n",
      "Average Regression Loss: 2.517 \n",
      "Average Shared Loss: 2.538 \n",
      "Average Independence Loss: 8.563 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 13771.630 \n",
      "Average KL Loss: 108.080 \n",
      "Average Regression Loss: 2.608 \n",
      "Average Shared Loss: 2.545 \n",
      "Average Independence Loss: 7.973 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 13621.181 \n",
      "Average KL Loss: 107.970 \n",
      "Average Regression Loss: 2.706 \n",
      "Average Shared Loss: 2.542 \n",
      "Average Independence Loss: 7.663 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 13603.231 \n",
      "Average KL Loss: 107.971 \n",
      "Average Regression Loss: 2.551 \n",
      "Average Shared Loss: 2.533 \n",
      "Average Independence Loss: 7.492 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 13587.410 \n",
      "Average KL Loss: 107.948 \n",
      "Average Regression Loss: 2.556 \n",
      "Average Shared Loss: 2.526 \n",
      "Average Independence Loss: 7.940 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 13513.124 \n",
      "Average KL Loss: 107.861 \n",
      "Average Regression Loss: 2.729 \n",
      "Average Shared Loss: 2.524 \n",
      "Average Independence Loss: 7.158 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 13546.132 \n",
      "Average KL Loss: 107.908 \n",
      "Average Regression Loss: 2.587 \n",
      "Average Shared Loss: 2.513 \n",
      "Average Independence Loss: 7.725 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 13479.764 \n",
      "Average KL Loss: 107.844 \n",
      "Average Regression Loss: 2.579 \n",
      "Average Shared Loss: 2.503 \n",
      "Average Independence Loss: 7.142 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 13450.500 \n",
      "Average KL Loss: 107.815 \n",
      "Average Regression Loss: 2.628 \n",
      "Average Shared Loss: 2.506 \n",
      "Average Independence Loss: 7.145 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 13436.807 \n",
      "Average KL Loss: 107.806 \n",
      "Average Regression Loss: 2.607 \n",
      "Average Shared Loss: 2.503 \n",
      "Average Independence Loss: 7.120 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 13434.377 \n",
      "Average KL Loss: 107.746 \n",
      "Average Regression Loss: 2.731 \n",
      "Average Shared Loss: 2.498 \n",
      "Average Independence Loss: 6.945 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 13383.833 \n",
      "Average KL Loss: 107.655 \n",
      "Average Regression Loss: 2.557 \n",
      "Average Shared Loss: 2.483 \n",
      "Average Independence Loss: 6.691 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 13256.643 \n",
      "Average KL Loss: 107.643 \n",
      "Average Regression Loss: 2.629 \n",
      "Average Shared Loss: 2.481 \n",
      "Average Independence Loss: 6.626 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 13304.567 \n",
      "Average KL Loss: 107.498 \n",
      "Average Regression Loss: 2.600 \n",
      "Average Shared Loss: 2.481 \n",
      "Average Independence Loss: 6.294 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 13251.070 \n",
      "Average KL Loss: 107.491 \n",
      "Average Regression Loss: 2.597 \n",
      "Average Shared Loss: 2.473 \n",
      "Average Independence Loss: 6.360 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 13203.130 \n",
      "Average KL Loss: 107.387 \n",
      "Average Regression Loss: 2.678 \n",
      "Average Shared Loss: 2.460 \n",
      "Average Independence Loss: 6.079 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 13188.593 \n",
      "Average KL Loss: 107.435 \n",
      "Average Regression Loss: 2.634 \n",
      "Average Shared Loss: 2.459 \n",
      "Average Independence Loss: 6.251 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 13180.656 \n",
      "Average KL Loss: 107.281 \n",
      "Average Regression Loss: 2.715 \n",
      "Average Shared Loss: 2.460 \n",
      "Average Independence Loss: 5.914 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 13144.879 \n",
      "Average KL Loss: 107.297 \n",
      "Average Regression Loss: 2.668 \n",
      "Average Shared Loss: 2.451 \n",
      "Average Independence Loss: 5.979 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 13119.250 \n",
      "Average KL Loss: 107.436 \n",
      "Average Regression Loss: 2.654 \n",
      "Average Shared Loss: 2.445 \n",
      "Average Independence Loss: 6.080 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 13122.985 \n",
      "Average KL Loss: 107.275 \n",
      "Average Regression Loss: 2.671 \n",
      "Average Shared Loss: 2.438 \n",
      "Average Independence Loss: 5.962 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 13064.442 \n",
      "Average KL Loss: 107.241 \n",
      "Average Regression Loss: 2.657 \n",
      "Average Shared Loss: 2.436 \n",
      "Average Independence Loss: 5.744 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 13045.519 \n",
      "Average KL Loss: 107.130 \n",
      "Average Regression Loss: 2.707 \n",
      "Average Shared Loss: 2.433 \n",
      "Average Independence Loss: 5.576 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 13031.835 \n",
      "Average KL Loss: 107.104 \n",
      "Average Regression Loss: 2.556 \n",
      "Average Shared Loss: 2.426 \n",
      "Average Independence Loss: 5.687 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 13117.800 \n",
      "Average KL Loss: 107.089 \n",
      "Average Regression Loss: 2.508 \n",
      "Average Shared Loss: 2.416 \n",
      "Average Independence Loss: 5.234 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 13279.102 \n",
      "Average KL Loss: 107.068 \n",
      "Average Regression Loss: 2.464 \n",
      "Average Shared Loss: 2.422 \n",
      "Average Independence Loss: 4.794 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 13344.807 \n",
      "Average KL Loss: 107.015 \n",
      "Average Regression Loss: 2.373 \n",
      "Average Shared Loss: 2.423 \n",
      "Average Independence Loss: 4.211 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 13367.549 \n",
      "Average KL Loss: 107.056 \n",
      "Average Regression Loss: 2.270 \n",
      "Average Shared Loss: 2.425 \n",
      "Average Independence Loss: 3.856 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 13297.666 \n",
      "Average KL Loss: 107.096 \n",
      "Average Regression Loss: 1.979 \n",
      "Average Shared Loss: 2.420 \n",
      "Average Independence Loss: 3.717 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 13334.231 \n",
      "Average KL Loss: 107.066 \n",
      "Average Regression Loss: 1.986 \n",
      "Average Shared Loss: 2.419 \n",
      "Average Independence Loss: 3.454 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 13372.231 \n",
      "Average KL Loss: 107.033 \n",
      "Average Regression Loss: 1.834 \n",
      "Average Shared Loss: 2.414 \n",
      "Average Independence Loss: 3.056 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 13395.429 \n",
      "Average KL Loss: 106.950 \n",
      "Average Regression Loss: 1.837 \n",
      "Average Shared Loss: 2.419 \n",
      "Average Independence Loss: 2.889 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 13326.150 \n",
      "Average KL Loss: 107.047 \n",
      "Average Regression Loss: 1.789 \n",
      "Average Shared Loss: 2.419 \n",
      "Average Independence Loss: 2.781 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 13348.987 \n",
      "Average KL Loss: 107.012 \n",
      "Average Regression Loss: 1.782 \n",
      "Average Shared Loss: 2.422 \n",
      "Average Independence Loss: 2.722 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 13370.643 \n",
      "Average KL Loss: 107.019 \n",
      "Average Regression Loss: 1.785 \n",
      "Average Shared Loss: 2.409 \n",
      "Average Independence Loss: 2.464 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 13379.002 \n",
      "Average KL Loss: 107.096 \n",
      "Average Regression Loss: 1.768 \n",
      "Average Shared Loss: 2.417 \n",
      "Average Independence Loss: 2.330 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 13330.381 \n",
      "Average KL Loss: 107.043 \n",
      "Average Regression Loss: 1.758 \n",
      "Average Shared Loss: 2.417 \n",
      "Average Independence Loss: 2.230 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 13350.897 \n",
      "Average KL Loss: 106.982 \n",
      "Average Regression Loss: 1.702 \n",
      "Average Shared Loss: 2.425 \n",
      "Average Independence Loss: 2.095 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 13309.395 \n",
      "Average KL Loss: 107.042 \n",
      "Average Regression Loss: 1.599 \n",
      "Average Shared Loss: 2.412 \n",
      "Average Independence Loss: 1.961 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 13366.732 \n",
      "Average KL Loss: 106.919 \n",
      "Average Regression Loss: 1.685 \n",
      "Average Shared Loss: 2.413 \n",
      "Average Independence Loss: 1.736 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 13350.581 \n",
      "Average KL Loss: 107.002 \n",
      "Average Regression Loss: 1.637 \n",
      "Average Shared Loss: 2.411 \n",
      "Average Independence Loss: 1.641 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 13412.567 \n",
      "Average KL Loss: 107.009 \n",
      "Average Regression Loss: 1.676 \n",
      "Average Shared Loss: 2.405 \n",
      "Average Independence Loss: 1.540 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 13299.002 \n",
      "Average KL Loss: 107.041 \n",
      "Average Regression Loss: 1.458 \n",
      "Average Shared Loss: 2.414 \n",
      "Average Independence Loss: 1.493 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 13377.353 \n",
      "Average KL Loss: 106.974 \n",
      "Average Regression Loss: 1.547 \n",
      "Average Shared Loss: 2.410 \n",
      "Average Independence Loss: 1.276 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 13413.208 \n",
      "Average KL Loss: 106.985 \n",
      "Average Regression Loss: 1.588 \n",
      "Average Shared Loss: 2.411 \n",
      "Average Independence Loss: 1.084 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 13344.728 \n",
      "Average KL Loss: 106.918 \n",
      "Average Regression Loss: 1.580 \n",
      "Average Shared Loss: 2.408 \n",
      "Average Independence Loss: 1.044 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 13333.355 \n",
      "Average KL Loss: 107.039 \n",
      "Average Regression Loss: 1.553 \n",
      "Average Shared Loss: 2.403 \n",
      "Average Independence Loss: 0.925 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 13375.483 \n",
      "Average KL Loss: 107.021 \n",
      "Average Regression Loss: 1.544 \n",
      "Average Shared Loss: 2.409 \n",
      "Average Independence Loss: 0.910 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 13368.817 \n",
      "Average KL Loss: 106.982 \n",
      "Average Regression Loss: 1.596 \n",
      "Average Shared Loss: 2.413 \n",
      "Average Independence Loss: 0.737 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 13375.441 \n",
      "Average KL Loss: 107.030 \n",
      "Average Regression Loss: 1.646 \n",
      "Average Shared Loss: 2.409 \n",
      "Average Independence Loss: 0.658 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 13337.604 \n",
      "Average KL Loss: 107.043 \n",
      "Average Regression Loss: 1.523 \n",
      "Average Shared Loss: 2.411 \n",
      "Average Independence Loss: 0.562 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 13395.808 \n",
      "Average KL Loss: 106.959 \n",
      "Average Regression Loss: 1.448 \n",
      "Average Shared Loss: 2.411 \n",
      "Average Independence Loss: 0.426 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 13362.757 \n",
      "Average KL Loss: 106.915 \n",
      "Average Regression Loss: 1.437 \n",
      "Average Shared Loss: 2.403 \n",
      "Average Independence Loss: 0.332 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 13338.629 \n",
      "Average KL Loss: 107.057 \n",
      "Average Regression Loss: 1.375 \n",
      "Average Shared Loss: 2.407 \n",
      "Average Independence Loss: 0.274 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 13344.806 \n",
      "Average KL Loss: 106.982 \n",
      "Average Regression Loss: 1.426 \n",
      "Average Shared Loss: 2.413 \n",
      "Average Independence Loss: 0.173 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 13293.482 \n",
      "Average KL Loss: 107.022 \n",
      "Average Regression Loss: 1.548 \n",
      "Average Shared Loss: 2.411 \n",
      "Average Independence Loss: 0.095 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 13293.245 \n",
      "Average KL Loss: 106.999 \n",
      "Average Regression Loss: 1.388 \n",
      "Average Shared Loss: 2.407 \n",
      "Average Independence Loss: 0.037 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 13318.237 \n",
      "Average KL Loss: 106.971 \n",
      "Average Regression Loss: 1.351 \n",
      "Average Shared Loss: 2.411 \n",
      "Average Independence Loss: 0.030 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 13377.999 \n",
      "Average KL Loss: 106.962 \n",
      "Average Regression Loss: 1.426 \n",
      "Average Shared Loss: 2.407 \n",
      "Average Independence Loss: 0.022 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 13371.392 \n",
      "Average KL Loss: 107.030 \n",
      "Average Regression Loss: 1.394 \n",
      "Average Shared Loss: 2.409 \n",
      "Average Independence Loss: 0.023 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 13301.417 \n",
      "Average KL Loss: 106.978 \n",
      "Average Regression Loss: 1.436 \n",
      "Average Shared Loss: 2.404 \n",
      "Average Independence Loss: 0.020 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 13362.447 \n",
      "Average KL Loss: 107.003 \n",
      "Average Regression Loss: 1.274 \n",
      "Average Shared Loss: 2.402 \n",
      "Average Independence Loss: 0.019 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 13396.124 \n",
      "Average KL Loss: 106.995 \n",
      "Average Regression Loss: 1.485 \n",
      "Average Shared Loss: 2.410 \n",
      "Average Independence Loss: 0.018 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 13345.193 \n",
      "Average KL Loss: 107.002 \n",
      "Average Regression Loss: 1.309 \n",
      "Average Shared Loss: 2.405 \n",
      "Average Independence Loss: 0.016 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 13388.300 \n",
      "Average KL Loss: 106.997 \n",
      "Average Regression Loss: 1.353 \n",
      "Average Shared Loss: 2.407 \n",
      "Average Independence Loss: 0.016 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 13367.937 \n",
      "Average KL Loss: 106.953 \n",
      "Average Regression Loss: 1.289 \n",
      "Average Shared Loss: 2.409 \n",
      "Average Independence Loss: 0.014 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 13316.673 \n",
      "Average KL Loss: 106.921 \n",
      "Average Regression Loss: 1.293 \n",
      "Average Shared Loss: 2.396 \n",
      "Average Independence Loss: 0.014 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 13315.730 \n",
      "Average KL Loss: 106.976 \n",
      "Average Regression Loss: 1.525 \n",
      "Average Shared Loss: 2.398 \n",
      "Average Independence Loss: 0.013 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 13360.191 \n",
      "Average KL Loss: 106.911 \n",
      "Average Regression Loss: 1.528 \n",
      "Average Shared Loss: 2.398 \n",
      "Average Independence Loss: 0.013 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 13311.915 \n",
      "Average KL Loss: 106.923 \n",
      "Average Regression Loss: 1.418 \n",
      "Average Shared Loss: 2.398 \n",
      "Average Independence Loss: 0.015 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 13368.977 \n",
      "Average KL Loss: 106.988 \n",
      "Average Regression Loss: 1.435 \n",
      "Average Shared Loss: 2.406 \n",
      "Average Independence Loss: 0.014 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 13346.562 \n",
      "Average KL Loss: 106.914 \n",
      "Average Regression Loss: 1.303 \n",
      "Average Shared Loss: 2.403 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 13336.162 \n",
      "Average KL Loss: 106.939 \n",
      "Average Regression Loss: 1.272 \n",
      "Average Shared Loss: 2.396 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 13354.814 \n",
      "Average KL Loss: 106.874 \n",
      "Average Regression Loss: 1.298 \n",
      "Average Shared Loss: 2.401 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "Average DNA Recon Loss: 29284.029 \n",
      "Average Gene Recon Loss: 3304.276 \n",
      "Average DNA KL Loss: 205.896 \n",
      "Average Gene KL Loss: 5.069 \n",
      "Average Regressor Loss: 2.433 \n",
      "Average RMSE Loss: 1.560 \n",
      "Average Shared MSE Loss: 2.036 \n",
      "Average Shared RMSE Loss: 1.427 \n",
      "Average R2: 0.705 \n",
      "Average Indepence Loss: 0.014 \n",
      "\n",
      "Fold 5\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 13570.390 \n",
      "Average KL Loss: 107.049 \n",
      "Average Regression Loss: 1.538 \n",
      "Average Shared Loss: 2.386 \n",
      "Average Independence Loss: 0.112 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 13470.432 \n",
      "Average KL Loss: 107.058 \n",
      "Average Regression Loss: 1.656 \n",
      "Average Shared Loss: 2.387 \n",
      "Average Independence Loss: 0.276 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 13405.104 \n",
      "Average KL Loss: 107.113 \n",
      "Average Regression Loss: 1.819 \n",
      "Average Shared Loss: 2.392 \n",
      "Average Independence Loss: 0.382 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 13291.358 \n",
      "Average KL Loss: 107.294 \n",
      "Average Regression Loss: 1.834 \n",
      "Average Shared Loss: 2.384 \n",
      "Average Independence Loss: 0.495 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 13300.227 \n",
      "Average KL Loss: 107.365 \n",
      "Average Regression Loss: 1.839 \n",
      "Average Shared Loss: 2.389 \n",
      "Average Independence Loss: 0.565 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 13129.634 \n",
      "Average KL Loss: 107.530 \n",
      "Average Regression Loss: 1.789 \n",
      "Average Shared Loss: 2.374 \n",
      "Average Independence Loss: 0.627 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 13098.541 \n",
      "Average KL Loss: 107.489 \n",
      "Average Regression Loss: 1.851 \n",
      "Average Shared Loss: 2.372 \n",
      "Average Independence Loss: 0.664 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 12951.414 \n",
      "Average KL Loss: 107.539 \n",
      "Average Regression Loss: 1.758 \n",
      "Average Shared Loss: 2.374 \n",
      "Average Independence Loss: 0.723 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 12973.725 \n",
      "Average KL Loss: 107.444 \n",
      "Average Regression Loss: 1.897 \n",
      "Average Shared Loss: 2.370 \n",
      "Average Independence Loss: 0.745 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 12853.580 \n",
      "Average KL Loss: 107.395 \n",
      "Average Regression Loss: 1.731 \n",
      "Average Shared Loss: 2.363 \n",
      "Average Independence Loss: 0.780 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 12815.867 \n",
      "Average KL Loss: 107.339 \n",
      "Average Regression Loss: 1.770 \n",
      "Average Shared Loss: 2.347 \n",
      "Average Independence Loss: 0.792 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 12814.379 \n",
      "Average KL Loss: 107.348 \n",
      "Average Regression Loss: 1.817 \n",
      "Average Shared Loss: 2.354 \n",
      "Average Independence Loss: 0.867 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 12823.989 \n",
      "Average KL Loss: 107.266 \n",
      "Average Regression Loss: 1.746 \n",
      "Average Shared Loss: 2.342 \n",
      "Average Independence Loss: 0.893 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 12684.452 \n",
      "Average KL Loss: 107.131 \n",
      "Average Regression Loss: 1.744 \n",
      "Average Shared Loss: 2.341 \n",
      "Average Independence Loss: 0.908 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 12664.377 \n",
      "Average KL Loss: 107.105 \n",
      "Average Regression Loss: 1.721 \n",
      "Average Shared Loss: 2.345 \n",
      "Average Independence Loss: 0.928 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 12697.304 \n",
      "Average KL Loss: 107.072 \n",
      "Average Regression Loss: 1.698 \n",
      "Average Shared Loss: 2.338 \n",
      "Average Independence Loss: 0.981 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 12637.503 \n",
      "Average KL Loss: 106.908 \n",
      "Average Regression Loss: 1.790 \n",
      "Average Shared Loss: 2.332 \n",
      "Average Independence Loss: 0.972 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 12671.505 \n",
      "Average KL Loss: 106.834 \n",
      "Average Regression Loss: 1.922 \n",
      "Average Shared Loss: 2.315 \n",
      "Average Independence Loss: 0.964 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 12600.580 \n",
      "Average KL Loss: 106.792 \n",
      "Average Regression Loss: 1.947 \n",
      "Average Shared Loss: 2.310 \n",
      "Average Independence Loss: 1.023 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 12545.896 \n",
      "Average KL Loss: 106.835 \n",
      "Average Regression Loss: 1.709 \n",
      "Average Shared Loss: 2.310 \n",
      "Average Independence Loss: 1.072 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 12538.627 \n",
      "Average KL Loss: 106.658 \n",
      "Average Regression Loss: 1.756 \n",
      "Average Shared Loss: 2.307 \n",
      "Average Independence Loss: 1.036 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 12498.679 \n",
      "Average KL Loss: 106.642 \n",
      "Average Regression Loss: 1.705 \n",
      "Average Shared Loss: 2.306 \n",
      "Average Independence Loss: 1.053 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 12426.230 \n",
      "Average KL Loss: 106.474 \n",
      "Average Regression Loss: 1.673 \n",
      "Average Shared Loss: 2.296 \n",
      "Average Independence Loss: 1.069 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 12391.618 \n",
      "Average KL Loss: 106.367 \n",
      "Average Regression Loss: 1.849 \n",
      "Average Shared Loss: 2.300 \n",
      "Average Independence Loss: 1.108 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 12464.265 \n",
      "Average KL Loss: 106.267 \n",
      "Average Regression Loss: 1.768 \n",
      "Average Shared Loss: 2.288 \n",
      "Average Independence Loss: 1.107 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 12461.928 \n",
      "Average KL Loss: 106.199 \n",
      "Average Regression Loss: 1.779 \n",
      "Average Shared Loss: 2.277 \n",
      "Average Independence Loss: 1.133 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 12376.922 \n",
      "Average KL Loss: 106.099 \n",
      "Average Regression Loss: 1.642 \n",
      "Average Shared Loss: 2.269 \n",
      "Average Independence Loss: 1.125 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 12356.130 \n",
      "Average KL Loss: 105.942 \n",
      "Average Regression Loss: 1.722 \n",
      "Average Shared Loss: 2.259 \n",
      "Average Independence Loss: 1.124 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 12310.484 \n",
      "Average KL Loss: 105.852 \n",
      "Average Regression Loss: 1.798 \n",
      "Average Shared Loss: 2.259 \n",
      "Average Independence Loss: 1.123 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 12277.882 \n",
      "Average KL Loss: 105.787 \n",
      "Average Regression Loss: 1.664 \n",
      "Average Shared Loss: 2.258 \n",
      "Average Independence Loss: 1.129 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 12303.353 \n",
      "Average KL Loss: 105.767 \n",
      "Average Regression Loss: 1.688 \n",
      "Average Shared Loss: 2.248 \n",
      "Average Independence Loss: 1.185 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 12302.239 \n",
      "Average KL Loss: 105.651 \n",
      "Average Regression Loss: 1.790 \n",
      "Average Shared Loss: 2.244 \n",
      "Average Independence Loss: 1.153 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 12272.489 \n",
      "Average KL Loss: 105.576 \n",
      "Average Regression Loss: 1.774 \n",
      "Average Shared Loss: 2.235 \n",
      "Average Independence Loss: 1.147 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 12226.306 \n",
      "Average KL Loss: 105.495 \n",
      "Average Regression Loss: 1.676 \n",
      "Average Shared Loss: 2.237 \n",
      "Average Independence Loss: 1.170 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 12200.559 \n",
      "Average KL Loss: 105.295 \n",
      "Average Regression Loss: 1.702 \n",
      "Average Shared Loss: 2.217 \n",
      "Average Independence Loss: 1.143 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 12202.688 \n",
      "Average KL Loss: 105.161 \n",
      "Average Regression Loss: 1.832 \n",
      "Average Shared Loss: 2.211 \n",
      "Average Independence Loss: 1.152 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 12183.754 \n",
      "Average KL Loss: 105.080 \n",
      "Average Regression Loss: 1.724 \n",
      "Average Shared Loss: 2.216 \n",
      "Average Independence Loss: 1.123 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 12219.191 \n",
      "Average KL Loss: 104.991 \n",
      "Average Regression Loss: 1.825 \n",
      "Average Shared Loss: 2.200 \n",
      "Average Independence Loss: 1.138 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 12148.021 \n",
      "Average KL Loss: 104.896 \n",
      "Average Regression Loss: 1.806 \n",
      "Average Shared Loss: 2.201 \n",
      "Average Independence Loss: 1.182 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 12141.010 \n",
      "Average KL Loss: 104.855 \n",
      "Average Regression Loss: 1.727 \n",
      "Average Shared Loss: 2.205 \n",
      "Average Independence Loss: 1.178 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 12143.759 \n",
      "Average KL Loss: 104.721 \n",
      "Average Regression Loss: 1.712 \n",
      "Average Shared Loss: 2.189 \n",
      "Average Independence Loss: 1.181 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 12115.012 \n",
      "Average KL Loss: 104.684 \n",
      "Average Regression Loss: 1.722 \n",
      "Average Shared Loss: 2.194 \n",
      "Average Independence Loss: 1.228 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 12062.908 \n",
      "Average KL Loss: 104.652 \n",
      "Average Regression Loss: 1.661 \n",
      "Average Shared Loss: 2.179 \n",
      "Average Independence Loss: 1.221 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 12119.632 \n",
      "Average KL Loss: 104.465 \n",
      "Average Regression Loss: 1.811 \n",
      "Average Shared Loss: 2.182 \n",
      "Average Independence Loss: 1.250 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 12074.768 \n",
      "Average KL Loss: 104.399 \n",
      "Average Regression Loss: 1.850 \n",
      "Average Shared Loss: 2.179 \n",
      "Average Independence Loss: 1.244 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 12015.785 \n",
      "Average KL Loss: 104.284 \n",
      "Average Regression Loss: 1.826 \n",
      "Average Shared Loss: 2.180 \n",
      "Average Independence Loss: 1.235 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 12035.931 \n",
      "Average KL Loss: 104.186 \n",
      "Average Regression Loss: 1.819 \n",
      "Average Shared Loss: 2.167 \n",
      "Average Independence Loss: 1.210 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 12014.068 \n",
      "Average KL Loss: 104.038 \n",
      "Average Regression Loss: 1.676 \n",
      "Average Shared Loss: 2.158 \n",
      "Average Independence Loss: 1.153 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 11995.634 \n",
      "Average KL Loss: 103.889 \n",
      "Average Regression Loss: 1.804 \n",
      "Average Shared Loss: 2.160 \n",
      "Average Independence Loss: 1.146 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 11943.539 \n",
      "Average KL Loss: 103.784 \n",
      "Average Regression Loss: 1.827 \n",
      "Average Shared Loss: 2.148 \n",
      "Average Independence Loss: 1.127 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 11979.158 \n",
      "Average KL Loss: 103.704 \n",
      "Average Regression Loss: 1.666 \n",
      "Average Shared Loss: 2.152 \n",
      "Average Independence Loss: 0.928 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 12113.748 \n",
      "Average KL Loss: 103.649 \n",
      "Average Regression Loss: 1.490 \n",
      "Average Shared Loss: 2.154 \n",
      "Average Independence Loss: 0.517 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 12080.489 \n",
      "Average KL Loss: 103.611 \n",
      "Average Regression Loss: 1.425 \n",
      "Average Shared Loss: 2.161 \n",
      "Average Independence Loss: 0.185 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 12094.649 \n",
      "Average KL Loss: 103.666 \n",
      "Average Regression Loss: 1.549 \n",
      "Average Shared Loss: 2.152 \n",
      "Average Independence Loss: 0.075 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 12088.686 \n",
      "Average KL Loss: 103.649 \n",
      "Average Regression Loss: 1.330 \n",
      "Average Shared Loss: 2.149 \n",
      "Average Independence Loss: 0.028 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 12153.623 \n",
      "Average KL Loss: 103.676 \n",
      "Average Regression Loss: 1.408 \n",
      "Average Shared Loss: 2.149 \n",
      "Average Independence Loss: 0.017 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 12098.191 \n",
      "Average KL Loss: 103.633 \n",
      "Average Regression Loss: 1.606 \n",
      "Average Shared Loss: 2.147 \n",
      "Average Independence Loss: 0.014 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 12077.904 \n",
      "Average KL Loss: 103.712 \n",
      "Average Regression Loss: 1.398 \n",
      "Average Shared Loss: 2.143 \n",
      "Average Independence Loss: 0.015 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 12094.042 \n",
      "Average KL Loss: 103.678 \n",
      "Average Regression Loss: 1.242 \n",
      "Average Shared Loss: 2.158 \n",
      "Average Independence Loss: 0.017 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 12104.048 \n",
      "Average KL Loss: 103.733 \n",
      "Average Regression Loss: 1.283 \n",
      "Average Shared Loss: 2.144 \n",
      "Average Independence Loss: 0.013 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 12122.298 \n",
      "Average KL Loss: 103.666 \n",
      "Average Regression Loss: 1.238 \n",
      "Average Shared Loss: 2.148 \n",
      "Average Independence Loss: 0.013 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 12043.399 \n",
      "Average KL Loss: 103.650 \n",
      "Average Regression Loss: 1.222 \n",
      "Average Shared Loss: 2.137 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 12105.555 \n",
      "Average KL Loss: 103.661 \n",
      "Average Regression Loss: 1.231 \n",
      "Average Shared Loss: 2.142 \n",
      "Average Independence Loss: 0.014 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 12065.741 \n",
      "Average KL Loss: 103.679 \n",
      "Average Regression Loss: 1.222 \n",
      "Average Shared Loss: 2.147 \n",
      "Average Independence Loss: 0.013 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 12095.193 \n",
      "Average KL Loss: 103.663 \n",
      "Average Regression Loss: 1.139 \n",
      "Average Shared Loss: 2.136 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 12097.098 \n",
      "Average KL Loss: 103.645 \n",
      "Average Regression Loss: 1.327 \n",
      "Average Shared Loss: 2.141 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 12107.893 \n",
      "Average KL Loss: 103.669 \n",
      "Average Regression Loss: 1.167 \n",
      "Average Shared Loss: 2.145 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 12105.612 \n",
      "Average KL Loss: 103.577 \n",
      "Average Regression Loss: 1.178 \n",
      "Average Shared Loss: 2.137 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 12147.262 \n",
      "Average KL Loss: 103.587 \n",
      "Average Regression Loss: 1.149 \n",
      "Average Shared Loss: 2.131 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 12126.802 \n",
      "Average KL Loss: 103.605 \n",
      "Average Regression Loss: 1.154 \n",
      "Average Shared Loss: 2.137 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 12084.113 \n",
      "Average KL Loss: 103.626 \n",
      "Average Regression Loss: 1.070 \n",
      "Average Shared Loss: 2.133 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 12101.075 \n",
      "Average KL Loss: 103.601 \n",
      "Average Regression Loss: 1.079 \n",
      "Average Shared Loss: 2.129 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 12072.090 \n",
      "Average KL Loss: 103.637 \n",
      "Average Regression Loss: 1.068 \n",
      "Average Shared Loss: 2.135 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 12168.413 \n",
      "Average KL Loss: 103.591 \n",
      "Average Regression Loss: 1.193 \n",
      "Average Shared Loss: 2.136 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 12086.226 \n",
      "Average KL Loss: 103.603 \n",
      "Average Regression Loss: 1.153 \n",
      "Average Shared Loss: 2.136 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 12124.861 \n",
      "Average KL Loss: 103.674 \n",
      "Average Regression Loss: 1.145 \n",
      "Average Shared Loss: 2.131 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 12087.204 \n",
      "Average KL Loss: 103.644 \n",
      "Average Regression Loss: 1.107 \n",
      "Average Shared Loss: 2.134 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 12112.992 \n",
      "Average KL Loss: 103.714 \n",
      "Average Regression Loss: 1.038 \n",
      "Average Shared Loss: 2.140 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 12062.818 \n",
      "Average KL Loss: 103.659 \n",
      "Average Regression Loss: 1.090 \n",
      "Average Shared Loss: 2.120 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 12124.377 \n",
      "Average KL Loss: 103.624 \n",
      "Average Regression Loss: 1.207 \n",
      "Average Shared Loss: 2.132 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 12078.775 \n",
      "Average KL Loss: 103.634 \n",
      "Average Regression Loss: 1.125 \n",
      "Average Shared Loss: 2.115 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 12094.218 \n",
      "Average KL Loss: 103.646 \n",
      "Average Regression Loss: 1.092 \n",
      "Average Shared Loss: 2.128 \n",
      "Average Independence Loss: 0.013 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 12097.599 \n",
      "Average KL Loss: 103.661 \n",
      "Average Regression Loss: 1.181 \n",
      "Average Shared Loss: 2.133 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 12158.979 \n",
      "Average KL Loss: 103.721 \n",
      "Average Regression Loss: 1.161 \n",
      "Average Shared Loss: 2.133 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 12133.514 \n",
      "Average KL Loss: 103.749 \n",
      "Average Regression Loss: 1.063 \n",
      "Average Shared Loss: 2.122 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 12033.709 \n",
      "Average KL Loss: 103.660 \n",
      "Average Regression Loss: 1.048 \n",
      "Average Shared Loss: 2.124 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 12125.369 \n",
      "Average KL Loss: 103.675 \n",
      "Average Regression Loss: 1.024 \n",
      "Average Shared Loss: 2.123 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 12113.349 \n",
      "Average KL Loss: 103.713 \n",
      "Average Regression Loss: 1.060 \n",
      "Average Shared Loss: 2.120 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 12144.153 \n",
      "Average KL Loss: 103.641 \n",
      "Average Regression Loss: 1.087 \n",
      "Average Shared Loss: 2.122 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 12164.525 \n",
      "Average KL Loss: 103.639 \n",
      "Average Regression Loss: 1.085 \n",
      "Average Shared Loss: 2.124 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 12094.687 \n",
      "Average KL Loss: 103.665 \n",
      "Average Regression Loss: 1.028 \n",
      "Average Shared Loss: 2.120 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 12074.268 \n",
      "Average KL Loss: 103.645 \n",
      "Average Regression Loss: 1.056 \n",
      "Average Shared Loss: 2.120 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 12109.104 \n",
      "Average KL Loss: 103.705 \n",
      "Average Regression Loss: 1.083 \n",
      "Average Shared Loss: 2.115 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 12038.724 \n",
      "Average KL Loss: 103.590 \n",
      "Average Regression Loss: 0.939 \n",
      "Average Shared Loss: 2.117 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 12127.218 \n",
      "Average KL Loss: 103.643 \n",
      "Average Regression Loss: 1.035 \n",
      "Average Shared Loss: 2.113 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 12088.065 \n",
      "Average KL Loss: 103.571 \n",
      "Average Regression Loss: 1.097 \n",
      "Average Shared Loss: 2.110 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 12092.254 \n",
      "Average KL Loss: 103.582 \n",
      "Average Regression Loss: 1.016 \n",
      "Average Shared Loss: 2.114 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 12037.718 \n",
      "Average KL Loss: 103.650 \n",
      "Average Regression Loss: 0.884 \n",
      "Average Shared Loss: 2.107 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 12097.742 \n",
      "Average KL Loss: 103.618 \n",
      "Average Regression Loss: 0.946 \n",
      "Average Shared Loss: 2.111 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 12093.037 \n",
      "Average KL Loss: 103.675 \n",
      "Average Regression Loss: 0.919 \n",
      "Average Shared Loss: 2.103 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "Average DNA Recon Loss: 25580.748 \n",
      "Average Gene Recon Loss: 1842.883 \n",
      "Average DNA KL Loss: 197.647 \n",
      "Average Gene KL Loss: 2.035 \n",
      "Average Regressor Loss: 1.592 \n",
      "Average RMSE Loss: 1.262 \n",
      "Average Shared MSE Loss: 1.827 \n",
      "Average Shared RMSE Loss: 1.352 \n",
      "Average R2: 0.829 \n",
      "Average Indepence Loss: 0.006 \n",
      "\n",
      "Fold 6\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 12370.420 \n",
      "Average KL Loss: 103.601 \n",
      "Average Regression Loss: 1.271 \n",
      "Average Shared Loss: 2.110 \n",
      "Average Independence Loss: 0.090 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 12391.863 \n",
      "Average KL Loss: 103.684 \n",
      "Average Regression Loss: 1.414 \n",
      "Average Shared Loss: 2.104 \n",
      "Average Independence Loss: 0.240 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 12275.164 \n",
      "Average KL Loss: 103.822 \n",
      "Average Regression Loss: 1.372 \n",
      "Average Shared Loss: 2.121 \n",
      "Average Independence Loss: 0.375 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 12268.782 \n",
      "Average KL Loss: 103.871 \n",
      "Average Regression Loss: 1.505 \n",
      "Average Shared Loss: 2.118 \n",
      "Average Independence Loss: 0.507 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 12146.834 \n",
      "Average KL Loss: 103.901 \n",
      "Average Regression Loss: 1.311 \n",
      "Average Shared Loss: 2.105 \n",
      "Average Independence Loss: 0.593 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 12137.686 \n",
      "Average KL Loss: 103.932 \n",
      "Average Regression Loss: 1.389 \n",
      "Average Shared Loss: 2.104 \n",
      "Average Independence Loss: 0.694 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 12076.339 \n",
      "Average KL Loss: 103.965 \n",
      "Average Regression Loss: 1.359 \n",
      "Average Shared Loss: 2.110 \n",
      "Average Independence Loss: 0.716 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 12021.342 \n",
      "Average KL Loss: 104.004 \n",
      "Average Regression Loss: 1.354 \n",
      "Average Shared Loss: 2.106 \n",
      "Average Independence Loss: 0.768 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 11976.298 \n",
      "Average KL Loss: 103.930 \n",
      "Average Regression Loss: 1.419 \n",
      "Average Shared Loss: 2.094 \n",
      "Average Independence Loss: 0.777 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 11896.062 \n",
      "Average KL Loss: 103.815 \n",
      "Average Regression Loss: 1.453 \n",
      "Average Shared Loss: 2.101 \n",
      "Average Independence Loss: 0.790 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 11945.367 \n",
      "Average KL Loss: 103.762 \n",
      "Average Regression Loss: 1.362 \n",
      "Average Shared Loss: 2.089 \n",
      "Average Independence Loss: 0.805 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 11892.150 \n",
      "Average KL Loss: 103.758 \n",
      "Average Regression Loss: 1.371 \n",
      "Average Shared Loss: 2.078 \n",
      "Average Independence Loss: 0.831 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 11859.051 \n",
      "Average KL Loss: 103.642 \n",
      "Average Regression Loss: 1.290 \n",
      "Average Shared Loss: 2.079 \n",
      "Average Independence Loss: 0.831 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 11823.921 \n",
      "Average KL Loss: 103.571 \n",
      "Average Regression Loss: 1.454 \n",
      "Average Shared Loss: 2.071 \n",
      "Average Independence Loss: 0.865 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 11796.577 \n",
      "Average KL Loss: 103.449 \n",
      "Average Regression Loss: 1.345 \n",
      "Average Shared Loss: 2.064 \n",
      "Average Independence Loss: 0.850 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 11760.076 \n",
      "Average KL Loss: 103.374 \n",
      "Average Regression Loss: 1.348 \n",
      "Average Shared Loss: 2.068 \n",
      "Average Independence Loss: 0.913 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 11738.604 \n",
      "Average KL Loss: 103.228 \n",
      "Average Regression Loss: 1.436 \n",
      "Average Shared Loss: 2.058 \n",
      "Average Independence Loss: 0.979 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 11725.025 \n",
      "Average KL Loss: 103.121 \n",
      "Average Regression Loss: 1.365 \n",
      "Average Shared Loss: 2.057 \n",
      "Average Independence Loss: 1.040 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 11753.943 \n",
      "Average KL Loss: 103.018 \n",
      "Average Regression Loss: 1.416 \n",
      "Average Shared Loss: 2.046 \n",
      "Average Independence Loss: 1.021 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 11768.913 \n",
      "Average KL Loss: 102.944 \n",
      "Average Regression Loss: 1.319 \n",
      "Average Shared Loss: 2.039 \n",
      "Average Independence Loss: 1.071 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 11757.148 \n",
      "Average KL Loss: 102.812 \n",
      "Average Regression Loss: 1.373 \n",
      "Average Shared Loss: 2.039 \n",
      "Average Independence Loss: 1.024 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 11746.532 \n",
      "Average KL Loss: 102.702 \n",
      "Average Regression Loss: 1.294 \n",
      "Average Shared Loss: 2.030 \n",
      "Average Independence Loss: 1.024 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 11672.286 \n",
      "Average KL Loss: 102.615 \n",
      "Average Regression Loss: 1.362 \n",
      "Average Shared Loss: 2.030 \n",
      "Average Independence Loss: 1.040 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 11699.232 \n",
      "Average KL Loss: 102.503 \n",
      "Average Regression Loss: 1.363 \n",
      "Average Shared Loss: 2.024 \n",
      "Average Independence Loss: 0.997 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 11656.193 \n",
      "Average KL Loss: 102.387 \n",
      "Average Regression Loss: 1.385 \n",
      "Average Shared Loss: 2.013 \n",
      "Average Independence Loss: 1.019 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 11673.778 \n",
      "Average KL Loss: 102.339 \n",
      "Average Regression Loss: 1.407 \n",
      "Average Shared Loss: 2.001 \n",
      "Average Independence Loss: 1.029 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 11687.342 \n",
      "Average KL Loss: 102.302 \n",
      "Average Regression Loss: 1.362 \n",
      "Average Shared Loss: 1.992 \n",
      "Average Independence Loss: 1.032 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 11635.333 \n",
      "Average KL Loss: 102.118 \n",
      "Average Regression Loss: 1.436 \n",
      "Average Shared Loss: 1.988 \n",
      "Average Independence Loss: 0.991 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 11620.355 \n",
      "Average KL Loss: 101.944 \n",
      "Average Regression Loss: 1.371 \n",
      "Average Shared Loss: 1.987 \n",
      "Average Independence Loss: 0.985 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 11577.978 \n",
      "Average KL Loss: 101.863 \n",
      "Average Regression Loss: 1.342 \n",
      "Average Shared Loss: 1.987 \n",
      "Average Independence Loss: 1.030 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 11578.223 \n",
      "Average KL Loss: 101.772 \n",
      "Average Regression Loss: 1.377 \n",
      "Average Shared Loss: 1.979 \n",
      "Average Independence Loss: 0.912 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 11574.515 \n",
      "Average KL Loss: 101.631 \n",
      "Average Regression Loss: 1.346 \n",
      "Average Shared Loss: 1.969 \n",
      "Average Independence Loss: 0.915 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 11545.413 \n",
      "Average KL Loss: 101.486 \n",
      "Average Regression Loss: 1.359 \n",
      "Average Shared Loss: 1.965 \n",
      "Average Independence Loss: 0.932 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 11548.954 \n",
      "Average KL Loss: 101.411 \n",
      "Average Regression Loss: 1.300 \n",
      "Average Shared Loss: 1.955 \n",
      "Average Independence Loss: 0.952 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 11572.937 \n",
      "Average KL Loss: 101.295 \n",
      "Average Regression Loss: 1.294 \n",
      "Average Shared Loss: 1.946 \n",
      "Average Independence Loss: 0.960 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 11582.387 \n",
      "Average KL Loss: 101.258 \n",
      "Average Regression Loss: 1.309 \n",
      "Average Shared Loss: 1.951 \n",
      "Average Independence Loss: 0.955 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 11545.181 \n",
      "Average KL Loss: 101.121 \n",
      "Average Regression Loss: 1.386 \n",
      "Average Shared Loss: 1.941 \n",
      "Average Independence Loss: 0.966 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 11617.703 \n",
      "Average KL Loss: 101.089 \n",
      "Average Regression Loss: 1.474 \n",
      "Average Shared Loss: 1.933 \n",
      "Average Independence Loss: 0.978 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 11547.056 \n",
      "Average KL Loss: 100.978 \n",
      "Average Regression Loss: 1.429 \n",
      "Average Shared Loss: 1.934 \n",
      "Average Independence Loss: 0.975 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 11479.397 \n",
      "Average KL Loss: 100.912 \n",
      "Average Regression Loss: 1.383 \n",
      "Average Shared Loss: 1.932 \n",
      "Average Independence Loss: 1.012 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 11491.994 \n",
      "Average KL Loss: 100.806 \n",
      "Average Regression Loss: 1.382 \n",
      "Average Shared Loss: 1.927 \n",
      "Average Independence Loss: 1.011 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 11524.787 \n",
      "Average KL Loss: 100.748 \n",
      "Average Regression Loss: 1.465 \n",
      "Average Shared Loss: 1.923 \n",
      "Average Independence Loss: 0.968 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 11438.707 \n",
      "Average KL Loss: 100.646 \n",
      "Average Regression Loss: 1.364 \n",
      "Average Shared Loss: 1.927 \n",
      "Average Independence Loss: 0.927 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 11452.848 \n",
      "Average KL Loss: 100.510 \n",
      "Average Regression Loss: 1.384 \n",
      "Average Shared Loss: 1.909 \n",
      "Average Independence Loss: 0.981 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 11496.931 \n",
      "Average KL Loss: 100.394 \n",
      "Average Regression Loss: 1.309 \n",
      "Average Shared Loss: 1.905 \n",
      "Average Independence Loss: 1.034 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 11420.820 \n",
      "Average KL Loss: 100.343 \n",
      "Average Regression Loss: 1.362 \n",
      "Average Shared Loss: 1.893 \n",
      "Average Independence Loss: 1.120 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 11430.556 \n",
      "Average KL Loss: 100.241 \n",
      "Average Regression Loss: 1.430 \n",
      "Average Shared Loss: 1.901 \n",
      "Average Independence Loss: 1.212 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 11431.696 \n",
      "Average KL Loss: 100.169 \n",
      "Average Regression Loss: 1.341 \n",
      "Average Shared Loss: 1.899 \n",
      "Average Independence Loss: 1.238 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 11397.867 \n",
      "Average KL Loss: 100.031 \n",
      "Average Regression Loss: 1.274 \n",
      "Average Shared Loss: 1.894 \n",
      "Average Independence Loss: 1.166 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 11467.550 \n",
      "Average KL Loss: 99.963 \n",
      "Average Regression Loss: 1.556 \n",
      "Average Shared Loss: 1.889 \n",
      "Average Independence Loss: 1.169 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 11502.667 \n",
      "Average KL Loss: 99.937 \n",
      "Average Regression Loss: 1.233 \n",
      "Average Shared Loss: 1.880 \n",
      "Average Independence Loss: 0.843 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 11559.079 \n",
      "Average KL Loss: 99.892 \n",
      "Average Regression Loss: 1.238 \n",
      "Average Shared Loss: 1.881 \n",
      "Average Independence Loss: 0.210 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 11619.423 \n",
      "Average KL Loss: 99.870 \n",
      "Average Regression Loss: 1.248 \n",
      "Average Shared Loss: 1.879 \n",
      "Average Independence Loss: 0.070 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 11583.226 \n",
      "Average KL Loss: 99.893 \n",
      "Average Regression Loss: 1.283 \n",
      "Average Shared Loss: 1.883 \n",
      "Average Independence Loss: 0.027 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 11680.553 \n",
      "Average KL Loss: 99.863 \n",
      "Average Regression Loss: 1.343 \n",
      "Average Shared Loss: 1.873 \n",
      "Average Independence Loss: 0.016 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 11610.668 \n",
      "Average KL Loss: 99.925 \n",
      "Average Regression Loss: 1.203 \n",
      "Average Shared Loss: 1.870 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 11647.626 \n",
      "Average KL Loss: 99.871 \n",
      "Average Regression Loss: 1.199 \n",
      "Average Shared Loss: 1.887 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 11639.428 \n",
      "Average KL Loss: 99.891 \n",
      "Average Regression Loss: 1.220 \n",
      "Average Shared Loss: 1.877 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 11620.371 \n",
      "Average KL Loss: 99.913 \n",
      "Average Regression Loss: 1.120 \n",
      "Average Shared Loss: 1.878 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 11603.971 \n",
      "Average KL Loss: 99.914 \n",
      "Average Regression Loss: 1.105 \n",
      "Average Shared Loss: 1.869 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 11662.992 \n",
      "Average KL Loss: 99.877 \n",
      "Average Regression Loss: 1.294 \n",
      "Average Shared Loss: 1.876 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 11599.342 \n",
      "Average KL Loss: 99.879 \n",
      "Average Regression Loss: 1.074 \n",
      "Average Shared Loss: 1.876 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 11617.976 \n",
      "Average KL Loss: 99.883 \n",
      "Average Regression Loss: 1.178 \n",
      "Average Shared Loss: 1.874 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 11614.104 \n",
      "Average KL Loss: 99.862 \n",
      "Average Regression Loss: 1.025 \n",
      "Average Shared Loss: 1.867 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 11609.210 \n",
      "Average KL Loss: 99.896 \n",
      "Average Regression Loss: 1.075 \n",
      "Average Shared Loss: 1.871 \n",
      "Average Independence Loss: 0.014 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 11660.888 \n",
      "Average KL Loss: 99.889 \n",
      "Average Regression Loss: 1.026 \n",
      "Average Shared Loss: 1.872 \n",
      "Average Independence Loss: 0.014 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 11593.193 \n",
      "Average KL Loss: 99.861 \n",
      "Average Regression Loss: 1.074 \n",
      "Average Shared Loss: 1.858 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 11612.999 \n",
      "Average KL Loss: 99.860 \n",
      "Average Regression Loss: 1.156 \n",
      "Average Shared Loss: 1.860 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 11569.102 \n",
      "Average KL Loss: 99.881 \n",
      "Average Regression Loss: 1.136 \n",
      "Average Shared Loss: 1.864 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 11649.863 \n",
      "Average KL Loss: 99.896 \n",
      "Average Regression Loss: 1.176 \n",
      "Average Shared Loss: 1.870 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 11626.813 \n",
      "Average KL Loss: 99.874 \n",
      "Average Regression Loss: 1.069 \n",
      "Average Shared Loss: 1.865 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 11641.337 \n",
      "Average KL Loss: 99.898 \n",
      "Average Regression Loss: 1.152 \n",
      "Average Shared Loss: 1.860 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 11654.175 \n",
      "Average KL Loss: 99.852 \n",
      "Average Regression Loss: 1.059 \n",
      "Average Shared Loss: 1.863 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 11724.438 \n",
      "Average KL Loss: 99.891 \n",
      "Average Regression Loss: 1.092 \n",
      "Average Shared Loss: 1.859 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 11650.529 \n",
      "Average KL Loss: 99.910 \n",
      "Average Regression Loss: 1.062 \n",
      "Average Shared Loss: 1.856 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 11626.000 \n",
      "Average KL Loss: 99.858 \n",
      "Average Regression Loss: 0.995 \n",
      "Average Shared Loss: 1.868 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 11603.879 \n",
      "Average KL Loss: 99.849 \n",
      "Average Regression Loss: 1.048 \n",
      "Average Shared Loss: 1.858 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 11584.462 \n",
      "Average KL Loss: 99.881 \n",
      "Average Regression Loss: 0.948 \n",
      "Average Shared Loss: 1.861 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 11601.229 \n",
      "Average KL Loss: 99.904 \n",
      "Average Regression Loss: 1.155 \n",
      "Average Shared Loss: 1.860 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 11638.161 \n",
      "Average KL Loss: 99.887 \n",
      "Average Regression Loss: 1.008 \n",
      "Average Shared Loss: 1.861 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 11631.274 \n",
      "Average KL Loss: 99.893 \n",
      "Average Regression Loss: 1.014 \n",
      "Average Shared Loss: 1.856 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 11662.269 \n",
      "Average KL Loss: 99.891 \n",
      "Average Regression Loss: 0.964 \n",
      "Average Shared Loss: 1.859 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 11555.046 \n",
      "Average KL Loss: 99.905 \n",
      "Average Regression Loss: 1.020 \n",
      "Average Shared Loss: 1.852 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 11572.000 \n",
      "Average KL Loss: 99.861 \n",
      "Average Regression Loss: 0.948 \n",
      "Average Shared Loss: 1.849 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 11598.807 \n",
      "Average KL Loss: 99.878 \n",
      "Average Regression Loss: 0.905 \n",
      "Average Shared Loss: 1.848 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 11641.050 \n",
      "Average KL Loss: 99.875 \n",
      "Average Regression Loss: 1.059 \n",
      "Average Shared Loss: 1.855 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 11581.152 \n",
      "Average KL Loss: 99.847 \n",
      "Average Regression Loss: 0.926 \n",
      "Average Shared Loss: 1.854 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 11622.517 \n",
      "Average KL Loss: 99.882 \n",
      "Average Regression Loss: 0.957 \n",
      "Average Shared Loss: 1.847 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 11681.274 \n",
      "Average KL Loss: 99.859 \n",
      "Average Regression Loss: 0.990 \n",
      "Average Shared Loss: 1.836 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 11632.098 \n",
      "Average KL Loss: 99.842 \n",
      "Average Regression Loss: 1.031 \n",
      "Average Shared Loss: 1.842 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 11662.131 \n",
      "Average KL Loss: 99.850 \n",
      "Average Regression Loss: 1.044 \n",
      "Average Shared Loss: 1.841 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 11567.206 \n",
      "Average KL Loss: 99.847 \n",
      "Average Regression Loss: 1.038 \n",
      "Average Shared Loss: 1.846 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 11633.271 \n",
      "Average KL Loss: 99.857 \n",
      "Average Regression Loss: 0.908 \n",
      "Average Shared Loss: 1.836 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 11612.563 \n",
      "Average KL Loss: 99.866 \n",
      "Average Regression Loss: 1.068 \n",
      "Average Shared Loss: 1.836 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 11598.698 \n",
      "Average KL Loss: 99.874 \n",
      "Average Regression Loss: 0.908 \n",
      "Average Shared Loss: 1.846 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 11670.663 \n",
      "Average KL Loss: 99.868 \n",
      "Average Regression Loss: 1.091 \n",
      "Average Shared Loss: 1.834 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 11602.182 \n",
      "Average KL Loss: 99.856 \n",
      "Average Regression Loss: 0.942 \n",
      "Average Shared Loss: 1.837 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 11634.417 \n",
      "Average KL Loss: 99.860 \n",
      "Average Regression Loss: 0.981 \n",
      "Average Shared Loss: 1.836 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 11580.367 \n",
      "Average KL Loss: 99.829 \n",
      "Average Regression Loss: 0.932 \n",
      "Average Shared Loss: 1.840 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 11626.138 \n",
      "Average KL Loss: 99.878 \n",
      "Average Regression Loss: 0.848 \n",
      "Average Shared Loss: 1.828 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "Average DNA Recon Loss: 24846.466 \n",
      "Average Gene Recon Loss: 1062.470 \n",
      "Average DNA KL Loss: 191.701 \n",
      "Average Gene KL Loss: 1.060 \n",
      "Average Regressor Loss: 1.577 \n",
      "Average RMSE Loss: 1.256 \n",
      "Average Shared MSE Loss: 1.856 \n",
      "Average Shared RMSE Loss: 1.362 \n",
      "Average R2: 0.839 \n",
      "Average Indepence Loss: 0.010 \n",
      "\n",
      "Fold 7\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 11609.142 \n",
      "Average KL Loss: 99.811 \n",
      "Average Regression Loss: 1.161 \n",
      "Average Shared Loss: 1.852 \n",
      "Average Independence Loss: 0.124 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 11579.880 \n",
      "Average KL Loss: 99.826 \n",
      "Average Regression Loss: 1.232 \n",
      "Average Shared Loss: 1.849 \n",
      "Average Independence Loss: 0.412 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 11537.059 \n",
      "Average KL Loss: 99.932 \n",
      "Average Regression Loss: 1.217 \n",
      "Average Shared Loss: 1.857 \n",
      "Average Independence Loss: 0.731 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 11474.041 \n",
      "Average KL Loss: 99.961 \n",
      "Average Regression Loss: 1.207 \n",
      "Average Shared Loss: 1.840 \n",
      "Average Independence Loss: 0.942 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 11540.986 \n",
      "Average KL Loss: 100.051 \n",
      "Average Regression Loss: 1.309 \n",
      "Average Shared Loss: 1.859 \n",
      "Average Independence Loss: 1.199 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 11482.823 \n",
      "Average KL Loss: 100.083 \n",
      "Average Regression Loss: 1.285 \n",
      "Average Shared Loss: 1.849 \n",
      "Average Independence Loss: 1.284 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 11363.858 \n",
      "Average KL Loss: 100.039 \n",
      "Average Regression Loss: 1.292 \n",
      "Average Shared Loss: 1.861 \n",
      "Average Independence Loss: 1.359 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 11390.621 \n",
      "Average KL Loss: 99.979 \n",
      "Average Regression Loss: 1.326 \n",
      "Average Shared Loss: 1.839 \n",
      "Average Independence Loss: 1.418 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 11336.086 \n",
      "Average KL Loss: 99.930 \n",
      "Average Regression Loss: 1.253 \n",
      "Average Shared Loss: 1.837 \n",
      "Average Independence Loss: 1.492 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 11301.918 \n",
      "Average KL Loss: 99.877 \n",
      "Average Regression Loss: 1.231 \n",
      "Average Shared Loss: 1.830 \n",
      "Average Independence Loss: 1.568 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 11317.849 \n",
      "Average KL Loss: 99.785 \n",
      "Average Regression Loss: 1.428 \n",
      "Average Shared Loss: 1.824 \n",
      "Average Independence Loss: 1.555 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 11372.135 \n",
      "Average KL Loss: 99.736 \n",
      "Average Regression Loss: 1.330 \n",
      "Average Shared Loss: 1.823 \n",
      "Average Independence Loss: 1.604 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 11247.249 \n",
      "Average KL Loss: 99.656 \n",
      "Average Regression Loss: 1.280 \n",
      "Average Shared Loss: 1.821 \n",
      "Average Independence Loss: 1.629 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 11274.355 \n",
      "Average KL Loss: 99.546 \n",
      "Average Regression Loss: 1.277 \n",
      "Average Shared Loss: 1.806 \n",
      "Average Independence Loss: 1.613 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 11248.206 \n",
      "Average KL Loss: 99.475 \n",
      "Average Regression Loss: 1.202 \n",
      "Average Shared Loss: 1.808 \n",
      "Average Independence Loss: 1.660 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 11271.346 \n",
      "Average KL Loss: 99.391 \n",
      "Average Regression Loss: 1.272 \n",
      "Average Shared Loss: 1.808 \n",
      "Average Independence Loss: 1.644 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 11232.413 \n",
      "Average KL Loss: 99.303 \n",
      "Average Regression Loss: 1.370 \n",
      "Average Shared Loss: 1.800 \n",
      "Average Independence Loss: 1.638 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 11259.872 \n",
      "Average KL Loss: 99.172 \n",
      "Average Regression Loss: 1.264 \n",
      "Average Shared Loss: 1.788 \n",
      "Average Independence Loss: 1.554 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 11222.571 \n",
      "Average KL Loss: 99.105 \n",
      "Average Regression Loss: 1.296 \n",
      "Average Shared Loss: 1.790 \n",
      "Average Independence Loss: 1.494 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 11152.901 \n",
      "Average KL Loss: 99.012 \n",
      "Average Regression Loss: 1.231 \n",
      "Average Shared Loss: 1.786 \n",
      "Average Independence Loss: 1.542 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 11142.818 \n",
      "Average KL Loss: 98.881 \n",
      "Average Regression Loss: 1.192 \n",
      "Average Shared Loss: 1.778 \n",
      "Average Independence Loss: 1.532 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 11221.036 \n",
      "Average KL Loss: 98.769 \n",
      "Average Regression Loss: 1.373 \n",
      "Average Shared Loss: 1.774 \n",
      "Average Independence Loss: 1.613 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 11191.951 \n",
      "Average KL Loss: 98.687 \n",
      "Average Regression Loss: 1.266 \n",
      "Average Shared Loss: 1.765 \n",
      "Average Independence Loss: 1.687 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 11214.827 \n",
      "Average KL Loss: 98.657 \n",
      "Average Regression Loss: 1.318 \n",
      "Average Shared Loss: 1.772 \n",
      "Average Independence Loss: 1.774 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 11170.154 \n",
      "Average KL Loss: 98.570 \n",
      "Average Regression Loss: 1.264 \n",
      "Average Shared Loss: 1.762 \n",
      "Average Independence Loss: 1.800 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 11154.951 \n",
      "Average KL Loss: 98.508 \n",
      "Average Regression Loss: 1.249 \n",
      "Average Shared Loss: 1.761 \n",
      "Average Independence Loss: 1.803 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 11122.113 \n",
      "Average KL Loss: 98.405 \n",
      "Average Regression Loss: 1.247 \n",
      "Average Shared Loss: 1.753 \n",
      "Average Independence Loss: 1.869 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 11108.433 \n",
      "Average KL Loss: 98.326 \n",
      "Average Regression Loss: 1.330 \n",
      "Average Shared Loss: 1.751 \n",
      "Average Independence Loss: 1.927 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 11151.638 \n",
      "Average KL Loss: 98.284 \n",
      "Average Regression Loss: 1.260 \n",
      "Average Shared Loss: 1.747 \n",
      "Average Independence Loss: 1.925 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 11207.444 \n",
      "Average KL Loss: 98.195 \n",
      "Average Regression Loss: 1.355 \n",
      "Average Shared Loss: 1.730 \n",
      "Average Independence Loss: 1.911 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 11130.336 \n",
      "Average KL Loss: 98.113 \n",
      "Average Regression Loss: 1.255 \n",
      "Average Shared Loss: 1.735 \n",
      "Average Independence Loss: 1.963 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 11178.538 \n",
      "Average KL Loss: 98.053 \n",
      "Average Regression Loss: 1.370 \n",
      "Average Shared Loss: 1.732 \n",
      "Average Independence Loss: 2.092 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 11093.753 \n",
      "Average KL Loss: 97.961 \n",
      "Average Regression Loss: 1.243 \n",
      "Average Shared Loss: 1.732 \n",
      "Average Independence Loss: 2.116 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 11093.341 \n",
      "Average KL Loss: 97.834 \n",
      "Average Regression Loss: 1.183 \n",
      "Average Shared Loss: 1.737 \n",
      "Average Independence Loss: 2.057 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 11157.909 \n",
      "Average KL Loss: 97.744 \n",
      "Average Regression Loss: 1.314 \n",
      "Average Shared Loss: 1.724 \n",
      "Average Independence Loss: 2.057 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 11108.221 \n",
      "Average KL Loss: 97.646 \n",
      "Average Regression Loss: 1.270 \n",
      "Average Shared Loss: 1.717 \n",
      "Average Independence Loss: 2.079 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 11082.965 \n",
      "Average KL Loss: 97.513 \n",
      "Average Regression Loss: 1.559 \n",
      "Average Shared Loss: 1.716 \n",
      "Average Independence Loss: 2.109 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 11050.454 \n",
      "Average KL Loss: 97.367 \n",
      "Average Regression Loss: 1.334 \n",
      "Average Shared Loss: 1.718 \n",
      "Average Independence Loss: 2.143 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 11022.781 \n",
      "Average KL Loss: 97.242 \n",
      "Average Regression Loss: 1.300 \n",
      "Average Shared Loss: 1.704 \n",
      "Average Independence Loss: 2.139 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 11052.617 \n",
      "Average KL Loss: 97.130 \n",
      "Average Regression Loss: 1.314 \n",
      "Average Shared Loss: 1.699 \n",
      "Average Independence Loss: 2.053 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 11058.180 \n",
      "Average KL Loss: 97.020 \n",
      "Average Regression Loss: 1.283 \n",
      "Average Shared Loss: 1.701 \n",
      "Average Independence Loss: 2.142 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 11047.813 \n",
      "Average KL Loss: 96.943 \n",
      "Average Regression Loss: 1.269 \n",
      "Average Shared Loss: 1.697 \n",
      "Average Independence Loss: 2.159 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 11080.653 \n",
      "Average KL Loss: 96.892 \n",
      "Average Regression Loss: 1.376 \n",
      "Average Shared Loss: 1.700 \n",
      "Average Independence Loss: 2.106 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 11069.093 \n",
      "Average KL Loss: 96.835 \n",
      "Average Regression Loss: 1.215 \n",
      "Average Shared Loss: 1.683 \n",
      "Average Independence Loss: 2.139 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 11056.141 \n",
      "Average KL Loss: 96.772 \n",
      "Average Regression Loss: 1.301 \n",
      "Average Shared Loss: 1.686 \n",
      "Average Independence Loss: 2.046 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 11028.153 \n",
      "Average KL Loss: 96.661 \n",
      "Average Regression Loss: 1.252 \n",
      "Average Shared Loss: 1.676 \n",
      "Average Independence Loss: 2.029 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 10999.288 \n",
      "Average KL Loss: 96.574 \n",
      "Average Regression Loss: 1.190 \n",
      "Average Shared Loss: 1.685 \n",
      "Average Independence Loss: 2.105 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 11042.460 \n",
      "Average KL Loss: 96.478 \n",
      "Average Regression Loss: 1.273 \n",
      "Average Shared Loss: 1.675 \n",
      "Average Independence Loss: 2.157 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 11047.653 \n",
      "Average KL Loss: 96.395 \n",
      "Average Regression Loss: 1.255 \n",
      "Average Shared Loss: 1.671 \n",
      "Average Independence Loss: 2.246 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 11005.331 \n",
      "Average KL Loss: 96.297 \n",
      "Average Regression Loss: 1.311 \n",
      "Average Shared Loss: 1.668 \n",
      "Average Independence Loss: 2.188 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 11080.260 \n",
      "Average KL Loss: 96.233 \n",
      "Average Regression Loss: 1.148 \n",
      "Average Shared Loss: 1.671 \n",
      "Average Independence Loss: 1.685 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 11093.001 \n",
      "Average KL Loss: 96.206 \n",
      "Average Regression Loss: 1.151 \n",
      "Average Shared Loss: 1.676 \n",
      "Average Independence Loss: 0.483 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 11210.727 \n",
      "Average KL Loss: 96.220 \n",
      "Average Regression Loss: 1.126 \n",
      "Average Shared Loss: 1.670 \n",
      "Average Independence Loss: 0.148 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 11149.067 \n",
      "Average KL Loss: 96.216 \n",
      "Average Regression Loss: 1.026 \n",
      "Average Shared Loss: 1.667 \n",
      "Average Independence Loss: 0.048 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 11170.013 \n",
      "Average KL Loss: 96.205 \n",
      "Average Regression Loss: 1.062 \n",
      "Average Shared Loss: 1.668 \n",
      "Average Independence Loss: 0.021 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 11152.547 \n",
      "Average KL Loss: 96.219 \n",
      "Average Regression Loss: 1.174 \n",
      "Average Shared Loss: 1.670 \n",
      "Average Independence Loss: 0.013 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 11129.425 \n",
      "Average KL Loss: 96.230 \n",
      "Average Regression Loss: 1.205 \n",
      "Average Shared Loss: 1.669 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 11200.742 \n",
      "Average KL Loss: 96.216 \n",
      "Average Regression Loss: 1.233 \n",
      "Average Shared Loss: 1.673 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 11139.061 \n",
      "Average KL Loss: 96.223 \n",
      "Average Regression Loss: 1.129 \n",
      "Average Shared Loss: 1.673 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 11150.122 \n",
      "Average KL Loss: 96.212 \n",
      "Average Regression Loss: 1.053 \n",
      "Average Shared Loss: 1.673 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 11171.317 \n",
      "Average KL Loss: 96.216 \n",
      "Average Regression Loss: 1.085 \n",
      "Average Shared Loss: 1.667 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 11177.257 \n",
      "Average KL Loss: 96.222 \n",
      "Average Regression Loss: 0.969 \n",
      "Average Shared Loss: 1.660 \n",
      "Average Independence Loss: 0.015 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 11185.753 \n",
      "Average KL Loss: 96.205 \n",
      "Average Regression Loss: 1.083 \n",
      "Average Shared Loss: 1.668 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 11167.335 \n",
      "Average KL Loss: 96.210 \n",
      "Average Regression Loss: 1.068 \n",
      "Average Shared Loss: 1.669 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 11112.827 \n",
      "Average KL Loss: 96.212 \n",
      "Average Regression Loss: 0.929 \n",
      "Average Shared Loss: 1.664 \n",
      "Average Independence Loss: 0.013 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 11172.151 \n",
      "Average KL Loss: 96.195 \n",
      "Average Regression Loss: 0.920 \n",
      "Average Shared Loss: 1.657 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 11191.402 \n",
      "Average KL Loss: 96.196 \n",
      "Average Regression Loss: 1.069 \n",
      "Average Shared Loss: 1.653 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 11187.319 \n",
      "Average KL Loss: 96.205 \n",
      "Average Regression Loss: 1.026 \n",
      "Average Shared Loss: 1.655 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 11225.740 \n",
      "Average KL Loss: 96.196 \n",
      "Average Regression Loss: 1.018 \n",
      "Average Shared Loss: 1.654 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 11136.378 \n",
      "Average KL Loss: 96.202 \n",
      "Average Regression Loss: 0.969 \n",
      "Average Shared Loss: 1.657 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 11164.679 \n",
      "Average KL Loss: 96.218 \n",
      "Average Regression Loss: 0.919 \n",
      "Average Shared Loss: 1.659 \n",
      "Average Independence Loss: 0.013 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 11130.836 \n",
      "Average KL Loss: 96.205 \n",
      "Average Regression Loss: 1.110 \n",
      "Average Shared Loss: 1.659 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 11143.478 \n",
      "Average KL Loss: 96.209 \n",
      "Average Regression Loss: 1.107 \n",
      "Average Shared Loss: 1.651 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 11230.012 \n",
      "Average KL Loss: 96.196 \n",
      "Average Regression Loss: 1.075 \n",
      "Average Shared Loss: 1.664 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 11149.560 \n",
      "Average KL Loss: 96.190 \n",
      "Average Regression Loss: 1.030 \n",
      "Average Shared Loss: 1.654 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 11190.843 \n",
      "Average KL Loss: 96.200 \n",
      "Average Regression Loss: 0.939 \n",
      "Average Shared Loss: 1.655 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 11257.573 \n",
      "Average KL Loss: 96.204 \n",
      "Average Regression Loss: 1.117 \n",
      "Average Shared Loss: 1.645 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 11193.676 \n",
      "Average KL Loss: 96.187 \n",
      "Average Regression Loss: 0.920 \n",
      "Average Shared Loss: 1.644 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 11192.015 \n",
      "Average KL Loss: 96.179 \n",
      "Average Regression Loss: 0.889 \n",
      "Average Shared Loss: 1.643 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 11114.416 \n",
      "Average KL Loss: 96.195 \n",
      "Average Regression Loss: 0.925 \n",
      "Average Shared Loss: 1.646 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 11174.174 \n",
      "Average KL Loss: 96.183 \n",
      "Average Regression Loss: 0.930 \n",
      "Average Shared Loss: 1.633 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 11124.016 \n",
      "Average KL Loss: 96.183 \n",
      "Average Regression Loss: 0.919 \n",
      "Average Shared Loss: 1.641 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 11106.899 \n",
      "Average KL Loss: 96.190 \n",
      "Average Regression Loss: 0.777 \n",
      "Average Shared Loss: 1.642 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 11179.154 \n",
      "Average KL Loss: 96.186 \n",
      "Average Regression Loss: 0.898 \n",
      "Average Shared Loss: 1.653 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 11144.903 \n",
      "Average KL Loss: 96.188 \n",
      "Average Regression Loss: 0.877 \n",
      "Average Shared Loss: 1.635 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 11169.679 \n",
      "Average KL Loss: 96.180 \n",
      "Average Regression Loss: 0.859 \n",
      "Average Shared Loss: 1.647 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 11217.438 \n",
      "Average KL Loss: 96.194 \n",
      "Average Regression Loss: 0.954 \n",
      "Average Shared Loss: 1.639 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 11167.082 \n",
      "Average KL Loss: 96.183 \n",
      "Average Regression Loss: 1.001 \n",
      "Average Shared Loss: 1.634 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 11150.061 \n",
      "Average KL Loss: 96.194 \n",
      "Average Regression Loss: 0.970 \n",
      "Average Shared Loss: 1.643 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 11149.811 \n",
      "Average KL Loss: 96.181 \n",
      "Average Regression Loss: 0.871 \n",
      "Average Shared Loss: 1.634 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 11135.507 \n",
      "Average KL Loss: 96.176 \n",
      "Average Regression Loss: 0.993 \n",
      "Average Shared Loss: 1.643 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 11119.395 \n",
      "Average KL Loss: 96.191 \n",
      "Average Regression Loss: 0.888 \n",
      "Average Shared Loss: 1.644 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 11196.578 \n",
      "Average KL Loss: 96.173 \n",
      "Average Regression Loss: 0.908 \n",
      "Average Shared Loss: 1.634 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 11140.839 \n",
      "Average KL Loss: 96.182 \n",
      "Average Regression Loss: 0.872 \n",
      "Average Shared Loss: 1.634 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 11155.448 \n",
      "Average KL Loss: 96.175 \n",
      "Average Regression Loss: 0.864 \n",
      "Average Shared Loss: 1.636 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 11208.570 \n",
      "Average KL Loss: 96.185 \n",
      "Average Regression Loss: 0.896 \n",
      "Average Shared Loss: 1.627 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 11183.638 \n",
      "Average KL Loss: 96.176 \n",
      "Average Regression Loss: 1.047 \n",
      "Average Shared Loss: 1.628 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 11146.121 \n",
      "Average KL Loss: 96.183 \n",
      "Average Regression Loss: 0.925 \n",
      "Average Shared Loss: 1.630 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 11110.138 \n",
      "Average KL Loss: 96.182 \n",
      "Average Regression Loss: 1.032 \n",
      "Average Shared Loss: 1.619 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 11139.991 \n",
      "Average KL Loss: 96.168 \n",
      "Average Regression Loss: 0.795 \n",
      "Average Shared Loss: 1.629 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "Average DNA Recon Loss: 25703.352 \n",
      "Average Gene Recon Loss: 635.530 \n",
      "Average DNA KL Loss: 180.930 \n",
      "Average Gene KL Loss: 0.625 \n",
      "Average Regressor Loss: 1.534 \n",
      "Average RMSE Loss: 1.238 \n",
      "Average Shared MSE Loss: 1.528 \n",
      "Average Shared RMSE Loss: 1.236 \n",
      "Average R2: 0.837 \n",
      "Average Indepence Loss: 0.007 \n",
      "\n",
      "Fold 8\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 11285.904 \n",
      "Average KL Loss: 96.201 \n",
      "Average Regression Loss: 1.181 \n",
      "Average Shared Loss: 1.613 \n",
      "Average Independence Loss: 0.203 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 11259.364 \n",
      "Average KL Loss: 96.304 \n",
      "Average Regression Loss: 1.313 \n",
      "Average Shared Loss: 1.620 \n",
      "Average Independence Loss: 0.819 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 11196.993 \n",
      "Average KL Loss: 96.380 \n",
      "Average Regression Loss: 1.345 \n",
      "Average Shared Loss: 1.631 \n",
      "Average Independence Loss: 1.325 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 11186.939 \n",
      "Average KL Loss: 96.476 \n",
      "Average Regression Loss: 1.245 \n",
      "Average Shared Loss: 1.629 \n",
      "Average Independence Loss: 1.682 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 11138.936 \n",
      "Average KL Loss: 96.530 \n",
      "Average Regression Loss: 1.201 \n",
      "Average Shared Loss: 1.627 \n",
      "Average Independence Loss: 1.855 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 11125.801 \n",
      "Average KL Loss: 96.553 \n",
      "Average Regression Loss: 1.122 \n",
      "Average Shared Loss: 1.627 \n",
      "Average Independence Loss: 1.918 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 11125.613 \n",
      "Average KL Loss: 96.534 \n",
      "Average Regression Loss: 1.160 \n",
      "Average Shared Loss: 1.625 \n",
      "Average Independence Loss: 1.994 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 11108.155 \n",
      "Average KL Loss: 96.519 \n",
      "Average Regression Loss: 1.331 \n",
      "Average Shared Loss: 1.614 \n",
      "Average Independence Loss: 2.169 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 11087.866 \n",
      "Average KL Loss: 96.524 \n",
      "Average Regression Loss: 1.357 \n",
      "Average Shared Loss: 1.610 \n",
      "Average Independence Loss: 2.178 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 11051.244 \n",
      "Average KL Loss: 96.503 \n",
      "Average Regression Loss: 1.218 \n",
      "Average Shared Loss: 1.604 \n",
      "Average Independence Loss: 2.265 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 11030.413 \n",
      "Average KL Loss: 96.469 \n",
      "Average Regression Loss: 1.331 \n",
      "Average Shared Loss: 1.607 \n",
      "Average Independence Loss: 2.277 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 11038.677 \n",
      "Average KL Loss: 96.431 \n",
      "Average Regression Loss: 1.248 \n",
      "Average Shared Loss: 1.610 \n",
      "Average Independence Loss: 2.190 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 10989.489 \n",
      "Average KL Loss: 96.408 \n",
      "Average Regression Loss: 1.181 \n",
      "Average Shared Loss: 1.609 \n",
      "Average Independence Loss: 2.239 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 11028.462 \n",
      "Average KL Loss: 96.343 \n",
      "Average Regression Loss: 1.322 \n",
      "Average Shared Loss: 1.601 \n",
      "Average Independence Loss: 2.286 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 11052.446 \n",
      "Average KL Loss: 96.270 \n",
      "Average Regression Loss: 1.299 \n",
      "Average Shared Loss: 1.605 \n",
      "Average Independence Loss: 2.211 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 10945.107 \n",
      "Average KL Loss: 96.236 \n",
      "Average Regression Loss: 1.251 \n",
      "Average Shared Loss: 1.600 \n",
      "Average Independence Loss: 2.367 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 11089.373 \n",
      "Average KL Loss: 96.165 \n",
      "Average Regression Loss: 1.353 \n",
      "Average Shared Loss: 1.590 \n",
      "Average Independence Loss: 2.358 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 10971.083 \n",
      "Average KL Loss: 96.109 \n",
      "Average Regression Loss: 1.244 \n",
      "Average Shared Loss: 1.585 \n",
      "Average Independence Loss: 2.463 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 10927.720 \n",
      "Average KL Loss: 96.037 \n",
      "Average Regression Loss: 1.146 \n",
      "Average Shared Loss: 1.578 \n",
      "Average Independence Loss: 2.408 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 11011.143 \n",
      "Average KL Loss: 95.946 \n",
      "Average Regression Loss: 1.309 \n",
      "Average Shared Loss: 1.591 \n",
      "Average Independence Loss: 2.517 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 10992.375 \n",
      "Average KL Loss: 95.892 \n",
      "Average Regression Loss: 1.350 \n",
      "Average Shared Loss: 1.575 \n",
      "Average Independence Loss: 2.630 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 10941.232 \n",
      "Average KL Loss: 95.806 \n",
      "Average Regression Loss: 1.148 \n",
      "Average Shared Loss: 1.586 \n",
      "Average Independence Loss: 2.685 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 10916.668 \n",
      "Average KL Loss: 95.716 \n",
      "Average Regression Loss: 1.205 \n",
      "Average Shared Loss: 1.569 \n",
      "Average Independence Loss: 2.604 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 10911.458 \n",
      "Average KL Loss: 95.616 \n",
      "Average Regression Loss: 1.271 \n",
      "Average Shared Loss: 1.570 \n",
      "Average Independence Loss: 2.528 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 10939.177 \n",
      "Average KL Loss: 95.504 \n",
      "Average Regression Loss: 1.200 \n",
      "Average Shared Loss: 1.567 \n",
      "Average Independence Loss: 2.417 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 10907.683 \n",
      "Average KL Loss: 95.435 \n",
      "Average Regression Loss: 1.207 \n",
      "Average Shared Loss: 1.562 \n",
      "Average Independence Loss: 2.502 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 10922.585 \n",
      "Average KL Loss: 95.360 \n",
      "Average Regression Loss: 1.257 \n",
      "Average Shared Loss: 1.562 \n",
      "Average Independence Loss: 2.600 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 10905.082 \n",
      "Average KL Loss: 95.256 \n",
      "Average Regression Loss: 1.278 \n",
      "Average Shared Loss: 1.551 \n",
      "Average Independence Loss: 2.503 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 10916.720 \n",
      "Average KL Loss: 95.147 \n",
      "Average Regression Loss: 1.245 \n",
      "Average Shared Loss: 1.555 \n",
      "Average Independence Loss: 2.592 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 10930.750 \n",
      "Average KL Loss: 95.044 \n",
      "Average Regression Loss: 1.248 \n",
      "Average Shared Loss: 1.550 \n",
      "Average Independence Loss: 2.564 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 10892.868 \n",
      "Average KL Loss: 94.981 \n",
      "Average Regression Loss: 1.114 \n",
      "Average Shared Loss: 1.534 \n",
      "Average Independence Loss: 2.484 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 10916.371 \n",
      "Average KL Loss: 94.942 \n",
      "Average Regression Loss: 1.279 \n",
      "Average Shared Loss: 1.537 \n",
      "Average Independence Loss: 2.584 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 10882.563 \n",
      "Average KL Loss: 94.874 \n",
      "Average Regression Loss: 1.156 \n",
      "Average Shared Loss: 1.533 \n",
      "Average Independence Loss: 2.626 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 10895.744 \n",
      "Average KL Loss: 94.775 \n",
      "Average Regression Loss: 1.318 \n",
      "Average Shared Loss: 1.535 \n",
      "Average Independence Loss: 2.626 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 10881.883 \n",
      "Average KL Loss: 94.683 \n",
      "Average Regression Loss: 1.170 \n",
      "Average Shared Loss: 1.539 \n",
      "Average Independence Loss: 2.714 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 10904.554 \n",
      "Average KL Loss: 94.588 \n",
      "Average Regression Loss: 1.175 \n",
      "Average Shared Loss: 1.535 \n",
      "Average Independence Loss: 2.603 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 10882.184 \n",
      "Average KL Loss: 94.511 \n",
      "Average Regression Loss: 1.250 \n",
      "Average Shared Loss: 1.516 \n",
      "Average Independence Loss: 2.458 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 10886.520 \n",
      "Average KL Loss: 94.447 \n",
      "Average Regression Loss: 1.165 \n",
      "Average Shared Loss: 1.524 \n",
      "Average Independence Loss: 2.373 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 10872.059 \n",
      "Average KL Loss: 94.368 \n",
      "Average Regression Loss: 1.256 \n",
      "Average Shared Loss: 1.519 \n",
      "Average Independence Loss: 2.561 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 10842.001 \n",
      "Average KL Loss: 94.292 \n",
      "Average Regression Loss: 1.198 \n",
      "Average Shared Loss: 1.516 \n",
      "Average Independence Loss: 2.712 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 10863.621 \n",
      "Average KL Loss: 94.200 \n",
      "Average Regression Loss: 1.264 \n",
      "Average Shared Loss: 1.517 \n",
      "Average Independence Loss: 2.640 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 10876.167 \n",
      "Average KL Loss: 94.111 \n",
      "Average Regression Loss: 1.171 \n",
      "Average Shared Loss: 1.513 \n",
      "Average Independence Loss: 2.650 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 10886.197 \n",
      "Average KL Loss: 94.049 \n",
      "Average Regression Loss: 1.318 \n",
      "Average Shared Loss: 1.505 \n",
      "Average Independence Loss: 2.712 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 10860.936 \n",
      "Average KL Loss: 94.016 \n",
      "Average Regression Loss: 1.212 \n",
      "Average Shared Loss: 1.515 \n",
      "Average Independence Loss: 2.744 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 10864.038 \n",
      "Average KL Loss: 93.950 \n",
      "Average Regression Loss: 1.204 \n",
      "Average Shared Loss: 1.496 \n",
      "Average Independence Loss: 2.708 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 10881.313 \n",
      "Average KL Loss: 93.871 \n",
      "Average Regression Loss: 1.216 \n",
      "Average Shared Loss: 1.507 \n",
      "Average Independence Loss: 2.661 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 10840.566 \n",
      "Average KL Loss: 93.793 \n",
      "Average Regression Loss: 1.190 \n",
      "Average Shared Loss: 1.496 \n",
      "Average Independence Loss: 2.729 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 10831.441 \n",
      "Average KL Loss: 93.715 \n",
      "Average Regression Loss: 1.175 \n",
      "Average Shared Loss: 1.498 \n",
      "Average Independence Loss: 2.681 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 10885.692 \n",
      "Average KL Loss: 93.634 \n",
      "Average Regression Loss: 1.220 \n",
      "Average Shared Loss: 1.495 \n",
      "Average Independence Loss: 2.647 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 10918.252 \n",
      "Average KL Loss: 93.574 \n",
      "Average Regression Loss: 1.198 \n",
      "Average Shared Loss: 1.485 \n",
      "Average Independence Loss: 2.606 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 10823.507 \n",
      "Average KL Loss: 93.550 \n",
      "Average Regression Loss: 1.184 \n",
      "Average Shared Loss: 1.492 \n",
      "Average Independence Loss: 2.114 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 10936.986 \n",
      "Average KL Loss: 93.539 \n",
      "Average Regression Loss: 1.263 \n",
      "Average Shared Loss: 1.483 \n",
      "Average Independence Loss: 0.756 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 10937.960 \n",
      "Average KL Loss: 93.536 \n",
      "Average Regression Loss: 1.126 \n",
      "Average Shared Loss: 1.483 \n",
      "Average Independence Loss: 0.183 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 10933.303 \n",
      "Average KL Loss: 93.533 \n",
      "Average Regression Loss: 1.098 \n",
      "Average Shared Loss: 1.489 \n",
      "Average Independence Loss: 0.069 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 10924.674 \n",
      "Average KL Loss: 93.536 \n",
      "Average Regression Loss: 0.997 \n",
      "Average Shared Loss: 1.490 \n",
      "Average Independence Loss: 0.025 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 10985.353 \n",
      "Average KL Loss: 93.535 \n",
      "Average Regression Loss: 1.002 \n",
      "Average Shared Loss: 1.490 \n",
      "Average Independence Loss: 0.014 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 11029.935 \n",
      "Average KL Loss: 93.534 \n",
      "Average Regression Loss: 1.039 \n",
      "Average Shared Loss: 1.480 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 10939.977 \n",
      "Average KL Loss: 93.533 \n",
      "Average Regression Loss: 1.058 \n",
      "Average Shared Loss: 1.485 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 11073.061 \n",
      "Average KL Loss: 93.532 \n",
      "Average Regression Loss: 1.064 \n",
      "Average Shared Loss: 1.490 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 10986.987 \n",
      "Average KL Loss: 93.531 \n",
      "Average Regression Loss: 1.009 \n",
      "Average Shared Loss: 1.479 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 10982.422 \n",
      "Average KL Loss: 93.533 \n",
      "Average Regression Loss: 0.973 \n",
      "Average Shared Loss: 1.486 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 10958.264 \n",
      "Average KL Loss: 93.533 \n",
      "Average Regression Loss: 0.965 \n",
      "Average Shared Loss: 1.483 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 10984.433 \n",
      "Average KL Loss: 93.531 \n",
      "Average Regression Loss: 0.962 \n",
      "Average Shared Loss: 1.490 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 10973.233 \n",
      "Average KL Loss: 93.530 \n",
      "Average Regression Loss: 0.933 \n",
      "Average Shared Loss: 1.476 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 10970.087 \n",
      "Average KL Loss: 93.529 \n",
      "Average Regression Loss: 0.938 \n",
      "Average Shared Loss: 1.480 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 10916.009 \n",
      "Average KL Loss: 93.528 \n",
      "Average Regression Loss: 0.888 \n",
      "Average Shared Loss: 1.480 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 11012.656 \n",
      "Average KL Loss: 93.528 \n",
      "Average Regression Loss: 1.014 \n",
      "Average Shared Loss: 1.474 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 10989.014 \n",
      "Average KL Loss: 93.529 \n",
      "Average Regression Loss: 0.975 \n",
      "Average Shared Loss: 1.481 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 10975.776 \n",
      "Average KL Loss: 93.528 \n",
      "Average Regression Loss: 0.892 \n",
      "Average Shared Loss: 1.488 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 11007.785 \n",
      "Average KL Loss: 93.526 \n",
      "Average Regression Loss: 0.961 \n",
      "Average Shared Loss: 1.474 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 11017.034 \n",
      "Average KL Loss: 93.526 \n",
      "Average Regression Loss: 0.958 \n",
      "Average Shared Loss: 1.471 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 10965.949 \n",
      "Average KL Loss: 93.524 \n",
      "Average Regression Loss: 0.982 \n",
      "Average Shared Loss: 1.469 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 10965.330 \n",
      "Average KL Loss: 93.524 \n",
      "Average Regression Loss: 0.890 \n",
      "Average Shared Loss: 1.477 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 10940.239 \n",
      "Average KL Loss: 93.522 \n",
      "Average Regression Loss: 0.940 \n",
      "Average Shared Loss: 1.477 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 10940.523 \n",
      "Average KL Loss: 93.522 \n",
      "Average Regression Loss: 0.943 \n",
      "Average Shared Loss: 1.480 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 10969.040 \n",
      "Average KL Loss: 93.521 \n",
      "Average Regression Loss: 0.958 \n",
      "Average Shared Loss: 1.468 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 10974.118 \n",
      "Average KL Loss: 93.521 \n",
      "Average Regression Loss: 0.958 \n",
      "Average Shared Loss: 1.470 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 11025.065 \n",
      "Average KL Loss: 93.518 \n",
      "Average Regression Loss: 0.910 \n",
      "Average Shared Loss: 1.466 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 10953.142 \n",
      "Average KL Loss: 93.519 \n",
      "Average Regression Loss: 0.771 \n",
      "Average Shared Loss: 1.466 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 10968.715 \n",
      "Average KL Loss: 93.517 \n",
      "Average Regression Loss: 0.830 \n",
      "Average Shared Loss: 1.467 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 10970.068 \n",
      "Average KL Loss: 93.520 \n",
      "Average Regression Loss: 0.872 \n",
      "Average Shared Loss: 1.458 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 10990.674 \n",
      "Average KL Loss: 93.520 \n",
      "Average Regression Loss: 0.933 \n",
      "Average Shared Loss: 1.476 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 10975.227 \n",
      "Average KL Loss: 93.519 \n",
      "Average Regression Loss: 1.009 \n",
      "Average Shared Loss: 1.470 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 11012.276 \n",
      "Average KL Loss: 93.516 \n",
      "Average Regression Loss: 0.936 \n",
      "Average Shared Loss: 1.465 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 10958.014 \n",
      "Average KL Loss: 93.516 \n",
      "Average Regression Loss: 0.927 \n",
      "Average Shared Loss: 1.466 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 10986.504 \n",
      "Average KL Loss: 93.515 \n",
      "Average Regression Loss: 0.914 \n",
      "Average Shared Loss: 1.465 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 10918.742 \n",
      "Average KL Loss: 93.515 \n",
      "Average Regression Loss: 0.828 \n",
      "Average Shared Loss: 1.468 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 10997.148 \n",
      "Average KL Loss: 93.512 \n",
      "Average Regression Loss: 0.879 \n",
      "Average Shared Loss: 1.465 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 11028.901 \n",
      "Average KL Loss: 93.512 \n",
      "Average Regression Loss: 0.928 \n",
      "Average Shared Loss: 1.464 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 10984.581 \n",
      "Average KL Loss: 93.510 \n",
      "Average Regression Loss: 0.926 \n",
      "Average Shared Loss: 1.460 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 10951.360 \n",
      "Average KL Loss: 93.510 \n",
      "Average Regression Loss: 0.884 \n",
      "Average Shared Loss: 1.462 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 10980.785 \n",
      "Average KL Loss: 93.511 \n",
      "Average Regression Loss: 0.882 \n",
      "Average Shared Loss: 1.455 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 10943.887 \n",
      "Average KL Loss: 93.509 \n",
      "Average Regression Loss: 0.787 \n",
      "Average Shared Loss: 1.467 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 10990.661 \n",
      "Average KL Loss: 93.508 \n",
      "Average Regression Loss: 0.918 \n",
      "Average Shared Loss: 1.458 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 10922.031 \n",
      "Average KL Loss: 93.508 \n",
      "Average Regression Loss: 0.839 \n",
      "Average Shared Loss: 1.455 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 11009.220 \n",
      "Average KL Loss: 93.507 \n",
      "Average Regression Loss: 0.804 \n",
      "Average Shared Loss: 1.454 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 10990.000 \n",
      "Average KL Loss: 93.505 \n",
      "Average Regression Loss: 0.870 \n",
      "Average Shared Loss: 1.452 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 10932.631 \n",
      "Average KL Loss: 93.505 \n",
      "Average Regression Loss: 0.758 \n",
      "Average Shared Loss: 1.452 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 10942.569 \n",
      "Average KL Loss: 93.502 \n",
      "Average Regression Loss: 0.837 \n",
      "Average Shared Loss: 1.450 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 10950.727 \n",
      "Average KL Loss: 93.502 \n",
      "Average Regression Loss: 1.045 \n",
      "Average Shared Loss: 1.455 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "Average DNA Recon Loss: 25976.201 \n",
      "Average Gene Recon Loss: 411.676 \n",
      "Average DNA KL Loss: 177.738 \n",
      "Average Gene KL Loss: 1.004 \n",
      "Average Regressor Loss: 1.256 \n",
      "Average RMSE Loss: 1.121 \n",
      "Average Shared MSE Loss: 1.535 \n",
      "Average Shared RMSE Loss: 1.239 \n",
      "Average R2: 0.840 \n",
      "Average Indepence Loss: 0.008 \n",
      "\n",
      "Fold 9\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 11043.979 \n",
      "Average KL Loss: 93.551 \n",
      "Average Regression Loss: 1.055 \n",
      "Average Shared Loss: 1.471 \n",
      "Average Independence Loss: 0.278 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 11021.174 \n",
      "Average KL Loss: 93.681 \n",
      "Average Regression Loss: 1.148 \n",
      "Average Shared Loss: 1.468 \n",
      "Average Independence Loss: 1.142 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 10928.591 \n",
      "Average KL Loss: 93.772 \n",
      "Average Regression Loss: 1.191 \n",
      "Average Shared Loss: 1.481 \n",
      "Average Independence Loss: 1.749 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 10937.469 \n",
      "Average KL Loss: 93.827 \n",
      "Average Regression Loss: 1.219 \n",
      "Average Shared Loss: 1.484 \n",
      "Average Independence Loss: 2.097 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 10895.113 \n",
      "Average KL Loss: 93.896 \n",
      "Average Regression Loss: 1.185 \n",
      "Average Shared Loss: 1.491 \n",
      "Average Independence Loss: 2.429 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 10900.962 \n",
      "Average KL Loss: 93.908 \n",
      "Average Regression Loss: 1.186 \n",
      "Average Shared Loss: 1.483 \n",
      "Average Independence Loss: 2.292 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 10876.501 \n",
      "Average KL Loss: 93.932 \n",
      "Average Regression Loss: 1.244 \n",
      "Average Shared Loss: 1.483 \n",
      "Average Independence Loss: 2.398 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 10834.922 \n",
      "Average KL Loss: 93.903 \n",
      "Average Regression Loss: 1.202 \n",
      "Average Shared Loss: 1.479 \n",
      "Average Independence Loss: 2.491 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 10809.149 \n",
      "Average KL Loss: 93.836 \n",
      "Average Regression Loss: 1.050 \n",
      "Average Shared Loss: 1.473 \n",
      "Average Independence Loss: 2.415 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 10859.625 \n",
      "Average KL Loss: 93.778 \n",
      "Average Regression Loss: 1.144 \n",
      "Average Shared Loss: 1.463 \n",
      "Average Independence Loss: 2.483 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 10812.526 \n",
      "Average KL Loss: 93.735 \n",
      "Average Regression Loss: 1.180 \n",
      "Average Shared Loss: 1.476 \n",
      "Average Independence Loss: 2.582 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 10866.196 \n",
      "Average KL Loss: 93.686 \n",
      "Average Regression Loss: 1.239 \n",
      "Average Shared Loss: 1.469 \n",
      "Average Independence Loss: 2.614 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 10813.446 \n",
      "Average KL Loss: 93.643 \n",
      "Average Regression Loss: 1.197 \n",
      "Average Shared Loss: 1.461 \n",
      "Average Independence Loss: 2.599 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 10815.538 \n",
      "Average KL Loss: 93.611 \n",
      "Average Regression Loss: 1.139 \n",
      "Average Shared Loss: 1.458 \n",
      "Average Independence Loss: 2.602 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 10850.343 \n",
      "Average KL Loss: 93.586 \n",
      "Average Regression Loss: 1.175 \n",
      "Average Shared Loss: 1.467 \n",
      "Average Independence Loss: 2.634 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 10820.834 \n",
      "Average KL Loss: 93.564 \n",
      "Average Regression Loss: 1.113 \n",
      "Average Shared Loss: 1.465 \n",
      "Average Independence Loss: 2.611 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 10816.742 \n",
      "Average KL Loss: 93.532 \n",
      "Average Regression Loss: 1.143 \n",
      "Average Shared Loss: 1.468 \n",
      "Average Independence Loss: 2.549 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 10782.358 \n",
      "Average KL Loss: 93.491 \n",
      "Average Regression Loss: 1.256 \n",
      "Average Shared Loss: 1.445 \n",
      "Average Independence Loss: 2.513 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 10804.762 \n",
      "Average KL Loss: 93.429 \n",
      "Average Regression Loss: 1.114 \n",
      "Average Shared Loss: 1.450 \n",
      "Average Independence Loss: 2.475 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 10752.607 \n",
      "Average KL Loss: 93.336 \n",
      "Average Regression Loss: 1.109 \n",
      "Average Shared Loss: 1.452 \n",
      "Average Independence Loss: 2.474 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 10796.438 \n",
      "Average KL Loss: 93.262 \n",
      "Average Regression Loss: 1.167 \n",
      "Average Shared Loss: 1.452 \n",
      "Average Independence Loss: 2.501 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 10814.886 \n",
      "Average KL Loss: 93.214 \n",
      "Average Regression Loss: 1.176 \n",
      "Average Shared Loss: 1.441 \n",
      "Average Independence Loss: 2.490 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 10763.904 \n",
      "Average KL Loss: 93.163 \n",
      "Average Regression Loss: 1.180 \n",
      "Average Shared Loss: 1.449 \n",
      "Average Independence Loss: 2.485 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 10800.372 \n",
      "Average KL Loss: 93.110 \n",
      "Average Regression Loss: 1.124 \n",
      "Average Shared Loss: 1.440 \n",
      "Average Independence Loss: 2.460 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 10730.323 \n",
      "Average KL Loss: 93.053 \n",
      "Average Regression Loss: 1.170 \n",
      "Average Shared Loss: 1.442 \n",
      "Average Independence Loss: 2.386 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 10774.383 \n",
      "Average KL Loss: 93.007 \n",
      "Average Regression Loss: 1.213 \n",
      "Average Shared Loss: 1.437 \n",
      "Average Independence Loss: 2.377 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 10722.389 \n",
      "Average KL Loss: 92.962 \n",
      "Average Regression Loss: 1.164 \n",
      "Average Shared Loss: 1.435 \n",
      "Average Independence Loss: 2.488 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 10763.724 \n",
      "Average KL Loss: 92.893 \n",
      "Average Regression Loss: 1.109 \n",
      "Average Shared Loss: 1.426 \n",
      "Average Independence Loss: 2.490 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 10769.820 \n",
      "Average KL Loss: 92.837 \n",
      "Average Regression Loss: 1.376 \n",
      "Average Shared Loss: 1.422 \n",
      "Average Independence Loss: 2.440 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 10737.341 \n",
      "Average KL Loss: 92.790 \n",
      "Average Regression Loss: 1.070 \n",
      "Average Shared Loss: 1.424 \n",
      "Average Independence Loss: 2.284 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 10747.831 \n",
      "Average KL Loss: 92.707 \n",
      "Average Regression Loss: 1.183 \n",
      "Average Shared Loss: 1.409 \n",
      "Average Independence Loss: 2.208 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 10769.824 \n",
      "Average KL Loss: 92.617 \n",
      "Average Regression Loss: 1.165 \n",
      "Average Shared Loss: 1.412 \n",
      "Average Independence Loss: 2.154 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 10713.121 \n",
      "Average KL Loss: 92.533 \n",
      "Average Regression Loss: 1.169 \n",
      "Average Shared Loss: 1.418 \n",
      "Average Independence Loss: 2.070 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 10769.869 \n",
      "Average KL Loss: 92.472 \n",
      "Average Regression Loss: 1.183 \n",
      "Average Shared Loss: 1.414 \n",
      "Average Independence Loss: 2.035 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 10811.468 \n",
      "Average KL Loss: 92.436 \n",
      "Average Regression Loss: 1.165 \n",
      "Average Shared Loss: 1.413 \n",
      "Average Independence Loss: 2.002 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 10736.054 \n",
      "Average KL Loss: 92.436 \n",
      "Average Regression Loss: 1.102 \n",
      "Average Shared Loss: 1.417 \n",
      "Average Independence Loss: 2.007 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 10714.549 \n",
      "Average KL Loss: 92.390 \n",
      "Average Regression Loss: 1.235 \n",
      "Average Shared Loss: 1.409 \n",
      "Average Independence Loss: 1.946 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 10682.483 \n",
      "Average KL Loss: 92.326 \n",
      "Average Regression Loss: 1.172 \n",
      "Average Shared Loss: 1.418 \n",
      "Average Independence Loss: 1.932 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 10765.663 \n",
      "Average KL Loss: 92.219 \n",
      "Average Regression Loss: 1.207 \n",
      "Average Shared Loss: 1.420 \n",
      "Average Independence Loss: 1.941 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 10683.328 \n",
      "Average KL Loss: 92.135 \n",
      "Average Regression Loss: 1.078 \n",
      "Average Shared Loss: 1.404 \n",
      "Average Independence Loss: 1.972 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 10713.189 \n",
      "Average KL Loss: 92.047 \n",
      "Average Regression Loss: 1.136 \n",
      "Average Shared Loss: 1.403 \n",
      "Average Independence Loss: 1.926 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 10761.213 \n",
      "Average KL Loss: 91.981 \n",
      "Average Regression Loss: 1.250 \n",
      "Average Shared Loss: 1.403 \n",
      "Average Independence Loss: 1.866 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 10676.839 \n",
      "Average KL Loss: 91.945 \n",
      "Average Regression Loss: 1.159 \n",
      "Average Shared Loss: 1.413 \n",
      "Average Independence Loss: 1.767 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 10677.700 \n",
      "Average KL Loss: 91.858 \n",
      "Average Regression Loss: 1.147 \n",
      "Average Shared Loss: 1.402 \n",
      "Average Independence Loss: 1.655 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 10730.308 \n",
      "Average KL Loss: 91.800 \n",
      "Average Regression Loss: 1.146 \n",
      "Average Shared Loss: 1.392 \n",
      "Average Independence Loss: 1.634 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 10802.428 \n",
      "Average KL Loss: 91.736 \n",
      "Average Regression Loss: 1.186 \n",
      "Average Shared Loss: 1.397 \n",
      "Average Independence Loss: 1.579 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 10719.721 \n",
      "Average KL Loss: 91.666 \n",
      "Average Regression Loss: 1.151 \n",
      "Average Shared Loss: 1.390 \n",
      "Average Independence Loss: 1.504 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 10704.145 \n",
      "Average KL Loss: 91.608 \n",
      "Average Regression Loss: 1.125 \n",
      "Average Shared Loss: 1.392 \n",
      "Average Independence Loss: 1.515 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 10687.253 \n",
      "Average KL Loss: 91.540 \n",
      "Average Regression Loss: 1.243 \n",
      "Average Shared Loss: 1.380 \n",
      "Average Independence Loss: 1.490 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 10676.624 \n",
      "Average KL Loss: 91.471 \n",
      "Average Regression Loss: 1.122 \n",
      "Average Shared Loss: 1.391 \n",
      "Average Independence Loss: 1.461 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 10722.071 \n",
      "Average KL Loss: 91.410 \n",
      "Average Regression Loss: 1.061 \n",
      "Average Shared Loss: 1.375 \n",
      "Average Independence Loss: 0.948 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 10713.604 \n",
      "Average KL Loss: 91.390 \n",
      "Average Regression Loss: 1.016 \n",
      "Average Shared Loss: 1.371 \n",
      "Average Independence Loss: 0.210 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 10716.373 \n",
      "Average KL Loss: 91.387 \n",
      "Average Regression Loss: 0.903 \n",
      "Average Shared Loss: 1.382 \n",
      "Average Independence Loss: 0.076 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 10702.795 \n",
      "Average KL Loss: 91.386 \n",
      "Average Regression Loss: 0.937 \n",
      "Average Shared Loss: 1.365 \n",
      "Average Independence Loss: 0.028 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 10761.386 \n",
      "Average KL Loss: 91.386 \n",
      "Average Regression Loss: 1.114 \n",
      "Average Shared Loss: 1.365 \n",
      "Average Independence Loss: 0.014 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 10699.755 \n",
      "Average KL Loss: 91.387 \n",
      "Average Regression Loss: 1.045 \n",
      "Average Shared Loss: 1.375 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 10729.165 \n",
      "Average KL Loss: 91.388 \n",
      "Average Regression Loss: 1.004 \n",
      "Average Shared Loss: 1.367 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 10763.644 \n",
      "Average KL Loss: 91.388 \n",
      "Average Regression Loss: 0.906 \n",
      "Average Shared Loss: 1.360 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 10809.979 \n",
      "Average KL Loss: 91.387 \n",
      "Average Regression Loss: 0.996 \n",
      "Average Shared Loss: 1.374 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 10770.179 \n",
      "Average KL Loss: 91.387 \n",
      "Average Regression Loss: 0.921 \n",
      "Average Shared Loss: 1.361 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 10834.364 \n",
      "Average KL Loss: 91.387 \n",
      "Average Regression Loss: 0.981 \n",
      "Average Shared Loss: 1.375 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 10706.930 \n",
      "Average KL Loss: 91.387 \n",
      "Average Regression Loss: 0.895 \n",
      "Average Shared Loss: 1.366 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 10770.209 \n",
      "Average KL Loss: 91.387 \n",
      "Average Regression Loss: 0.958 \n",
      "Average Shared Loss: 1.368 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 10790.440 \n",
      "Average KL Loss: 91.387 \n",
      "Average Regression Loss: 0.975 \n",
      "Average Shared Loss: 1.363 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 10806.761 \n",
      "Average KL Loss: 91.387 \n",
      "Average Regression Loss: 0.839 \n",
      "Average Shared Loss: 1.368 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 10764.113 \n",
      "Average KL Loss: 91.386 \n",
      "Average Regression Loss: 0.917 \n",
      "Average Shared Loss: 1.365 \n",
      "Average Independence Loss: 0.013 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 10725.323 \n",
      "Average KL Loss: 91.386 \n",
      "Average Regression Loss: 0.849 \n",
      "Average Shared Loss: 1.361 \n",
      "Average Independence Loss: 0.013 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 10733.875 \n",
      "Average KL Loss: 91.386 \n",
      "Average Regression Loss: 0.892 \n",
      "Average Shared Loss: 1.357 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 10755.646 \n",
      "Average KL Loss: 91.387 \n",
      "Average Regression Loss: 0.841 \n",
      "Average Shared Loss: 1.360 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 10728.063 \n",
      "Average KL Loss: 91.386 \n",
      "Average Regression Loss: 0.883 \n",
      "Average Shared Loss: 1.362 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 10755.838 \n",
      "Average KL Loss: 91.386 \n",
      "Average Regression Loss: 0.875 \n",
      "Average Shared Loss: 1.354 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 10742.445 \n",
      "Average KL Loss: 91.386 \n",
      "Average Regression Loss: 0.828 \n",
      "Average Shared Loss: 1.357 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 10726.899 \n",
      "Average KL Loss: 91.386 \n",
      "Average Regression Loss: 0.810 \n",
      "Average Shared Loss: 1.357 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 10762.178 \n",
      "Average KL Loss: 91.386 \n",
      "Average Regression Loss: 0.836 \n",
      "Average Shared Loss: 1.352 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 10683.274 \n",
      "Average KL Loss: 91.386 \n",
      "Average Regression Loss: 0.767 \n",
      "Average Shared Loss: 1.361 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 10774.043 \n",
      "Average KL Loss: 91.385 \n",
      "Average Regression Loss: 0.891 \n",
      "Average Shared Loss: 1.348 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 10667.140 \n",
      "Average KL Loss: 91.385 \n",
      "Average Regression Loss: 0.773 \n",
      "Average Shared Loss: 1.358 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 10851.302 \n",
      "Average KL Loss: 91.385 \n",
      "Average Regression Loss: 0.855 \n",
      "Average Shared Loss: 1.359 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 10784.817 \n",
      "Average KL Loss: 91.385 \n",
      "Average Regression Loss: 0.883 \n",
      "Average Shared Loss: 1.355 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 10741.761 \n",
      "Average KL Loss: 91.384 \n",
      "Average Regression Loss: 0.863 \n",
      "Average Shared Loss: 1.346 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 10749.887 \n",
      "Average KL Loss: 91.384 \n",
      "Average Regression Loss: 0.840 \n",
      "Average Shared Loss: 1.359 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 10747.703 \n",
      "Average KL Loss: 91.384 \n",
      "Average Regression Loss: 0.827 \n",
      "Average Shared Loss: 1.345 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 10815.123 \n",
      "Average KL Loss: 91.383 \n",
      "Average Regression Loss: 0.928 \n",
      "Average Shared Loss: 1.347 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 10743.844 \n",
      "Average KL Loss: 91.381 \n",
      "Average Regression Loss: 0.850 \n",
      "Average Shared Loss: 1.347 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 10754.754 \n",
      "Average KL Loss: 91.380 \n",
      "Average Regression Loss: 0.810 \n",
      "Average Shared Loss: 1.347 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 10765.761 \n",
      "Average KL Loss: 91.381 \n",
      "Average Regression Loss: 0.813 \n",
      "Average Shared Loss: 1.340 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 10716.222 \n",
      "Average KL Loss: 91.381 \n",
      "Average Regression Loss: 0.847 \n",
      "Average Shared Loss: 1.342 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 10740.167 \n",
      "Average KL Loss: 91.381 \n",
      "Average Regression Loss: 0.859 \n",
      "Average Shared Loss: 1.343 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 10765.354 \n",
      "Average KL Loss: 91.380 \n",
      "Average Regression Loss: 0.863 \n",
      "Average Shared Loss: 1.348 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 10714.818 \n",
      "Average KL Loss: 91.379 \n",
      "Average Regression Loss: 0.812 \n",
      "Average Shared Loss: 1.340 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 10814.032 \n",
      "Average KL Loss: 91.379 \n",
      "Average Regression Loss: 0.821 \n",
      "Average Shared Loss: 1.350 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 10814.903 \n",
      "Average KL Loss: 91.379 \n",
      "Average Regression Loss: 0.831 \n",
      "Average Shared Loss: 1.332 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 10773.436 \n",
      "Average KL Loss: 91.380 \n",
      "Average Regression Loss: 0.856 \n",
      "Average Shared Loss: 1.333 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 10772.084 \n",
      "Average KL Loss: 91.381 \n",
      "Average Regression Loss: 0.740 \n",
      "Average Shared Loss: 1.340 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 10761.683 \n",
      "Average KL Loss: 91.382 \n",
      "Average Regression Loss: 0.936 \n",
      "Average Shared Loss: 1.335 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 10686.109 \n",
      "Average KL Loss: 91.381 \n",
      "Average Regression Loss: 0.817 \n",
      "Average Shared Loss: 1.334 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 10758.378 \n",
      "Average KL Loss: 91.381 \n",
      "Average Regression Loss: 0.807 \n",
      "Average Shared Loss: 1.339 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 10732.447 \n",
      "Average KL Loss: 91.380 \n",
      "Average Regression Loss: 0.846 \n",
      "Average Shared Loss: 1.340 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 10764.797 \n",
      "Average KL Loss: 91.380 \n",
      "Average Regression Loss: 0.994 \n",
      "Average Shared Loss: 1.334 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 10792.107 \n",
      "Average KL Loss: 91.381 \n",
      "Average Regression Loss: 0.863 \n",
      "Average Shared Loss: 1.325 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "Average DNA Recon Loss: 25463.413 \n",
      "Average Gene Recon Loss: 288.304 \n",
      "Average DNA KL Loss: 173.986 \n",
      "Average Gene KL Loss: 0.519 \n",
      "Average Regressor Loss: 1.143 \n",
      "Average RMSE Loss: 1.069 \n",
      "Average Shared MSE Loss: 1.186 \n",
      "Average Shared RMSE Loss: 1.089 \n",
      "Average R2: 0.872 \n",
      "Average Indepence Loss: 0.007 \n",
      "\n",
      "Fold 10\n",
      "Start Training (Unsupervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 10876.244 \n",
      "Average KL Loss: 91.374 \n",
      "Average Regression Loss: 0.954 \n",
      "Average Shared Loss: 1.347 \n",
      "Average Independence Loss: 0.200 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 10847.938 \n",
      "Average KL Loss: 91.424 \n",
      "Average Regression Loss: 1.005 \n",
      "Average Shared Loss: 1.352 \n",
      "Average Independence Loss: 0.665 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 10881.366 \n",
      "Average KL Loss: 91.496 \n",
      "Average Regression Loss: 1.005 \n",
      "Average Shared Loss: 1.367 \n",
      "Average Independence Loss: 1.014 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 10897.652 \n",
      "Average KL Loss: 91.609 \n",
      "Average Regression Loss: 1.177 \n",
      "Average Shared Loss: 1.369 \n",
      "Average Independence Loss: 1.142 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 10830.074 \n",
      "Average KL Loss: 91.712 \n",
      "Average Regression Loss: 1.060 \n",
      "Average Shared Loss: 1.363 \n",
      "Average Independence Loss: 1.169 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 10816.571 \n",
      "Average KL Loss: 91.761 \n",
      "Average Regression Loss: 1.017 \n",
      "Average Shared Loss: 1.376 \n",
      "Average Independence Loss: 1.278 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 10790.147 \n",
      "Average KL Loss: 91.773 \n",
      "Average Regression Loss: 1.030 \n",
      "Average Shared Loss: 1.383 \n",
      "Average Independence Loss: 1.391 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 10811.142 \n",
      "Average KL Loss: 91.799 \n",
      "Average Regression Loss: 1.184 \n",
      "Average Shared Loss: 1.379 \n",
      "Average Independence Loss: 1.364 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 10840.352 \n",
      "Average KL Loss: 91.777 \n",
      "Average Regression Loss: 1.132 \n",
      "Average Shared Loss: 1.378 \n",
      "Average Independence Loss: 1.314 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 10794.589 \n",
      "Average KL Loss: 91.751 \n",
      "Average Regression Loss: 1.045 \n",
      "Average Shared Loss: 1.373 \n",
      "Average Independence Loss: 1.304 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 10815.751 \n",
      "Average KL Loss: 91.733 \n",
      "Average Regression Loss: 1.067 \n",
      "Average Shared Loss: 1.377 \n",
      "Average Independence Loss: 1.322 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 10801.219 \n",
      "Average KL Loss: 91.700 \n",
      "Average Regression Loss: 1.298 \n",
      "Average Shared Loss: 1.370 \n",
      "Average Independence Loss: 1.332 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 10785.235 \n",
      "Average KL Loss: 91.703 \n",
      "Average Regression Loss: 1.024 \n",
      "Average Shared Loss: 1.369 \n",
      "Average Independence Loss: 1.320 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 10739.444 \n",
      "Average KL Loss: 91.653 \n",
      "Average Regression Loss: 1.085 \n",
      "Average Shared Loss: 1.370 \n",
      "Average Independence Loss: 1.334 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 10750.665 \n",
      "Average KL Loss: 91.584 \n",
      "Average Regression Loss: 1.234 \n",
      "Average Shared Loss: 1.364 \n",
      "Average Independence Loss: 1.371 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 10751.366 \n",
      "Average KL Loss: 91.519 \n",
      "Average Regression Loss: 1.091 \n",
      "Average Shared Loss: 1.366 \n",
      "Average Independence Loss: 1.402 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 10747.296 \n",
      "Average KL Loss: 91.480 \n",
      "Average Regression Loss: 1.077 \n",
      "Average Shared Loss: 1.371 \n",
      "Average Independence Loss: 1.318 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 10791.118 \n",
      "Average KL Loss: 91.415 \n",
      "Average Regression Loss: 1.153 \n",
      "Average Shared Loss: 1.360 \n",
      "Average Independence Loss: 1.240 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 10746.278 \n",
      "Average KL Loss: 91.381 \n",
      "Average Regression Loss: 1.140 \n",
      "Average Shared Loss: 1.356 \n",
      "Average Independence Loss: 1.174 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 10756.921 \n",
      "Average KL Loss: 91.348 \n",
      "Average Regression Loss: 1.012 \n",
      "Average Shared Loss: 1.359 \n",
      "Average Independence Loss: 1.164 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 10717.836 \n",
      "Average KL Loss: 91.304 \n",
      "Average Regression Loss: 1.090 \n",
      "Average Shared Loss: 1.353 \n",
      "Average Independence Loss: 1.203 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 10726.369 \n",
      "Average KL Loss: 91.279 \n",
      "Average Regression Loss: 1.032 \n",
      "Average Shared Loss: 1.358 \n",
      "Average Independence Loss: 1.280 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 10726.779 \n",
      "Average KL Loss: 91.228 \n",
      "Average Regression Loss: 1.100 \n",
      "Average Shared Loss: 1.347 \n",
      "Average Independence Loss: 1.305 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 10691.905 \n",
      "Average KL Loss: 91.145 \n",
      "Average Regression Loss: 1.058 \n",
      "Average Shared Loss: 1.336 \n",
      "Average Independence Loss: 1.206 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 10704.708 \n",
      "Average KL Loss: 91.071 \n",
      "Average Regression Loss: 1.076 \n",
      "Average Shared Loss: 1.338 \n",
      "Average Independence Loss: 1.208 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 10776.803 \n",
      "Average KL Loss: 90.993 \n",
      "Average Regression Loss: 1.135 \n",
      "Average Shared Loss: 1.347 \n",
      "Average Independence Loss: 1.194 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 10775.294 \n",
      "Average KL Loss: 90.954 \n",
      "Average Regression Loss: 1.168 \n",
      "Average Shared Loss: 1.333 \n",
      "Average Independence Loss: 1.185 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 10716.858 \n",
      "Average KL Loss: 90.928 \n",
      "Average Regression Loss: 1.074 \n",
      "Average Shared Loss: 1.338 \n",
      "Average Independence Loss: 1.185 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 10687.415 \n",
      "Average KL Loss: 90.896 \n",
      "Average Regression Loss: 0.993 \n",
      "Average Shared Loss: 1.328 \n",
      "Average Independence Loss: 1.186 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 10659.721 \n",
      "Average KL Loss: 90.837 \n",
      "Average Regression Loss: 1.106 \n",
      "Average Shared Loss: 1.332 \n",
      "Average Independence Loss: 1.177 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 10765.141 \n",
      "Average KL Loss: 90.759 \n",
      "Average Regression Loss: 1.258 \n",
      "Average Shared Loss: 1.321 \n",
      "Average Independence Loss: 1.182 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 10673.609 \n",
      "Average KL Loss: 90.739 \n",
      "Average Regression Loss: 1.090 \n",
      "Average Shared Loss: 1.339 \n",
      "Average Independence Loss: 1.217 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 10787.140 \n",
      "Average KL Loss: 90.742 \n",
      "Average Regression Loss: 1.166 \n",
      "Average Shared Loss: 1.339 \n",
      "Average Independence Loss: 1.226 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 10697.022 \n",
      "Average KL Loss: 90.715 \n",
      "Average Regression Loss: 1.051 \n",
      "Average Shared Loss: 1.329 \n",
      "Average Independence Loss: 1.199 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 10689.199 \n",
      "Average KL Loss: 90.649 \n",
      "Average Regression Loss: 1.099 \n",
      "Average Shared Loss: 1.333 \n",
      "Average Independence Loss: 1.162 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 10747.673 \n",
      "Average KL Loss: 90.591 \n",
      "Average Regression Loss: 1.184 \n",
      "Average Shared Loss: 1.331 \n",
      "Average Independence Loss: 1.168 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 10713.698 \n",
      "Average KL Loss: 90.528 \n",
      "Average Regression Loss: 1.144 \n",
      "Average Shared Loss: 1.336 \n",
      "Average Independence Loss: 1.125 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 10708.279 \n",
      "Average KL Loss: 90.486 \n",
      "Average Regression Loss: 1.095 \n",
      "Average Shared Loss: 1.339 \n",
      "Average Independence Loss: 1.101 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 10695.297 \n",
      "Average KL Loss: 90.427 \n",
      "Average Regression Loss: 1.116 \n",
      "Average Shared Loss: 1.325 \n",
      "Average Independence Loss: 1.097 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 10668.744 \n",
      "Average KL Loss: 90.385 \n",
      "Average Regression Loss: 1.174 \n",
      "Average Shared Loss: 1.327 \n",
      "Average Independence Loss: 1.140 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 10723.158 \n",
      "Average KL Loss: 90.309 \n",
      "Average Regression Loss: 1.059 \n",
      "Average Shared Loss: 1.320 \n",
      "Average Independence Loss: 1.113 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 10723.371 \n",
      "Average KL Loss: 90.230 \n",
      "Average Regression Loss: 1.138 \n",
      "Average Shared Loss: 1.314 \n",
      "Average Independence Loss: 1.159 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 10766.817 \n",
      "Average KL Loss: 90.182 \n",
      "Average Regression Loss: 1.253 \n",
      "Average Shared Loss: 1.316 \n",
      "Average Independence Loss: 1.153 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 10738.679 \n",
      "Average KL Loss: 90.166 \n",
      "Average Regression Loss: 1.084 \n",
      "Average Shared Loss: 1.318 \n",
      "Average Independence Loss: 1.172 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 10658.511 \n",
      "Average KL Loss: 90.130 \n",
      "Average Regression Loss: 1.059 \n",
      "Average Shared Loss: 1.318 \n",
      "Average Independence Loss: 1.228 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 10626.111 \n",
      "Average KL Loss: 90.048 \n",
      "Average Regression Loss: 0.938 \n",
      "Average Shared Loss: 1.314 \n",
      "Average Independence Loss: 1.219 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 10703.475 \n",
      "Average KL Loss: 89.947 \n",
      "Average Regression Loss: 1.046 \n",
      "Average Shared Loss: 1.311 \n",
      "Average Independence Loss: 1.149 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 10707.023 \n",
      "Average KL Loss: 89.884 \n",
      "Average Regression Loss: 1.122 \n",
      "Average Shared Loss: 1.313 \n",
      "Average Independence Loss: 1.156 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 10689.098 \n",
      "Average KL Loss: 89.824 \n",
      "Average Regression Loss: 1.149 \n",
      "Average Shared Loss: 1.314 \n",
      "Average Independence Loss: 1.155 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 10666.707 \n",
      "Average KL Loss: 89.748 \n",
      "Average Regression Loss: 1.009 \n",
      "Average Shared Loss: 1.309 \n",
      "Average Independence Loss: 1.185 \n",
      "\n",
      "Start Training (Supervised Phase)\n",
      "=====> Epoch 1 \n",
      "Average Recon Loss: 10703.498 \n",
      "Average KL Loss: 89.685 \n",
      "Average Regression Loss: 0.978 \n",
      "Average Shared Loss: 1.306 \n",
      "Average Independence Loss: 0.659 \n",
      "\n",
      "=====> Epoch 2 \n",
      "Average Recon Loss: 10746.709 \n",
      "Average KL Loss: 89.667 \n",
      "Average Regression Loss: 1.017 \n",
      "Average Shared Loss: 1.305 \n",
      "Average Independence Loss: 0.151 \n",
      "\n",
      "=====> Epoch 3 \n",
      "Average Recon Loss: 10727.806 \n",
      "Average KL Loss: 89.664 \n",
      "Average Regression Loss: 0.968 \n",
      "Average Shared Loss: 1.310 \n",
      "Average Independence Loss: 0.056 \n",
      "\n",
      "=====> Epoch 4 \n",
      "Average Recon Loss: 10695.177 \n",
      "Average KL Loss: 89.663 \n",
      "Average Regression Loss: 0.867 \n",
      "Average Shared Loss: 1.304 \n",
      "Average Independence Loss: 0.024 \n",
      "\n",
      "=====> Epoch 5 \n",
      "Average Recon Loss: 10726.016 \n",
      "Average KL Loss: 89.663 \n",
      "Average Regression Loss: 0.829 \n",
      "Average Shared Loss: 1.309 \n",
      "Average Independence Loss: 0.013 \n",
      "\n",
      "=====> Epoch 6 \n",
      "Average Recon Loss: 10714.043 \n",
      "Average KL Loss: 89.663 \n",
      "Average Regression Loss: 0.917 \n",
      "Average Shared Loss: 1.306 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 7 \n",
      "Average Recon Loss: 10729.097 \n",
      "Average KL Loss: 89.662 \n",
      "Average Regression Loss: 0.835 \n",
      "Average Shared Loss: 1.312 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 8 \n",
      "Average Recon Loss: 10705.683 \n",
      "Average KL Loss: 89.662 \n",
      "Average Regression Loss: 1.027 \n",
      "Average Shared Loss: 1.302 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 9 \n",
      "Average Recon Loss: 10663.745 \n",
      "Average KL Loss: 89.662 \n",
      "Average Regression Loss: 0.814 \n",
      "Average Shared Loss: 1.300 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 10 \n",
      "Average Recon Loss: 10751.689 \n",
      "Average KL Loss: 89.662 \n",
      "Average Regression Loss: 0.831 \n",
      "Average Shared Loss: 1.296 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 11 \n",
      "Average Recon Loss: 10790.854 \n",
      "Average KL Loss: 89.662 \n",
      "Average Regression Loss: 0.894 \n",
      "Average Shared Loss: 1.299 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 12 \n",
      "Average Recon Loss: 10699.533 \n",
      "Average KL Loss: 89.662 \n",
      "Average Regression Loss: 0.810 \n",
      "Average Shared Loss: 1.302 \n",
      "Average Independence Loss: 0.012 \n",
      "\n",
      "=====> Epoch 13 \n",
      "Average Recon Loss: 10799.961 \n",
      "Average KL Loss: 89.661 \n",
      "Average Regression Loss: 0.913 \n",
      "Average Shared Loss: 1.296 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 14 \n",
      "Average Recon Loss: 10758.973 \n",
      "Average KL Loss: 89.661 \n",
      "Average Regression Loss: 0.956 \n",
      "Average Shared Loss: 1.292 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 15 \n",
      "Average Recon Loss: 10731.607 \n",
      "Average KL Loss: 89.660 \n",
      "Average Regression Loss: 0.911 \n",
      "Average Shared Loss: 1.295 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 16 \n",
      "Average Recon Loss: 10703.918 \n",
      "Average KL Loss: 89.659 \n",
      "Average Regression Loss: 0.869 \n",
      "Average Shared Loss: 1.290 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 17 \n",
      "Average Recon Loss: 10711.761 \n",
      "Average KL Loss: 89.659 \n",
      "Average Regression Loss: 0.866 \n",
      "Average Shared Loss: 1.296 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 18 \n",
      "Average Recon Loss: 10740.144 \n",
      "Average KL Loss: 89.658 \n",
      "Average Regression Loss: 0.886 \n",
      "Average Shared Loss: 1.291 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 19 \n",
      "Average Recon Loss: 10756.294 \n",
      "Average KL Loss: 89.658 \n",
      "Average Regression Loss: 0.882 \n",
      "Average Shared Loss: 1.297 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 20 \n",
      "Average Recon Loss: 10708.642 \n",
      "Average KL Loss: 89.658 \n",
      "Average Regression Loss: 0.830 \n",
      "Average Shared Loss: 1.287 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 21 \n",
      "Average Recon Loss: 10721.019 \n",
      "Average KL Loss: 89.658 \n",
      "Average Regression Loss: 0.774 \n",
      "Average Shared Loss: 1.294 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 22 \n",
      "Average Recon Loss: 10702.094 \n",
      "Average KL Loss: 89.657 \n",
      "Average Regression Loss: 0.774 \n",
      "Average Shared Loss: 1.285 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 23 \n",
      "Average Recon Loss: 10749.076 \n",
      "Average KL Loss: 89.657 \n",
      "Average Regression Loss: 0.914 \n",
      "Average Shared Loss: 1.287 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 24 \n",
      "Average Recon Loss: 10747.330 \n",
      "Average KL Loss: 89.657 \n",
      "Average Regression Loss: 0.884 \n",
      "Average Shared Loss: 1.290 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 25 \n",
      "Average Recon Loss: 10704.565 \n",
      "Average KL Loss: 89.657 \n",
      "Average Regression Loss: 0.759 \n",
      "Average Shared Loss: 1.287 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 26 \n",
      "Average Recon Loss: 10728.035 \n",
      "Average KL Loss: 89.657 \n",
      "Average Regression Loss: 0.816 \n",
      "Average Shared Loss: 1.288 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 27 \n",
      "Average Recon Loss: 10749.603 \n",
      "Average KL Loss: 89.657 \n",
      "Average Regression Loss: 0.783 \n",
      "Average Shared Loss: 1.292 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 28 \n",
      "Average Recon Loss: 10739.697 \n",
      "Average KL Loss: 89.657 \n",
      "Average Regression Loss: 0.857 \n",
      "Average Shared Loss: 1.286 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 29 \n",
      "Average Recon Loss: 10775.267 \n",
      "Average KL Loss: 89.657 \n",
      "Average Regression Loss: 0.885 \n",
      "Average Shared Loss: 1.283 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 30 \n",
      "Average Recon Loss: 10737.493 \n",
      "Average KL Loss: 89.656 \n",
      "Average Regression Loss: 0.867 \n",
      "Average Shared Loss: 1.288 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 31 \n",
      "Average Recon Loss: 10736.191 \n",
      "Average KL Loss: 89.655 \n",
      "Average Regression Loss: 0.773 \n",
      "Average Shared Loss: 1.282 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 32 \n",
      "Average Recon Loss: 10698.306 \n",
      "Average KL Loss: 89.655 \n",
      "Average Regression Loss: 0.806 \n",
      "Average Shared Loss: 1.286 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 33 \n",
      "Average Recon Loss: 10708.501 \n",
      "Average KL Loss: 89.655 \n",
      "Average Regression Loss: 0.843 \n",
      "Average Shared Loss: 1.282 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 34 \n",
      "Average Recon Loss: 10739.945 \n",
      "Average KL Loss: 89.655 \n",
      "Average Regression Loss: 0.687 \n",
      "Average Shared Loss: 1.277 \n",
      "Average Independence Loss: 0.011 \n",
      "\n",
      "=====> Epoch 35 \n",
      "Average Recon Loss: 10736.991 \n",
      "Average KL Loss: 89.655 \n",
      "Average Regression Loss: 0.693 \n",
      "Average Shared Loss: 1.272 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 36 \n",
      "Average Recon Loss: 10716.007 \n",
      "Average KL Loss: 89.655 \n",
      "Average Regression Loss: 0.786 \n",
      "Average Shared Loss: 1.267 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 37 \n",
      "Average Recon Loss: 10726.306 \n",
      "Average KL Loss: 89.655 \n",
      "Average Regression Loss: 0.774 \n",
      "Average Shared Loss: 1.273 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 38 \n",
      "Average Recon Loss: 10710.177 \n",
      "Average KL Loss: 89.655 \n",
      "Average Regression Loss: 0.761 \n",
      "Average Shared Loss: 1.279 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "=====> Epoch 39 \n",
      "Average Recon Loss: 10778.957 \n",
      "Average KL Loss: 89.655 \n",
      "Average Regression Loss: 0.704 \n",
      "Average Shared Loss: 1.271 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 40 \n",
      "Average Recon Loss: 10760.858 \n",
      "Average KL Loss: 89.655 \n",
      "Average Regression Loss: 0.877 \n",
      "Average Shared Loss: 1.265 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 41 \n",
      "Average Recon Loss: 10698.885 \n",
      "Average KL Loss: 89.654 \n",
      "Average Regression Loss: 0.719 \n",
      "Average Shared Loss: 1.269 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 42 \n",
      "Average Recon Loss: 10711.188 \n",
      "Average KL Loss: 89.654 \n",
      "Average Regression Loss: 0.784 \n",
      "Average Shared Loss: 1.263 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 43 \n",
      "Average Recon Loss: 10740.831 \n",
      "Average KL Loss: 89.654 \n",
      "Average Regression Loss: 0.799 \n",
      "Average Shared Loss: 1.271 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 44 \n",
      "Average Recon Loss: 10753.932 \n",
      "Average KL Loss: 89.654 \n",
      "Average Regression Loss: 0.893 \n",
      "Average Shared Loss: 1.276 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 45 \n",
      "Average Recon Loss: 10743.538 \n",
      "Average KL Loss: 89.655 \n",
      "Average Regression Loss: 0.871 \n",
      "Average Shared Loss: 1.266 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 46 \n",
      "Average Recon Loss: 10724.058 \n",
      "Average KL Loss: 89.655 \n",
      "Average Regression Loss: 0.826 \n",
      "Average Shared Loss: 1.264 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 47 \n",
      "Average Recon Loss: 10738.931 \n",
      "Average KL Loss: 89.657 \n",
      "Average Regression Loss: 0.718 \n",
      "Average Shared Loss: 1.255 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 48 \n",
      "Average Recon Loss: 10768.966 \n",
      "Average KL Loss: 89.657 \n",
      "Average Regression Loss: 0.768 \n",
      "Average Shared Loss: 1.265 \n",
      "Average Independence Loss: 0.008 \n",
      "\n",
      "=====> Epoch 49 \n",
      "Average Recon Loss: 10748.295 \n",
      "Average KL Loss: 89.657 \n",
      "Average Regression Loss: 0.712 \n",
      "Average Shared Loss: 1.264 \n",
      "Average Independence Loss: 0.009 \n",
      "\n",
      "=====> Epoch 50 \n",
      "Average Recon Loss: 10702.380 \n",
      "Average KL Loss: 89.656 \n",
      "Average Regression Loss: 0.741 \n",
      "Average Shared Loss: 1.258 \n",
      "Average Independence Loss: 0.010 \n",
      "\n",
      "Average DNA Recon Loss: 24237.966 \n",
      "Average Gene Recon Loss: 208.191 \n",
      "Average DNA KL Loss: 170.253 \n",
      "Average Gene KL Loss: 0.650 \n",
      "Average Regressor Loss: 0.964 \n",
      "Average RMSE Loss: 0.982 \n",
      "Average Shared MSE Loss: 1.000 \n",
      "Average Shared RMSE Loss: 1.000 \n",
      "Average R2: 0.877 \n",
      "Average Indepence Loss: 0.009 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_train = {'Train Recon Loss': [], 'Train KL Loss': [], 'Train MSE Loss': [], 'Train Shared Loss': [], 'Train Independence Loss': []}\n",
    "history_test = {'Test DNA Recon Loss': [],'Test Gene Recon Loss': [], 'Test KL DNA Loss': [], 'Test KL Gene Loss': [], 'Test MSE Loss': [], 'Test RMSE Loss': [], 'Test Shared Loss': [], 'Test Shared RMSE Loss': [],  'Test R2': [], 'Test Independence Loss': []}\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(full_dataset)))):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(full_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(full_dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "    print(\"Start Training (Unsupervised Phase)\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_recon_ave, train_kl_ave, train_reg_ave, train_shared_ave, train_ind_ave = train(model, train_loader, epoch, optimizer, w_recon_loss= 1, w_kl_loss= 1, w_reg_loss= 0, w_shared_loss= 0, w_ind_loss= 0)\n",
    "        history_train['Train Recon Loss'].append(train_recon_ave)\n",
    "        history_train['Train KL Loss'].append(train_kl_ave)\n",
    "        history_train['Train MSE Loss'].append(train_reg_ave)\n",
    "        history_train['Train Shared Loss'].append(train_shared_ave)\n",
    "        history_train['Train Independence Loss'].append(train_ind_ave)\n",
    "    \n",
    "    print(\"Start Training (Supervised Phase)\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_recon_ave, train_kl_ave, train_reg_ave, train_shared_ave, train_ind_ave = train(model, train_loader, epoch, optimizer, w_recon_loss= 0, w_kl_loss= 0, w_reg_loss= 1, w_shared_loss= 1, w_ind_loss=1)\n",
    "        history_train['Train Recon Loss'].append(train_recon_ave)\n",
    "        history_train['Train KL Loss'].append(train_kl_ave)\n",
    "        history_train['Train MSE Loss'].append(train_reg_ave)\n",
    "        history_train['Train Shared Loss'].append(train_shared_ave)\n",
    "        history_train['Train Independence Loss'].append(train_ind_ave)\n",
    "\n",
    "    test_recon_dna_ave, test_recon_gene_ave, test_kl_dna_ave, test_kl_gene_ave, test_reg_ave, test_rmse_ave, test_shared_ave, test_shared_rmse_ave, test_r2_ave, test_ind_ave = test(test_loader, model)\n",
    "    history_test['Test DNA Recon Loss'].append(test_recon_dna_ave)\n",
    "    history_test['Test Gene Recon Loss'].append(test_recon_gene_ave)\n",
    "    history_test['Test KL DNA Loss'].append(test_kl_dna_ave)\n",
    "    history_test['Test KL Gene Loss'].append(test_kl_gene_ave)\n",
    "    history_test['Test MSE Loss'].append(test_reg_ave)\n",
    "    history_test['Test RMSE Loss'].append(test_rmse_ave)\n",
    "    history_test['Test Shared Loss'].append(test_shared_ave)\n",
    "    history_test['Test Shared RMSE Loss'].append(test_shared_rmse_ave)\n",
    "    history_test['Test R2'].append(test_r2_ave)\n",
    "    history_test['Test Independence Loss'].append(test_ind_ave)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of model through 5 Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of 10 fold cross validation using common and unique factors:\n",
      "\n",
      "Average Testing DNA Recon Loss: 30002.3058 \n",
      "Average Testing Gene Recon Loss: 5090.4284 \n",
      " Average Testing DNA KL Loss: 172.9617 \n",
      " Average Testing Gene KL Loss: 9.0482 \n",
      " Average Testing MSE Loss: 2.614 \n",
      " Average Testing RMSE Loss: 1.526 \n",
      " Average Testing Shared Loss: 1.726 \n",
      " Average Testing Shared RMSE Loss: 1.304 \n",
      " Average Testing R2 Loss: 0.702 \n",
      " Average Testing Independence Loss: 6.749 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_dna_recon_loss = np.mean(history_test['Test DNA Recon Loss'])\n",
    "cv_gene_recon_loss = np.mean(history_test['Test Gene Recon Loss'])\n",
    "cv_dna_kl_loss = np.mean(history_test['Test KL DNA Loss'])\n",
    "cv_gene_kl_loss = np.mean(history_test['Test KL Gene Loss'])\n",
    "cv_mse_loss = np.mean(history_test['Test MSE Loss'])\n",
    "cv_rmse_loss = np.mean(history_test['Test RMSE Loss'])\n",
    "cv_shared_loss = np.mean(history_test['Test Shared Loss'])\n",
    "cv_shared_rmse_loss = np.mean(history_test['Test Shared RMSE Loss'])\n",
    "cv_r2_loss = np.mean(history_test['Test R2'])\n",
    "cv_ind_loss = np.mean(history_test['Test Independence Loss'])\n",
    "\n",
    "print('Performance of {} fold cross validation using common and unique factors:'.format(k))\n",
    "print(\"\\nAverage Testing DNA Recon Loss: {:.4f} \\nAverage Testing Gene Recon Loss: {:.4f} \\n Average Testing DNA KL Loss: {:.4f} \\n Average Testing Gene KL Loss: {:.4f} \\n Average Testing MSE Loss: {:.3f} \\n Average Testing RMSE Loss: {:.3f} \\n Average Testing Shared Loss: {:.3f} \\n Average Testing Shared RMSE Loss: {:.3f} \\n Average Testing R2 Loss: {:.3f} \\n Average Testing Independence Loss: {:.3f} \\n\".format(cv_dna_recon_loss, cv_gene_recon_loss, cv_dna_kl_loss, cv_gene_kl_loss, cv_mse_loss, cv_rmse_loss, cv_shared_loss,cv_shared_rmse_loss, cv_r2_loss, cv_ind_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17159724235534668,\n",
       " 0.4582054018974304,\n",
       " 0.5908139050006866,\n",
       " 0.7051718235015869,\n",
       " 0.8294875621795654,\n",
       " 0.8394534885883331,\n",
       " 0.8368826806545258,\n",
       " 0.8399815261363983,\n",
       " 0.8721919655799866,\n",
       " 0.8772014677524567]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_test['Test R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
