{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import matplotlib\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "matplotlib.style.use('ggplot')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightning.pytorch as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, level_2, level_3, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.enc_fc1 = nn.Sequential(\n",
    "                        nn.Linear(input_size, level_2),\n",
    "                        nn.BatchNorm1d(level_2),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.enc_fc2 = nn.Sequential(\n",
    "                        nn.Linear(level_2, level_3),\n",
    "                        nn.BatchNorm1d(level_3),\n",
    "                        nn.ReLU())\n",
    "\n",
    "        self.enc_fc3_mean = nn.Sequential(\n",
    "                    nn.Linear(level_3, latent_dim),\n",
    "                    nn.BatchNorm1d(latent_dim))\n",
    "        \n",
    "        self.enc_fc3_log_var = nn.Sequential(\n",
    "                    nn.Linear(level_3, latent_dim),\n",
    "                    nn.BatchNorm1d(latent_dim))\n",
    "        \n",
    "        \n",
    "        # Decoder layers\n",
    "        self.dec_fc3 = nn.Sequential(\n",
    "                        nn.Linear(latent_dim, level_3),\n",
    "                        nn.BatchNorm1d(level_3),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.dec_fc2 = nn.Sequential(\n",
    "                        nn.Linear(level_3, level_2),\n",
    "                        nn.BatchNorm1d(level_2),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.dec_fc1 = nn.Sequential(\n",
    "                    nn.Linear(level_2, input_size),\n",
    "                    nn.BatchNorm1d(input_size),\n",
    "                    nn.Sigmoid())\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        l2_layer = self.enc_fc1(x)\n",
    "        l3_layer = self.enc_fc2(l2_layer)\n",
    "        \n",
    "        mu = self.enc_fc3_mean(l3_layer)\n",
    "        logvar = self.enc_fc3_log_var(l3_layer)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        l3_layer = self.dec_fc3(z)\n",
    "        l2_layer = self.dec_fc2(l3_layer)\n",
    "        x_hat = self.dec_fc1(l2_layer)\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mu, logvar, z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x_hat, x, mean, log_var): # recon loss and kld loss\n",
    "        bce = torch.nn.functional.binary_cross_entropy(x_hat, x, reduction = 'sum')\n",
    "        kld = 0.5 * torch.sum(log_var.exp() + mean.pow(2) - 1 - log_var)\n",
    "        loss = kld + bce\n",
    "        return loss, kld, bce"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitVAE(pl.LightningModule):\n",
    "    def __init__ (self, VAE):\n",
    "        super().__init__()\n",
    "        self.VAE = VAE\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_hat, mu, logvar, z = self.VAE(x)\n",
    "        loss, kld, bce = loss_function(x_hat, x, mu, logvar)\n",
    "        self.log('Total Train Loss', loss/len(x), on_epoch=True)\n",
    "        self.log('KL Train Loss', kld/len(x), on_epoch=True)\n",
    "        self.log('Recon TRain Loss', bce/len(x), on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_hat, mu, logvar, z = self.VAE(x)\n",
    "        return mu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "dna_meth = pd.read_csv('../datasets_transpose_csv/dna_meth_transpose.csv')\n",
    "dna_meth = dna_meth.drop(columns=['Unnamed: 0'])\n",
    "cell_lines = dna_meth['CpG_sites_hg19']\n",
    "dna_meth = dna_meth.drop(columns=['CpG_sites_hg19'])\n",
    "dna_meth = dna_meth.drop(columns=['Unnamed: 81039'])\n",
    "dna_meth_np = dna_meth.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dna_meth_full = torch.Tensor(dna_meth_np)\n",
    "full_loader = DataLoader(dna_meth_full,batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25153fce130>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "torch.manual_seed(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | VAE  | VAE  | 342 M \n",
      "------------------------------\n",
      "342 M     Trainable params\n",
      "0         Non-trainable params\n",
      "342 M     Total params\n",
      "1,371.800 Total estimated model params size (MB)\n",
      "c:\\Users\\mdzak\\anaconda3\\envs\\fyp\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (27) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 27/27 [00:06<00:00,  4.34it/s, v_num=4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 27/27 [00:06<00:00,  4.34it/s, v_num=4]\n"
     ]
    }
   ],
   "source": [
    "input_size = 81037 #dimension of gene expressions\n",
    "level_2 = 2048\n",
    "level_3 = 1500\n",
    "latent_dim = 1024 # target latent size\n",
    "# model\n",
    "VAE_model = LitVAE(VAE(input_size, level_2, level_3, latent_dim))\n",
    "\n",
    "# train model\n",
    "logger = TensorBoardLogger('tb_logs', name= 'vae_dna_meth')\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", max_epochs=100, enable_checkpointing= False, logger=logger)\n",
    "trainer.fit(model=VAE_model, train_dataloaders=full_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 27/27 [00:00<00:00, 64.90it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PREDICT Profiler Report\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  Action                                             \t|  Mean duration (s)\t|  Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  Total                                              \t|  -              \t|  82072          \t|  872.38         \t|  100 %          \t|\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  run_training_epoch                                 \t|  6.2807         \t|  100            \t|  628.07         \t|  71.996         \t|\n",
      "|  run_training_batch                                 \t|  0.11025        \t|  2700           \t|  297.67         \t|  34.122         \t|\n",
      "|  [LightningModule]LitVAE.optimizer_step             \t|  0.11021        \t|  2700           \t|  297.58         \t|  34.111         \t|\n",
      "|  [_TrainingEpochLoop].train_dataloader_next         \t|  0.092072       \t|  2700           \t|  248.6          \t|  28.496         \t|\n",
      "|  [Strategy]SingleDeviceStrategy.backward            \t|  0.039253       \t|  2700           \t|  105.98         \t|  12.149         \t|\n",
      "|  [Strategy]SingleDeviceStrategy.training_step       \t|  0.00396        \t|  2700           \t|  10.692         \t|  1.2256         \t|\n",
      "|  [_PredictionLoop].predict_next                     \t|  0.12037        \t|  27             \t|  3.25           \t|  0.37255        \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_batch_end       \t|  0.0010096      \t|  2700           \t|  2.726          \t|  0.31248        \t|\n",
      "|  [LightningModule]LitVAE.optimizer_zero_grad        \t|  0.00015741     \t|  2700           \t|  0.425          \t|  0.048718       \t|\n",
      "|  [LightningModule]LitVAE.configure_gradient_clipping\t|  6.963e-05      \t|  2700           \t|  0.188          \t|  0.02155        \t|\n",
      "|  [Strategy]SingleDeviceStrategy.batch_to_device     \t|  3.4103e-05     \t|  2727           \t|  0.093          \t|  0.010661       \t|\n",
      "|  [Strategy]SingleDeviceStrategy.predict_step        \t|  0.0023333      \t|  27             \t|  0.063          \t|  0.0072217      \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_epoch_start     \t|  0.00062        \t|  100            \t|  0.062          \t|  0.007107       \t|\n",
      "|  [LightningModule]LitVAE.transfer_batch_to_device   \t|  2.2736e-05     \t|  2727           \t|  0.062          \t|  0.007107       \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_epoch_end       \t|  0.00047        \t|  100            \t|  0.047          \t|  0.0053876      \t|\n",
      "|  [Callback]ModelSummary.on_before_zero_grad         \t|  1.1481e-05     \t|  2700           \t|  0.031          \t|  0.0035535      \t|\n",
      "|  [LightningModule]LitVAE.on_after_batch_transfer    \t|  5.8673e-06     \t|  2727           \t|  0.016          \t|  0.0018341      \t|\n",
      "|  [LightningModule]LitVAE.on_before_zero_grad        \t|  5.9259e-06     \t|  2700           \t|  0.016          \t|  0.0018341      \t|\n",
      "|  [Callback]TQDMProgressBar.on_after_backward        \t|  5.9259e-06     \t|  2700           \t|  0.016          \t|  0.0018341      \t|\n",
      "|  [Callback]ModelSummary.on_train_batch_end          \t|  5.9259e-06     \t|  2700           \t|  0.016          \t|  0.0018341      \t|\n",
      "|  [Callback]TQDMProgressBar.on_predict_batch_end     \t|  0.00059259     \t|  27             \t|  0.016          \t|  0.0018341      \t|\n",
      "|  [Callback]TQDMProgressBar.on_before_optimizer_step \t|  5.5556e-06     \t|  2700           \t|  0.015          \t|  0.0017194      \t|\n",
      "|  [LightningModule]LitVAE.on_before_optimizer_step   \t|  5.5556e-06     \t|  2700           \t|  0.015          \t|  0.0017194      \t|\n",
      "|  [LightningModule]LitVAE.configure_callbacks        \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.prepare_data               \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.setup                    \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.setup                       \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.setup                      \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.configure_sharded_model    \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.configure_optimizers       \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_fit_start             \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_fit_start                \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.on_fit_start               \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_start           \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_train_start              \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.on_train_start             \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_start      \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_train_epoch_start        \t|  0.0            \t|  100            \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.on_train_epoch_start       \t|  0.0            \t|  100            \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.on_before_batch_transfer   \t|  0.0            \t|  2727           \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_batch_start     \t|  0.0            \t|  2700           \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_train_batch_start        \t|  0.0            \t|  2700           \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.on_train_batch_start       \t|  0.0            \t|  2700           \t|  0.0            \t|  0.0            \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_batch_start\t|  0.0            \t|  2700           \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_before_zero_grad      \t|  0.0            \t|  2700           \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_before_backward       \t|  0.0            \t|  2700           \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_before_backward          \t|  0.0            \t|  2700           \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.on_before_backward         \t|  0.0            \t|  2700           \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_after_backward           \t|  0.0            \t|  2700           \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.on_after_backward          \t|  0.0            \t|  2700           \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_before_optimizer_step    \t|  0.0            \t|  2700           \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.on_train_batch_end         \t|  0.0            \t|  2700           \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_train_epoch_end          \t|  0.0            \t|  100            \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.on_train_epoch_end         \t|  0.0            \t|  100            \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_end             \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_train_end                \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.on_train_end               \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_end        \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_fit_end               \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_fit_end                  \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.on_fit_end                 \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.teardown                 \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.teardown                    \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.teardown                   \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.on_predict_model_eval      \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_predict_start         \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_predict_start            \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.on_predict_start           \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_predict_start    \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_predict_epoch_start   \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_predict_epoch_start      \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.on_predict_epoch_start     \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_predict_batch_start   \t|  0.0            \t|  27             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_predict_batch_start      \t|  0.0            \t|  27             \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.on_predict_batch_start     \t|  0.0            \t|  27             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_predict_batch_end        \t|  0.0            \t|  27             \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.on_predict_batch_end       \t|  0.0            \t|  27             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_predict_epoch_end     \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_predict_epoch_end        \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.on_predict_epoch_end       \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_predict_end           \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_predict_end              \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]LitVAE.on_predict_end             \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_predict_end      \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.predict(VAE_model, full_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    for j in range(pred[0].shape[0]):\n",
    "        try:\n",
    "            pred_list.append(pred[i][j])\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a list of tensors named \"tensor_list\"\n",
    "pred_list_np = [tensor.numpy() for tensor in pred_list]\n",
    "\n",
    "# Convert the list of NumPy arrays to a single NumPy array\n",
    "pred_np = np.array(pred_list_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'test.tsv'\n",
    "pred_df = pd.DataFrame(pred_np)\n",
    "pred_df.to_csv(file_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.429570</td>\n",
       "      <td>-1.909910</td>\n",
       "      <td>-1.472437</td>\n",
       "      <td>0.748720</td>\n",
       "      <td>-0.016270</td>\n",
       "      <td>0.655753</td>\n",
       "      <td>-0.239222</td>\n",
       "      <td>0.928533</td>\n",
       "      <td>-1.746679</td>\n",
       "      <td>-1.498429</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.167183</td>\n",
       "      <td>-0.828607</td>\n",
       "      <td>1.255535</td>\n",
       "      <td>0.486317</td>\n",
       "      <td>-0.253607</td>\n",
       "      <td>1.616354</td>\n",
       "      <td>-0.522952</td>\n",
       "      <td>-1.643377</td>\n",
       "      <td>1.878682</td>\n",
       "      <td>-1.064827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.799768</td>\n",
       "      <td>-1.317464</td>\n",
       "      <td>-0.283014</td>\n",
       "      <td>-0.348736</td>\n",
       "      <td>0.054551</td>\n",
       "      <td>-0.999007</td>\n",
       "      <td>0.488884</td>\n",
       "      <td>-0.439297</td>\n",
       "      <td>-1.248220</td>\n",
       "      <td>0.057584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.464248</td>\n",
       "      <td>-1.455493</td>\n",
       "      <td>0.738102</td>\n",
       "      <td>-0.426222</td>\n",
       "      <td>0.916154</td>\n",
       "      <td>0.237763</td>\n",
       "      <td>0.262881</td>\n",
       "      <td>-0.647636</td>\n",
       "      <td>-0.563344</td>\n",
       "      <td>-0.511420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.063261</td>\n",
       "      <td>0.137814</td>\n",
       "      <td>0.425855</td>\n",
       "      <td>1.471842</td>\n",
       "      <td>1.735726</td>\n",
       "      <td>-0.114720</td>\n",
       "      <td>0.246615</td>\n",
       "      <td>-1.031568</td>\n",
       "      <td>1.238343</td>\n",
       "      <td>-0.124922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169023</td>\n",
       "      <td>0.792818</td>\n",
       "      <td>-0.337984</td>\n",
       "      <td>1.076447</td>\n",
       "      <td>0.334452</td>\n",
       "      <td>-0.209356</td>\n",
       "      <td>-0.257603</td>\n",
       "      <td>0.988091</td>\n",
       "      <td>-0.565004</td>\n",
       "      <td>0.755501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.134225</td>\n",
       "      <td>1.318798</td>\n",
       "      <td>0.302894</td>\n",
       "      <td>0.531689</td>\n",
       "      <td>-0.053797</td>\n",
       "      <td>-1.038170</td>\n",
       "      <td>0.176276</td>\n",
       "      <td>-0.443220</td>\n",
       "      <td>0.889723</td>\n",
       "      <td>-0.112907</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.365432</td>\n",
       "      <td>0.479045</td>\n",
       "      <td>-0.164296</td>\n",
       "      <td>0.110523</td>\n",
       "      <td>-0.554673</td>\n",
       "      <td>-0.434859</td>\n",
       "      <td>0.692086</td>\n",
       "      <td>0.668916</td>\n",
       "      <td>-0.138584</td>\n",
       "      <td>0.248618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.029383</td>\n",
       "      <td>0.827876</td>\n",
       "      <td>0.367270</td>\n",
       "      <td>0.795097</td>\n",
       "      <td>0.525147</td>\n",
       "      <td>-0.352542</td>\n",
       "      <td>0.858132</td>\n",
       "      <td>-0.536660</td>\n",
       "      <td>1.324821</td>\n",
       "      <td>0.810718</td>\n",
       "      <td>...</td>\n",
       "      <td>2.028498</td>\n",
       "      <td>0.786475</td>\n",
       "      <td>-0.896838</td>\n",
       "      <td>0.305146</td>\n",
       "      <td>0.597017</td>\n",
       "      <td>0.786401</td>\n",
       "      <td>-1.115265</td>\n",
       "      <td>0.561232</td>\n",
       "      <td>-0.741169</td>\n",
       "      <td>0.706336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>0.406915</td>\n",
       "      <td>0.407979</td>\n",
       "      <td>0.790799</td>\n",
       "      <td>-0.065704</td>\n",
       "      <td>-0.178693</td>\n",
       "      <td>0.133986</td>\n",
       "      <td>-0.294570</td>\n",
       "      <td>0.907708</td>\n",
       "      <td>-0.532263</td>\n",
       "      <td>0.511409</td>\n",
       "      <td>...</td>\n",
       "      <td>1.018077</td>\n",
       "      <td>0.388512</td>\n",
       "      <td>0.163653</td>\n",
       "      <td>-0.042266</td>\n",
       "      <td>0.109107</td>\n",
       "      <td>-1.311447</td>\n",
       "      <td>0.074531</td>\n",
       "      <td>-0.708374</td>\n",
       "      <td>-0.111912</td>\n",
       "      <td>0.241496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>-1.108940</td>\n",
       "      <td>1.210906</td>\n",
       "      <td>1.041628</td>\n",
       "      <td>-0.037763</td>\n",
       "      <td>-0.117001</td>\n",
       "      <td>0.395379</td>\n",
       "      <td>-0.342543</td>\n",
       "      <td>0.380366</td>\n",
       "      <td>0.976832</td>\n",
       "      <td>0.584943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.453464</td>\n",
       "      <td>0.929581</td>\n",
       "      <td>-0.500538</td>\n",
       "      <td>0.133473</td>\n",
       "      <td>-0.978117</td>\n",
       "      <td>0.073899</td>\n",
       "      <td>0.300841</td>\n",
       "      <td>0.970426</td>\n",
       "      <td>-0.090345</td>\n",
       "      <td>0.103342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>-0.353198</td>\n",
       "      <td>0.095412</td>\n",
       "      <td>0.901895</td>\n",
       "      <td>0.286198</td>\n",
       "      <td>0.049067</td>\n",
       "      <td>0.979891</td>\n",
       "      <td>-0.113612</td>\n",
       "      <td>0.622324</td>\n",
       "      <td>0.131119</td>\n",
       "      <td>0.881184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595284</td>\n",
       "      <td>0.551816</td>\n",
       "      <td>-0.225171</td>\n",
       "      <td>-0.099614</td>\n",
       "      <td>0.022435</td>\n",
       "      <td>-0.195926</td>\n",
       "      <td>0.816269</td>\n",
       "      <td>0.122387</td>\n",
       "      <td>-0.223316</td>\n",
       "      <td>-0.330512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>-0.852346</td>\n",
       "      <td>0.393635</td>\n",
       "      <td>0.945212</td>\n",
       "      <td>0.263071</td>\n",
       "      <td>-0.075022</td>\n",
       "      <td>1.196889</td>\n",
       "      <td>-0.259242</td>\n",
       "      <td>0.906962</td>\n",
       "      <td>0.612942</td>\n",
       "      <td>0.578142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169133</td>\n",
       "      <td>0.684954</td>\n",
       "      <td>-0.239488</td>\n",
       "      <td>0.127601</td>\n",
       "      <td>-0.640551</td>\n",
       "      <td>0.255922</td>\n",
       "      <td>0.485135</td>\n",
       "      <td>0.284296</td>\n",
       "      <td>0.089397</td>\n",
       "      <td>-0.179119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>0.279059</td>\n",
       "      <td>-0.054011</td>\n",
       "      <td>0.832893</td>\n",
       "      <td>0.882452</td>\n",
       "      <td>0.647609</td>\n",
       "      <td>0.551013</td>\n",
       "      <td>0.734120</td>\n",
       "      <td>0.316017</td>\n",
       "      <td>-0.776379</td>\n",
       "      <td>0.406365</td>\n",
       "      <td>...</td>\n",
       "      <td>1.318971</td>\n",
       "      <td>0.776636</td>\n",
       "      <td>0.043477</td>\n",
       "      <td>0.564294</td>\n",
       "      <td>0.378724</td>\n",
       "      <td>-1.129252</td>\n",
       "      <td>1.336940</td>\n",
       "      <td>-0.207831</td>\n",
       "      <td>-0.356519</td>\n",
       "      <td>-0.713631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>843 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0    0.429570 -1.909910 -1.472437  0.748720 -0.016270  0.655753 -0.239222   \n",
       "1    0.799768 -1.317464 -0.283014 -0.348736  0.054551 -0.999007  0.488884   \n",
       "2   -0.063261  0.137814  0.425855  1.471842  1.735726 -0.114720  0.246615   \n",
       "3   -0.134225  1.318798  0.302894  0.531689 -0.053797 -1.038170  0.176276   \n",
       "4    0.029383  0.827876  0.367270  0.795097  0.525147 -0.352542  0.858132   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "838  0.406915  0.407979  0.790799 -0.065704 -0.178693  0.133986 -0.294570   \n",
       "839 -1.108940  1.210906  1.041628 -0.037763 -0.117001  0.395379 -0.342543   \n",
       "840 -0.353198  0.095412  0.901895  0.286198  0.049067  0.979891 -0.113612   \n",
       "841 -0.852346  0.393635  0.945212  0.263071 -0.075022  1.196889 -0.259242   \n",
       "842  0.279059 -0.054011  0.832893  0.882452  0.647609  0.551013  0.734120   \n",
       "\n",
       "         7         8         9     ...      1014      1015      1016  \\\n",
       "0    0.928533 -1.746679 -1.498429  ... -1.167183 -0.828607  1.255535   \n",
       "1   -0.439297 -1.248220  0.057584  ... -0.464248 -1.455493  0.738102   \n",
       "2   -1.031568  1.238343 -0.124922  ...  0.169023  0.792818 -0.337984   \n",
       "3   -0.443220  0.889723 -0.112907  ... -0.365432  0.479045 -0.164296   \n",
       "4   -0.536660  1.324821  0.810718  ...  2.028498  0.786475 -0.896838   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "838  0.907708 -0.532263  0.511409  ...  1.018077  0.388512  0.163653   \n",
       "839  0.380366  0.976832  0.584943  ...  0.453464  0.929581 -0.500538   \n",
       "840  0.622324  0.131119  0.881184  ...  0.595284  0.551816 -0.225171   \n",
       "841  0.906962  0.612942  0.578142  ...  0.169133  0.684954 -0.239488   \n",
       "842  0.316017 -0.776379  0.406365  ...  1.318971  0.776636  0.043477   \n",
       "\n",
       "         1017      1018      1019      1020      1021      1022      1023  \n",
       "0    0.486317 -0.253607  1.616354 -0.522952 -1.643377  1.878682 -1.064827  \n",
       "1   -0.426222  0.916154  0.237763  0.262881 -0.647636 -0.563344 -0.511420  \n",
       "2    1.076447  0.334452 -0.209356 -0.257603  0.988091 -0.565004  0.755501  \n",
       "3    0.110523 -0.554673 -0.434859  0.692086  0.668916 -0.138584  0.248618  \n",
       "4    0.305146  0.597017  0.786401 -1.115265  0.561232 -0.741169  0.706336  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "838 -0.042266  0.109107 -1.311447  0.074531 -0.708374 -0.111912  0.241496  \n",
       "839  0.133473 -0.978117  0.073899  0.300841  0.970426 -0.090345  0.103342  \n",
       "840 -0.099614  0.022435 -0.195926  0.816269  0.122387 -0.223316 -0.330512  \n",
       "841  0.127601 -0.640551  0.255922  0.485135  0.284296  0.089397 -0.179119  \n",
       "842  0.564294  0.378724 -1.129252  1.336940 -0.207831 -0.356519 -0.713631  \n",
       "\n",
       "[843 rows x 1024 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
