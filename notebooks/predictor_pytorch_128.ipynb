{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mdzak\\anaconda3\\envs\\fyp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import matplotlib\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "matplotlib.style.use('ggplot')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "import copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import CSV and merge with drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dim = 128\n",
    "dimension = pd.read_csv('../results/results' + str(selected_dim) + 'D_latent_space_dna_meth.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_line_name = pd.read_csv('../results_clean/cell_line_name_dna_meth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdsc_drug = pd.read_csv('../results_clean/gdsc_drug_nodash.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "gdsc_drug.drop(columns=['Unnamed: 0'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension['CELL_LINE_NAME'] = cell_line_name['CELL_LINE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>CELL_LINE_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.257649</td>\n",
       "      <td>0.367905</td>\n",
       "      <td>-0.316169</td>\n",
       "      <td>-2.450200</td>\n",
       "      <td>0.896207</td>\n",
       "      <td>-0.306396</td>\n",
       "      <td>0.040796</td>\n",
       "      <td>-1.562901</td>\n",
       "      <td>-0.191658</td>\n",
       "      <td>-0.303967</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390880</td>\n",
       "      <td>-1.762449</td>\n",
       "      <td>0.996050</td>\n",
       "      <td>1.685361</td>\n",
       "      <td>0.038959</td>\n",
       "      <td>-1.332917</td>\n",
       "      <td>2.574497</td>\n",
       "      <td>0.663017</td>\n",
       "      <td>-0.766124</td>\n",
       "      <td>dms53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.485408</td>\n",
       "      <td>0.689928</td>\n",
       "      <td>0.381287</td>\n",
       "      <td>-0.837483</td>\n",
       "      <td>0.192373</td>\n",
       "      <td>-0.574186</td>\n",
       "      <td>-0.865449</td>\n",
       "      <td>0.917400</td>\n",
       "      <td>0.749186</td>\n",
       "      <td>-0.295143</td>\n",
       "      <td>...</td>\n",
       "      <td>1.682005</td>\n",
       "      <td>-0.232119</td>\n",
       "      <td>0.503149</td>\n",
       "      <td>0.706627</td>\n",
       "      <td>0.159365</td>\n",
       "      <td>1.148933</td>\n",
       "      <td>3.465427</td>\n",
       "      <td>-0.159691</td>\n",
       "      <td>0.280858</td>\n",
       "      <td>sw1116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.310404</td>\n",
       "      <td>-0.946017</td>\n",
       "      <td>-0.861003</td>\n",
       "      <td>1.298446</td>\n",
       "      <td>0.304596</td>\n",
       "      <td>-1.072315</td>\n",
       "      <td>0.469882</td>\n",
       "      <td>0.935943</td>\n",
       "      <td>-0.021631</td>\n",
       "      <td>-0.012280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354510</td>\n",
       "      <td>1.536413</td>\n",
       "      <td>-0.982758</td>\n",
       "      <td>-0.017836</td>\n",
       "      <td>-0.048801</td>\n",
       "      <td>-0.800349</td>\n",
       "      <td>-2.850868</td>\n",
       "      <td>-0.159700</td>\n",
       "      <td>1.119022</td>\n",
       "      <td>p3hr1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.196453</td>\n",
       "      <td>0.840015</td>\n",
       "      <td>0.295631</td>\n",
       "      <td>0.277697</td>\n",
       "      <td>-0.336109</td>\n",
       "      <td>1.279360</td>\n",
       "      <td>-1.097295</td>\n",
       "      <td>0.559683</td>\n",
       "      <td>-0.346177</td>\n",
       "      <td>0.300485</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.126680</td>\n",
       "      <td>-0.166518</td>\n",
       "      <td>0.273345</td>\n",
       "      <td>0.135879</td>\n",
       "      <td>0.692845</td>\n",
       "      <td>1.114507</td>\n",
       "      <td>-2.869138</td>\n",
       "      <td>0.481234</td>\n",
       "      <td>2.813995</td>\n",
       "      <td>hut78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.243231</td>\n",
       "      <td>0.847392</td>\n",
       "      <td>0.984537</td>\n",
       "      <td>0.481817</td>\n",
       "      <td>-0.105398</td>\n",
       "      <td>0.994923</td>\n",
       "      <td>-0.443648</td>\n",
       "      <td>-0.661769</td>\n",
       "      <td>0.466033</td>\n",
       "      <td>-0.609941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.646114</td>\n",
       "      <td>0.628980</td>\n",
       "      <td>0.254079</td>\n",
       "      <td>0.368022</td>\n",
       "      <td>0.529851</td>\n",
       "      <td>-0.348529</td>\n",
       "      <td>-2.507193</td>\n",
       "      <td>-0.513243</td>\n",
       "      <td>2.655861</td>\n",
       "      <td>umuc3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>-0.186357</td>\n",
       "      <td>-0.515294</td>\n",
       "      <td>-1.050231</td>\n",
       "      <td>-0.178011</td>\n",
       "      <td>-1.592714</td>\n",
       "      <td>-0.032956</td>\n",
       "      <td>-0.363624</td>\n",
       "      <td>-0.002767</td>\n",
       "      <td>-0.295767</td>\n",
       "      <td>-0.544842</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.786941</td>\n",
       "      <td>-0.845840</td>\n",
       "      <td>0.441206</td>\n",
       "      <td>-0.669843</td>\n",
       "      <td>-0.316336</td>\n",
       "      <td>-0.003037</td>\n",
       "      <td>1.542336</td>\n",
       "      <td>-0.648602</td>\n",
       "      <td>1.377838</td>\n",
       "      <td>uo31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>0.679910</td>\n",
       "      <td>-0.143883</td>\n",
       "      <td>-0.445034</td>\n",
       "      <td>0.461096</td>\n",
       "      <td>0.257221</td>\n",
       "      <td>-0.373053</td>\n",
       "      <td>-1.185102</td>\n",
       "      <td>0.578107</td>\n",
       "      <td>-0.662319</td>\n",
       "      <td>-0.305469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.243613</td>\n",
       "      <td>0.393775</td>\n",
       "      <td>0.560154</td>\n",
       "      <td>0.698871</td>\n",
       "      <td>-0.213210</td>\n",
       "      <td>-1.336929</td>\n",
       "      <td>-2.107328</td>\n",
       "      <td>-0.230628</td>\n",
       "      <td>-0.450253</td>\n",
       "      <td>sf268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>-0.424310</td>\n",
       "      <td>0.506764</td>\n",
       "      <td>0.655425</td>\n",
       "      <td>0.905986</td>\n",
       "      <td>-0.731646</td>\n",
       "      <td>-1.261937</td>\n",
       "      <td>0.042817</td>\n",
       "      <td>0.535406</td>\n",
       "      <td>-0.291645</td>\n",
       "      <td>-0.384929</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.449366</td>\n",
       "      <td>0.820775</td>\n",
       "      <td>0.203442</td>\n",
       "      <td>0.337288</td>\n",
       "      <td>-0.162978</td>\n",
       "      <td>-0.244286</td>\n",
       "      <td>-0.824387</td>\n",
       "      <td>-0.559017</td>\n",
       "      <td>-2.426988</td>\n",
       "      <td>sf539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>0.704209</td>\n",
       "      <td>0.452495</td>\n",
       "      <td>-0.399363</td>\n",
       "      <td>0.749073</td>\n",
       "      <td>-1.067308</td>\n",
       "      <td>-1.602799</td>\n",
       "      <td>0.733508</td>\n",
       "      <td>0.263094</td>\n",
       "      <td>0.438836</td>\n",
       "      <td>-0.295644</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.147306</td>\n",
       "      <td>0.318956</td>\n",
       "      <td>0.455772</td>\n",
       "      <td>0.669230</td>\n",
       "      <td>0.437862</td>\n",
       "      <td>0.807621</td>\n",
       "      <td>-1.845123</td>\n",
       "      <td>0.092654</td>\n",
       "      <td>-1.348003</td>\n",
       "      <td>snb75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>-0.966875</td>\n",
       "      <td>0.383040</td>\n",
       "      <td>-0.215750</td>\n",
       "      <td>0.743864</td>\n",
       "      <td>-0.267682</td>\n",
       "      <td>0.622732</td>\n",
       "      <td>0.825473</td>\n",
       "      <td>-0.279673</td>\n",
       "      <td>-0.128047</td>\n",
       "      <td>0.609692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.735449</td>\n",
       "      <td>-0.031310</td>\n",
       "      <td>0.990979</td>\n",
       "      <td>0.738363</td>\n",
       "      <td>-0.358515</td>\n",
       "      <td>-0.175460</td>\n",
       "      <td>-0.031960</td>\n",
       "      <td>-0.360483</td>\n",
       "      <td>-2.192188</td>\n",
       "      <td>hop92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>843 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.257649  0.367905 -0.316169 -2.450200  0.896207 -0.306396  0.040796   \n",
       "1   -0.485408  0.689928  0.381287 -0.837483  0.192373 -0.574186 -0.865449   \n",
       "2    0.310404 -0.946017 -0.861003  1.298446  0.304596 -1.072315  0.469882   \n",
       "3    0.196453  0.840015  0.295631  0.277697 -0.336109  1.279360 -1.097295   \n",
       "4   -0.243231  0.847392  0.984537  0.481817 -0.105398  0.994923 -0.443648   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "838 -0.186357 -0.515294 -1.050231 -0.178011 -1.592714 -0.032956 -0.363624   \n",
       "839  0.679910 -0.143883 -0.445034  0.461096  0.257221 -0.373053 -1.185102   \n",
       "840 -0.424310  0.506764  0.655425  0.905986 -0.731646 -1.261937  0.042817   \n",
       "841  0.704209  0.452495 -0.399363  0.749073 -1.067308 -1.602799  0.733508   \n",
       "842 -0.966875  0.383040 -0.215750  0.743864 -0.267682  0.622732  0.825473   \n",
       "\n",
       "            7         8         9  ...       119       120       121  \\\n",
       "0   -1.562901 -0.191658 -0.303967  ... -0.390880 -1.762449  0.996050   \n",
       "1    0.917400  0.749186 -0.295143  ...  1.682005 -0.232119  0.503149   \n",
       "2    0.935943 -0.021631 -0.012280  ...  0.354510  1.536413 -0.982758   \n",
       "3    0.559683 -0.346177  0.300485  ... -1.126680 -0.166518  0.273345   \n",
       "4   -0.661769  0.466033 -0.609941  ...  0.646114  0.628980  0.254079   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "838 -0.002767 -0.295767 -0.544842  ... -1.786941 -0.845840  0.441206   \n",
       "839  0.578107 -0.662319 -0.305469  ... -0.243613  0.393775  0.560154   \n",
       "840  0.535406 -0.291645 -0.384929  ... -0.449366  0.820775  0.203442   \n",
       "841  0.263094  0.438836 -0.295644  ... -1.147306  0.318956  0.455772   \n",
       "842 -0.279673 -0.128047  0.609692  ...  0.735449 -0.031310  0.990979   \n",
       "\n",
       "          122       123       124       125       126       127  \\\n",
       "0    1.685361  0.038959 -1.332917  2.574497  0.663017 -0.766124   \n",
       "1    0.706627  0.159365  1.148933  3.465427 -0.159691  0.280858   \n",
       "2   -0.017836 -0.048801 -0.800349 -2.850868 -0.159700  1.119022   \n",
       "3    0.135879  0.692845  1.114507 -2.869138  0.481234  2.813995   \n",
       "4    0.368022  0.529851 -0.348529 -2.507193 -0.513243  2.655861   \n",
       "..        ...       ...       ...       ...       ...       ...   \n",
       "838 -0.669843 -0.316336 -0.003037  1.542336 -0.648602  1.377838   \n",
       "839  0.698871 -0.213210 -1.336929 -2.107328 -0.230628 -0.450253   \n",
       "840  0.337288 -0.162978 -0.244286 -0.824387 -0.559017 -2.426988   \n",
       "841  0.669230  0.437862  0.807621 -1.845123  0.092654 -1.348003   \n",
       "842  0.738363 -0.358515 -0.175460 -0.031960 -0.360483 -2.192188   \n",
       "\n",
       "     CELL_LINE_NAME  \n",
       "0             dms53  \n",
       "1            sw1116  \n",
       "2             p3hr1  \n",
       "3             hut78  \n",
       "4             umuc3  \n",
       "..              ...  \n",
       "838            uo31  \n",
       "839           sf268  \n",
       "840           sf539  \n",
       "841           snb75  \n",
       "842           hop92  \n",
       "\n",
       "[843 rows x 129 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LN_IC50</th>\n",
       "      <th>CELL_LINE_NAME</th>\n",
       "      <th>DRUG_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.462148</td>\n",
       "      <td>pfsk1</td>\n",
       "      <td>Camptothecin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.869447</td>\n",
       "      <td>a673</td>\n",
       "      <td>Camptothecin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.360684</td>\n",
       "      <td>es5</td>\n",
       "      <td>Camptothecin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.045014</td>\n",
       "      <td>es7</td>\n",
       "      <td>Camptothecin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.741620</td>\n",
       "      <td>ew11</td>\n",
       "      <td>Camptothecin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242031</th>\n",
       "      <td>10.134495</td>\n",
       "      <td>snu175</td>\n",
       "      <td>N-acetyl cysteine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242032</th>\n",
       "      <td>8.575555</td>\n",
       "      <td>snu407</td>\n",
       "      <td>N-acetyl cysteine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242033</th>\n",
       "      <td>10.520666</td>\n",
       "      <td>snu61</td>\n",
       "      <td>N-acetyl cysteine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242034</th>\n",
       "      <td>10.701430</td>\n",
       "      <td>snuc5</td>\n",
       "      <td>N-acetyl cysteine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242035</th>\n",
       "      <td>10.038769</td>\n",
       "      <td>difi</td>\n",
       "      <td>N-acetyl cysteine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242036 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          LN_IC50 CELL_LINE_NAME          DRUG_NAME\n",
       "0       -1.462148          pfsk1       Camptothecin\n",
       "1       -4.869447           a673       Camptothecin\n",
       "2       -3.360684            es5       Camptothecin\n",
       "3       -5.045014            es7       Camptothecin\n",
       "4       -3.741620           ew11       Camptothecin\n",
       "...           ...            ...                ...\n",
       "242031  10.134495         snu175  N-acetyl cysteine\n",
       "242032   8.575555         snu407  N-acetyl cysteine\n",
       "242033  10.520666          snu61  N-acetyl cysteine\n",
       "242034  10.701430          snuc5  N-acetyl cysteine\n",
       "242035  10.038769           difi  N-acetyl cysteine\n",
       "\n",
       "[242036 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdsc_drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_w_drug = pd.merge(dimension, gdsc_drug, on='CELL_LINE_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>CELL_LINE_NAME</th>\n",
       "      <th>LN_IC50</th>\n",
       "      <th>DRUG_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.257649</td>\n",
       "      <td>0.367905</td>\n",
       "      <td>-0.316169</td>\n",
       "      <td>-2.450200</td>\n",
       "      <td>0.896207</td>\n",
       "      <td>-0.306396</td>\n",
       "      <td>0.040796</td>\n",
       "      <td>-1.562901</td>\n",
       "      <td>-0.191658</td>\n",
       "      <td>-0.303967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996050</td>\n",
       "      <td>1.685361</td>\n",
       "      <td>0.038959</td>\n",
       "      <td>-1.332917</td>\n",
       "      <td>2.574497</td>\n",
       "      <td>0.663017</td>\n",
       "      <td>-0.766124</td>\n",
       "      <td>dms53</td>\n",
       "      <td>-3.193970</td>\n",
       "      <td>Camptothecin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.257649</td>\n",
       "      <td>0.367905</td>\n",
       "      <td>-0.316169</td>\n",
       "      <td>-2.450200</td>\n",
       "      <td>0.896207</td>\n",
       "      <td>-0.306396</td>\n",
       "      <td>0.040796</td>\n",
       "      <td>-1.562901</td>\n",
       "      <td>-0.191658</td>\n",
       "      <td>-0.303967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996050</td>\n",
       "      <td>1.685361</td>\n",
       "      <td>0.038959</td>\n",
       "      <td>-1.332917</td>\n",
       "      <td>2.574497</td>\n",
       "      <td>0.663017</td>\n",
       "      <td>-0.766124</td>\n",
       "      <td>dms53</td>\n",
       "      <td>0.868145</td>\n",
       "      <td>Vinblastine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.257649</td>\n",
       "      <td>0.367905</td>\n",
       "      <td>-0.316169</td>\n",
       "      <td>-2.450200</td>\n",
       "      <td>0.896207</td>\n",
       "      <td>-0.306396</td>\n",
       "      <td>0.040796</td>\n",
       "      <td>-1.562901</td>\n",
       "      <td>-0.191658</td>\n",
       "      <td>-0.303967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996050</td>\n",
       "      <td>1.685361</td>\n",
       "      <td>0.038959</td>\n",
       "      <td>-1.332917</td>\n",
       "      <td>2.574497</td>\n",
       "      <td>0.663017</td>\n",
       "      <td>-0.766124</td>\n",
       "      <td>dms53</td>\n",
       "      <td>3.342054</td>\n",
       "      <td>Cisplatin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.257649</td>\n",
       "      <td>0.367905</td>\n",
       "      <td>-0.316169</td>\n",
       "      <td>-2.450200</td>\n",
       "      <td>0.896207</td>\n",
       "      <td>-0.306396</td>\n",
       "      <td>0.040796</td>\n",
       "      <td>-1.562901</td>\n",
       "      <td>-0.191658</td>\n",
       "      <td>-0.303967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996050</td>\n",
       "      <td>1.685361</td>\n",
       "      <td>0.038959</td>\n",
       "      <td>-1.332917</td>\n",
       "      <td>2.574497</td>\n",
       "      <td>0.663017</td>\n",
       "      <td>-0.766124</td>\n",
       "      <td>dms53</td>\n",
       "      <td>1.567744</td>\n",
       "      <td>Cytarabine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.257649</td>\n",
       "      <td>0.367905</td>\n",
       "      <td>-0.316169</td>\n",
       "      <td>-2.450200</td>\n",
       "      <td>0.896207</td>\n",
       "      <td>-0.306396</td>\n",
       "      <td>0.040796</td>\n",
       "      <td>-1.562901</td>\n",
       "      <td>-0.191658</td>\n",
       "      <td>-0.303967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996050</td>\n",
       "      <td>1.685361</td>\n",
       "      <td>0.038959</td>\n",
       "      <td>-1.332917</td>\n",
       "      <td>2.574497</td>\n",
       "      <td>0.663017</td>\n",
       "      <td>-0.766124</td>\n",
       "      <td>dms53</td>\n",
       "      <td>-1.394378</td>\n",
       "      <td>Docetaxel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139134</th>\n",
       "      <td>-0.966875</td>\n",
       "      <td>0.383040</td>\n",
       "      <td>-0.215750</td>\n",
       "      <td>0.743864</td>\n",
       "      <td>-0.267682</td>\n",
       "      <td>0.622732</td>\n",
       "      <td>0.825473</td>\n",
       "      <td>-0.279673</td>\n",
       "      <td>-0.128047</td>\n",
       "      <td>0.609692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990979</td>\n",
       "      <td>0.738363</td>\n",
       "      <td>-0.358515</td>\n",
       "      <td>-0.175460</td>\n",
       "      <td>-0.031960</td>\n",
       "      <td>-0.360483</td>\n",
       "      <td>-2.192188</td>\n",
       "      <td>hop92</td>\n",
       "      <td>5.759404</td>\n",
       "      <td>THR-103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139135</th>\n",
       "      <td>-0.966875</td>\n",
       "      <td>0.383040</td>\n",
       "      <td>-0.215750</td>\n",
       "      <td>0.743864</td>\n",
       "      <td>-0.267682</td>\n",
       "      <td>0.622732</td>\n",
       "      <td>0.825473</td>\n",
       "      <td>-0.279673</td>\n",
       "      <td>-0.128047</td>\n",
       "      <td>0.609692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990979</td>\n",
       "      <td>0.738363</td>\n",
       "      <td>-0.358515</td>\n",
       "      <td>-0.175460</td>\n",
       "      <td>-0.031960</td>\n",
       "      <td>-0.360483</td>\n",
       "      <td>-2.192188</td>\n",
       "      <td>hop92</td>\n",
       "      <td>11.646054</td>\n",
       "      <td>ascorbate (vitamin C)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139136</th>\n",
       "      <td>-0.966875</td>\n",
       "      <td>0.383040</td>\n",
       "      <td>-0.215750</td>\n",
       "      <td>0.743864</td>\n",
       "      <td>-0.267682</td>\n",
       "      <td>0.622732</td>\n",
       "      <td>0.825473</td>\n",
       "      <td>-0.279673</td>\n",
       "      <td>-0.128047</td>\n",
       "      <td>0.609692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990979</td>\n",
       "      <td>0.738363</td>\n",
       "      <td>-0.358515</td>\n",
       "      <td>-0.175460</td>\n",
       "      <td>-0.031960</td>\n",
       "      <td>-0.360483</td>\n",
       "      <td>-2.192188</td>\n",
       "      <td>hop92</td>\n",
       "      <td>9.336574</td>\n",
       "      <td>glutathione</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139137</th>\n",
       "      <td>-0.966875</td>\n",
       "      <td>0.383040</td>\n",
       "      <td>-0.215750</td>\n",
       "      <td>0.743864</td>\n",
       "      <td>-0.267682</td>\n",
       "      <td>0.622732</td>\n",
       "      <td>0.825473</td>\n",
       "      <td>-0.279673</td>\n",
       "      <td>-0.128047</td>\n",
       "      <td>0.609692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990979</td>\n",
       "      <td>0.738363</td>\n",
       "      <td>-0.358515</td>\n",
       "      <td>-0.175460</td>\n",
       "      <td>-0.031960</td>\n",
       "      <td>-0.360483</td>\n",
       "      <td>-2.192188</td>\n",
       "      <td>hop92</td>\n",
       "      <td>7.010299</td>\n",
       "      <td>alpha-lipoic acid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139138</th>\n",
       "      <td>-0.966875</td>\n",
       "      <td>0.383040</td>\n",
       "      <td>-0.215750</td>\n",
       "      <td>0.743864</td>\n",
       "      <td>-0.267682</td>\n",
       "      <td>0.622732</td>\n",
       "      <td>0.825473</td>\n",
       "      <td>-0.279673</td>\n",
       "      <td>-0.128047</td>\n",
       "      <td>0.609692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990979</td>\n",
       "      <td>0.738363</td>\n",
       "      <td>-0.358515</td>\n",
       "      <td>-0.175460</td>\n",
       "      <td>-0.031960</td>\n",
       "      <td>-0.360483</td>\n",
       "      <td>-2.192188</td>\n",
       "      <td>hop92</td>\n",
       "      <td>10.046488</td>\n",
       "      <td>N-acetyl cysteine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139139 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0       0.257649  0.367905 -0.316169 -2.450200  0.896207 -0.306396  0.040796   \n",
       "1       0.257649  0.367905 -0.316169 -2.450200  0.896207 -0.306396  0.040796   \n",
       "2       0.257649  0.367905 -0.316169 -2.450200  0.896207 -0.306396  0.040796   \n",
       "3       0.257649  0.367905 -0.316169 -2.450200  0.896207 -0.306396  0.040796   \n",
       "4       0.257649  0.367905 -0.316169 -2.450200  0.896207 -0.306396  0.040796   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "139134 -0.966875  0.383040 -0.215750  0.743864 -0.267682  0.622732  0.825473   \n",
       "139135 -0.966875  0.383040 -0.215750  0.743864 -0.267682  0.622732  0.825473   \n",
       "139136 -0.966875  0.383040 -0.215750  0.743864 -0.267682  0.622732  0.825473   \n",
       "139137 -0.966875  0.383040 -0.215750  0.743864 -0.267682  0.622732  0.825473   \n",
       "139138 -0.966875  0.383040 -0.215750  0.743864 -0.267682  0.622732  0.825473   \n",
       "\n",
       "               7         8         9  ...       121       122       123  \\\n",
       "0      -1.562901 -0.191658 -0.303967  ...  0.996050  1.685361  0.038959   \n",
       "1      -1.562901 -0.191658 -0.303967  ...  0.996050  1.685361  0.038959   \n",
       "2      -1.562901 -0.191658 -0.303967  ...  0.996050  1.685361  0.038959   \n",
       "3      -1.562901 -0.191658 -0.303967  ...  0.996050  1.685361  0.038959   \n",
       "4      -1.562901 -0.191658 -0.303967  ...  0.996050  1.685361  0.038959   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "139134 -0.279673 -0.128047  0.609692  ...  0.990979  0.738363 -0.358515   \n",
       "139135 -0.279673 -0.128047  0.609692  ...  0.990979  0.738363 -0.358515   \n",
       "139136 -0.279673 -0.128047  0.609692  ...  0.990979  0.738363 -0.358515   \n",
       "139137 -0.279673 -0.128047  0.609692  ...  0.990979  0.738363 -0.358515   \n",
       "139138 -0.279673 -0.128047  0.609692  ...  0.990979  0.738363 -0.358515   \n",
       "\n",
       "             124       125       126       127  CELL_LINE_NAME    LN_IC50  \\\n",
       "0      -1.332917  2.574497  0.663017 -0.766124           dms53  -3.193970   \n",
       "1      -1.332917  2.574497  0.663017 -0.766124           dms53   0.868145   \n",
       "2      -1.332917  2.574497  0.663017 -0.766124           dms53   3.342054   \n",
       "3      -1.332917  2.574497  0.663017 -0.766124           dms53   1.567744   \n",
       "4      -1.332917  2.574497  0.663017 -0.766124           dms53  -1.394378   \n",
       "...          ...       ...       ...       ...             ...        ...   \n",
       "139134 -0.175460 -0.031960 -0.360483 -2.192188           hop92   5.759404   \n",
       "139135 -0.175460 -0.031960 -0.360483 -2.192188           hop92  11.646054   \n",
       "139136 -0.175460 -0.031960 -0.360483 -2.192188           hop92   9.336574   \n",
       "139137 -0.175460 -0.031960 -0.360483 -2.192188           hop92   7.010299   \n",
       "139138 -0.175460 -0.031960 -0.360483 -2.192188           hop92  10.046488   \n",
       "\n",
       "                    DRUG_NAME  \n",
       "0                Camptothecin  \n",
       "1                 Vinblastine  \n",
       "2                   Cisplatin  \n",
       "3                  Cytarabine  \n",
       "4                   Docetaxel  \n",
       "...                       ...  \n",
       "139134                THR-103  \n",
       "139135  ascorbate (vitamin C)  \n",
       "139136            glutathione  \n",
       "139137      alpha-lipoic acid  \n",
       "139138      N-acetyl cysteine  \n",
       "\n",
       "[139139 rows x 131 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension_w_drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_w_drug.drop(columns=['CELL_LINE_NAME'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_dim = 128\n",
    "# dimension = pd.read_csv('../results/results' + str(selected_dim) + 'D_latent_space_gene_exp.tsv',sep='\\t')\n",
    "# cell_line_name = pd.read_csv('../results_clean/cell_line_name.csv')\n",
    "# gdsc_drug = pd.read_csv('../results_clean/gdsc_drug_nodash.csv')\n",
    "# dimension.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "# gdsc_drug.drop(columns=['Unnamed: 0'], inplace= True)\n",
    "# dimension['CELL_LINE_NAME'] = cell_line_name['CELL_LINE_NAME']\n",
    "# dimension_w_drug = pd.merge(dimension, gdsc_drug, on='CELL_LINE_NAME')\n",
    "# dimension_w_drug.drop(columns=['CELL_LINE_NAME'],inplace=True)\n",
    "# # dimension_w_drug = pd.get_dummies(dimension_w_drug, columns=['DRUG_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>LN_IC50</th>\n",
       "      <th>DRUG_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.660497</td>\n",
       "      <td>-1.225458</td>\n",
       "      <td>0.298976</td>\n",
       "      <td>-0.572784</td>\n",
       "      <td>-0.607899</td>\n",
       "      <td>0.719592</td>\n",
       "      <td>0.188951</td>\n",
       "      <td>0.251659</td>\n",
       "      <td>-1.018698</td>\n",
       "      <td>-1.290491</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.596631</td>\n",
       "      <td>0.558098</td>\n",
       "      <td>-0.617923</td>\n",
       "      <td>-0.194980</td>\n",
       "      <td>-1.144901</td>\n",
       "      <td>-2.255126</td>\n",
       "      <td>-1.161410</td>\n",
       "      <td>0.863871</td>\n",
       "      <td>-1.671558</td>\n",
       "      <td>AZD8055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.148928</td>\n",
       "      <td>-0.192305</td>\n",
       "      <td>0.207944</td>\n",
       "      <td>-0.325320</td>\n",
       "      <td>0.216569</td>\n",
       "      <td>0.738689</td>\n",
       "      <td>0.305716</td>\n",
       "      <td>0.015035</td>\n",
       "      <td>0.515717</td>\n",
       "      <td>0.665093</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.224585</td>\n",
       "      <td>0.255838</td>\n",
       "      <td>0.708959</td>\n",
       "      <td>0.145579</td>\n",
       "      <td>0.740762</td>\n",
       "      <td>0.876360</td>\n",
       "      <td>0.641892</td>\n",
       "      <td>-1.228383</td>\n",
       "      <td>2.738085</td>\n",
       "      <td>150412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.537628</td>\n",
       "      <td>0.295881</td>\n",
       "      <td>-0.312474</td>\n",
       "      <td>0.045696</td>\n",
       "      <td>-0.969107</td>\n",
       "      <td>1.286327</td>\n",
       "      <td>-0.381257</td>\n",
       "      <td>0.217041</td>\n",
       "      <td>0.524994</td>\n",
       "      <td>-0.051314</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.187785</td>\n",
       "      <td>-0.505828</td>\n",
       "      <td>-0.085881</td>\n",
       "      <td>0.451674</td>\n",
       "      <td>-0.327806</td>\n",
       "      <td>0.012291</td>\n",
       "      <td>-0.457253</td>\n",
       "      <td>1.272840</td>\n",
       "      <td>1.343824</td>\n",
       "      <td>Taselisib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.113711</td>\n",
       "      <td>-0.433449</td>\n",
       "      <td>-0.216219</td>\n",
       "      <td>0.716303</td>\n",
       "      <td>-0.103957</td>\n",
       "      <td>-1.795534</td>\n",
       "      <td>0.945597</td>\n",
       "      <td>-0.861893</td>\n",
       "      <td>0.410452</td>\n",
       "      <td>-0.578108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273412</td>\n",
       "      <td>0.500376</td>\n",
       "      <td>0.042380</td>\n",
       "      <td>0.023065</td>\n",
       "      <td>0.802750</td>\n",
       "      <td>-1.075832</td>\n",
       "      <td>-0.111321</td>\n",
       "      <td>-0.259111</td>\n",
       "      <td>5.835869</td>\n",
       "      <td>N30652-18-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.809540</td>\n",
       "      <td>0.233606</td>\n",
       "      <td>-0.568925</td>\n",
       "      <td>0.388955</td>\n",
       "      <td>-0.047145</td>\n",
       "      <td>-0.758056</td>\n",
       "      <td>1.284933</td>\n",
       "      <td>-1.159245</td>\n",
       "      <td>0.257241</td>\n",
       "      <td>-1.380166</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.560416</td>\n",
       "      <td>0.546079</td>\n",
       "      <td>0.349454</td>\n",
       "      <td>0.232099</td>\n",
       "      <td>-0.527831</td>\n",
       "      <td>-1.049276</td>\n",
       "      <td>0.025659</td>\n",
       "      <td>-2.405237</td>\n",
       "      <td>0.607005</td>\n",
       "      <td>Mycophenolic acid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139134</th>\n",
       "      <td>0.389052</td>\n",
       "      <td>0.551605</td>\n",
       "      <td>0.748280</td>\n",
       "      <td>0.484167</td>\n",
       "      <td>-0.275389</td>\n",
       "      <td>-2.121249</td>\n",
       "      <td>-0.030644</td>\n",
       "      <td>-0.354845</td>\n",
       "      <td>-0.044124</td>\n",
       "      <td>-1.706383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.828513</td>\n",
       "      <td>-0.542032</td>\n",
       "      <td>-0.540505</td>\n",
       "      <td>-0.130605</td>\n",
       "      <td>-1.141231</td>\n",
       "      <td>1.047644</td>\n",
       "      <td>0.294905</td>\n",
       "      <td>-3.105221</td>\n",
       "      <td>2.131133</td>\n",
       "      <td>GSK-LSD1-2HCl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139135</th>\n",
       "      <td>-1.114373</td>\n",
       "      <td>-0.155593</td>\n",
       "      <td>-0.472478</td>\n",
       "      <td>-0.620802</td>\n",
       "      <td>-0.637903</td>\n",
       "      <td>-0.547858</td>\n",
       "      <td>0.743013</td>\n",
       "      <td>0.854975</td>\n",
       "      <td>-0.235164</td>\n",
       "      <td>0.161446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.814348</td>\n",
       "      <td>-0.893375</td>\n",
       "      <td>-0.317431</td>\n",
       "      <td>-0.082845</td>\n",
       "      <td>-1.763968</td>\n",
       "      <td>1.759656</td>\n",
       "      <td>0.213867</td>\n",
       "      <td>0.327702</td>\n",
       "      <td>3.109407</td>\n",
       "      <td>Obatoclax Mesylate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139136</th>\n",
       "      <td>0.832460</td>\n",
       "      <td>-1.080370</td>\n",
       "      <td>-0.593841</td>\n",
       "      <td>0.875753</td>\n",
       "      <td>0.931568</td>\n",
       "      <td>-1.333344</td>\n",
       "      <td>0.808531</td>\n",
       "      <td>-0.802226</td>\n",
       "      <td>0.288631</td>\n",
       "      <td>0.054172</td>\n",
       "      <td>...</td>\n",
       "      <td>1.595705</td>\n",
       "      <td>-0.375960</td>\n",
       "      <td>-1.019392</td>\n",
       "      <td>-0.106573</td>\n",
       "      <td>-1.221605</td>\n",
       "      <td>-2.708111</td>\n",
       "      <td>-0.827750</td>\n",
       "      <td>0.452205</td>\n",
       "      <td>1.389483</td>\n",
       "      <td>Cediranib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139137</th>\n",
       "      <td>-0.175985</td>\n",
       "      <td>0.271786</td>\n",
       "      <td>0.062224</td>\n",
       "      <td>-0.222707</td>\n",
       "      <td>0.097327</td>\n",
       "      <td>-0.568416</td>\n",
       "      <td>-0.470046</td>\n",
       "      <td>1.210731</td>\n",
       "      <td>-0.945002</td>\n",
       "      <td>0.482943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.259105</td>\n",
       "      <td>-0.247450</td>\n",
       "      <td>-1.114933</td>\n",
       "      <td>0.195331</td>\n",
       "      <td>0.526805</td>\n",
       "      <td>-0.526970</td>\n",
       "      <td>0.040478</td>\n",
       "      <td>1.450730</td>\n",
       "      <td>2.507624</td>\n",
       "      <td>RO-3306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139138</th>\n",
       "      <td>0.351970</td>\n",
       "      <td>0.377516</td>\n",
       "      <td>-0.190342</td>\n",
       "      <td>-0.574893</td>\n",
       "      <td>0.479878</td>\n",
       "      <td>1.402368</td>\n",
       "      <td>-0.648448</td>\n",
       "      <td>-1.484429</td>\n",
       "      <td>-0.525522</td>\n",
       "      <td>-0.065574</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.532926</td>\n",
       "      <td>1.450664</td>\n",
       "      <td>-0.430319</td>\n",
       "      <td>-0.398300</td>\n",
       "      <td>0.357128</td>\n",
       "      <td>1.207134</td>\n",
       "      <td>-0.479981</td>\n",
       "      <td>-0.452650</td>\n",
       "      <td>2.977019</td>\n",
       "      <td>AZD5438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139139 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0      -0.660497 -1.225458  0.298976 -0.572784 -0.607899  0.719592  0.188951   \n",
       "1       0.148928 -0.192305  0.207944 -0.325320  0.216569  0.738689  0.305716   \n",
       "2      -0.537628  0.295881 -0.312474  0.045696 -0.969107  1.286327 -0.381257   \n",
       "3       2.113711 -0.433449 -0.216219  0.716303 -0.103957 -1.795534  0.945597   \n",
       "4       0.809540  0.233606 -0.568925  0.388955 -0.047145 -0.758056  1.284933   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "139134  0.389052  0.551605  0.748280  0.484167 -0.275389 -2.121249 -0.030644   \n",
       "139135 -1.114373 -0.155593 -0.472478 -0.620802 -0.637903 -0.547858  0.743013   \n",
       "139136  0.832460 -1.080370 -0.593841  0.875753  0.931568 -1.333344  0.808531   \n",
       "139137 -0.175985  0.271786  0.062224 -0.222707  0.097327 -0.568416 -0.470046   \n",
       "139138  0.351970  0.377516 -0.190342 -0.574893  0.479878  1.402368 -0.648448   \n",
       "\n",
       "               7         8         9  ...       120       121       122  \\\n",
       "0       0.251659 -1.018698 -1.290491  ... -0.596631  0.558098 -0.617923   \n",
       "1       0.015035  0.515717  0.665093  ... -0.224585  0.255838  0.708959   \n",
       "2       0.217041  0.524994 -0.051314  ... -0.187785 -0.505828 -0.085881   \n",
       "3      -0.861893  0.410452 -0.578108  ...  0.273412  0.500376  0.042380   \n",
       "4      -1.159245  0.257241 -1.380166  ... -0.560416  0.546079  0.349454   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "139134 -0.354845 -0.044124 -1.706383  ...  0.828513 -0.542032 -0.540505   \n",
       "139135  0.854975 -0.235164  0.161446  ... -0.814348 -0.893375 -0.317431   \n",
       "139136 -0.802226  0.288631  0.054172  ...  1.595705 -0.375960 -1.019392   \n",
       "139137  1.210731 -0.945002  0.482943  ... -0.259105 -0.247450 -1.114933   \n",
       "139138 -1.484429 -0.525522 -0.065574  ... -0.532926  1.450664 -0.430319   \n",
       "\n",
       "             123       124       125       126       127   LN_IC50  \\\n",
       "0      -0.194980 -1.144901 -2.255126 -1.161410  0.863871 -1.671558   \n",
       "1       0.145579  0.740762  0.876360  0.641892 -1.228383  2.738085   \n",
       "2       0.451674 -0.327806  0.012291 -0.457253  1.272840  1.343824   \n",
       "3       0.023065  0.802750 -1.075832 -0.111321 -0.259111  5.835869   \n",
       "4       0.232099 -0.527831 -1.049276  0.025659 -2.405237  0.607005   \n",
       "...          ...       ...       ...       ...       ...       ...   \n",
       "139134 -0.130605 -1.141231  1.047644  0.294905 -3.105221  2.131133   \n",
       "139135 -0.082845 -1.763968  1.759656  0.213867  0.327702  3.109407   \n",
       "139136 -0.106573 -1.221605 -2.708111 -0.827750  0.452205  1.389483   \n",
       "139137  0.195331  0.526805 -0.526970  0.040478  1.450730  2.507624   \n",
       "139138 -0.398300  0.357128  1.207134 -0.479981 -0.452650  2.977019   \n",
       "\n",
       "                 DRUG_NAME  \n",
       "0                  AZD8055  \n",
       "1                   150412  \n",
       "2                Taselisib  \n",
       "3              N30652-18-1  \n",
       "4        Mycophenolic acid  \n",
       "...                    ...  \n",
       "139134      GSK-LSD1-2HCl   \n",
       "139135  Obatoclax Mesylate  \n",
       "139136           Cediranib  \n",
       "139137             RO-3306  \n",
       "139138             AZD5438  \n",
       "\n",
       "[139139 rows x 130 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension_w_drug = dimension_w_drug.sample(frac=1, random_state=33).reset_index(drop=True)\n",
    "dimension_w_drug"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate continuous and categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['DRUG_NAME']\n",
    "cont_cols = dimension_w_drug.drop(columns=['DRUG_NAME', 'LN_IC50']).columns\n",
    "label_cols = ['LN_IC50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
       "       ...\n",
       "       '118', '119', '120', '121', '122', '123', '124', '125', '126', '127'],\n",
       "      dtype='object', length=128)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DRUG_NAME']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in cat_cols:\n",
    "    dimension_w_drug[cat] = dimension_w_drug[cat].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             float64\n",
       "1             float64\n",
       "2             float64\n",
       "3             float64\n",
       "4             float64\n",
       "               ...   \n",
       "125           float64\n",
       "126           float64\n",
       "127           float64\n",
       "LN_IC50       float64\n",
       "DRUG_NAME    category\n",
       "Length: 130, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension_w_drug.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              AZD8055\n",
       "1               150412\n",
       "2            Taselisib\n",
       "3          N30652-18-1\n",
       "4    Mycophenolic acid\n",
       "Name: DRUG_NAME, dtype: category\n",
       "Categories (286, object): ['123138', '123829', '150412', '5-Fluorouracil', ..., 'Zoledronate', 'alpha-lipoic acid', 'ascorbate (vitamin C)', 'glutathione']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension_w_drug['DRUG_NAME'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['123138', '123829', '150412', '5-Fluorouracil', '5-azacytidine',\n",
       "       '50869', '615590', '630600', '667880', '720427',\n",
       "       ...\n",
       "       'WZ4003', 'Wee1 Inhibitor', 'Wnt-C59', 'XAV939', 'YK-4-279', 'ZM447439',\n",
       "       'Zoledronate', 'alpha-lipoic acid', 'ascorbate (vitamin C)',\n",
       "       'glutathione'],\n",
       "      dtype='object', length=286)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension_w_drug['DRUG_NAME'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_name = dimension_w_drug['DRUG_NAME'].cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = np.stack([drug_name],1)\n",
    "cats = torch.tensor(cats, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 37],\n",
       "        [  2],\n",
       "        [247],\n",
       "        ...,\n",
       "        [ 74],\n",
       "        [212],\n",
       "        [ 31]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous variables to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6605, -1.2255,  0.2990,  ..., -2.2551, -1.1614,  0.8639],\n",
       "        [ 0.1489, -0.1923,  0.2079,  ...,  0.8764,  0.6419, -1.2284],\n",
       "        [-0.5376,  0.2959, -0.3125,  ...,  0.0123, -0.4573,  1.2728],\n",
       "        ...,\n",
       "        [ 0.8325, -1.0804, -0.5938,  ..., -2.7081, -0.8277,  0.4522],\n",
       "        [-0.1760,  0.2718,  0.0622,  ..., -0.5270,  0.0405,  1.4507],\n",
       "        [ 0.3520,  0.3775, -0.1903,  ...,  1.2071, -0.4800, -0.4527]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conts = np.stack([dimension_w_drug[col].values for col in cont_cols], 1)\n",
    "conts = torch.tensor(conts, dtype=torch.float)\n",
    "conts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6716],\n",
       "        [ 2.7381],\n",
       "        [ 1.3438],\n",
       "        ...,\n",
       "        [ 1.3895],\n",
       "        [ 2.5076],\n",
       "        [ 2.9770]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.stack([dimension_w_drug[col].values for col in label_cols], 1)\n",
    "labels = torch.tensor(labels, dtype=torch.float)\n",
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(286, 50)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_szs = [len(dimension_w_drug[col].cat.categories) for col in cat_cols]\n",
    "emb_szs = [(size, min(50, (size+1)//2)) for size in cat_szs]\n",
    "emb_szs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_szs, n_cont, out_sz, layers, p=0.5):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "        \n",
    "        layerlist = []\n",
    "        n_emb = sum((nf for ni,nf in emb_szs))\n",
    "        n_in = n_emb + n_cont\n",
    "        \n",
    "        for i in layers:\n",
    "            layerlist.append(nn.Linear(n_in,i)) \n",
    "            layerlist.append(nn.ReLU(inplace=True))\n",
    "            layerlist.append(nn.BatchNorm1d(i))\n",
    "            layerlist.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "        layerlist.append(nn.Linear(layers[-1],out_sz))\n",
    "            \n",
    "        self.layers = nn.Sequential(*layerlist)\n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x_cat[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        \n",
    "        x_cont = self.bn_cont(x_cont)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "torch.manual_seed(33)\n",
    "model = TabularModel(emb_szs, conts.shape[1], 1, [200,100], p=0.4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(286, 50)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=178, out_features=200, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Optimizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0) # Get name device with ID '0'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform train/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = conts.shape[0]\n",
    "test_size = int(batch_size * .2)\n",
    "\n",
    "cat_train = cats[:batch_size-test_size]\n",
    "cat_test = cats[batch_size-test_size:batch_size]\n",
    "con_train = conts[:batch_size-test_size]\n",
    "con_test = conts[batch_size-test_size:batch_size]\n",
    "y_train = labels[:batch_size-test_size]\n",
    "y_test = labels[batch_size-test_size:batch_size]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try with dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train_l, cat_test_l, y_train_l, y_test_l, con_train_l, con_test_l = train_test_split(cats, labels, conts, test_size=0.2, random_state= 33, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(cat_train_l, con_train_l, y_train_l)\n",
    "test_dataset = torch.utils.data.TensorDataset(cat_test_l, con_test_l, y_test_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, batch_size = batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_dataset.tensors[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.0994],\n",
      "        [-0.9121],\n",
      "        [ 6.6825],\n",
      "        ...,\n",
      "        [ 1.3895],\n",
      "        [ 2.5076],\n",
      "        [ 2.9770]])\n"
     ]
    }
   ],
   "source": [
    "for cat, con, y in test_loader:\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_train_l.shape[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1  loss: 1.77127957\n",
      "epoch:  26  loss: 1.08074939\n",
      "epoch:  51  loss: 1.00445557\n",
      "epoch:  76  loss: 0.95520687\n",
      "epoch: 100  loss: 1.17885780\n",
      "\n",
      "Duration: 250 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "device = torch.device('cuda')\n",
    "epochs = 100\n",
    "losses = []\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    i+=1\n",
    "    best_loss = np.inf\n",
    "    for cat_train, con_train, y_train in train_loader:\n",
    "        cat_train = cat_train.to(device)\n",
    "        con_train = con_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "\n",
    "        y_pred = model(cat_train, con_train)\n",
    "        y_pred = y_pred.to(device)\n",
    "        loss = torch.sqrt(loss_fn(y_pred, y_train))\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if i%25 == 1:\n",
    "            print(f'epoch: {i:3}  loss: {best_loss:10.8f}')\n",
    "    losses.append(best_loss)\n",
    "\n",
    "print(f'epoch: {i:3}  loss: {loss.item():10.8f}')\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epoch')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs), losses)\n",
    "plt.ylabel('RMSE Loss')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "RMSE: 1.19066882\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    for cat_test, con_test, y_test in test_loader:\n",
    "        cat_test = cat_test.to(device)\n",
    "        con_test = con_test.to(device)\n",
    "        y_test = y_test.to(device)\n",
    "        \n",
    "        y_val = model(cat_test, con_test)\n",
    "        loss = torch.sqrt(loss_fn(y_val, y_test))\n",
    "        i += 1\n",
    "        print(i)\n",
    "print(f'RMSE: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PREDICTED   ACTUAL     DIFF\n",
      " 1.   4.6455   4.0994   0.5461\n",
      " 2.  -1.7840  -0.9121   0.8719\n",
      " 3.   4.8439   6.6825   1.8386\n",
      " 4.   3.2999   1.8101   1.4899\n",
      " 5.   4.6959   3.9212   0.7747\n",
      " 6.   5.1515   5.6071   0.4556\n",
      " 7.   1.5938   2.6574   1.0635\n",
      " 8.  -3.3793  -2.0265   1.3527\n",
      " 9.   5.3625   6.1000   0.7376\n",
      "10.   9.4591  11.0995   1.6404\n",
      "11.   5.3809   5.7157   0.3348\n",
      "12.   4.6441   4.5104   0.1337\n",
      "13.   4.6865   5.6257   0.9392\n",
      "14.   2.6873   2.8756   0.1884\n",
      "15.   3.5597   2.8913   0.6684\n",
      "16.   4.4174   4.3893   0.0281\n",
      "17.  -3.6572   1.1053   4.7626\n",
      "18.   4.7971   4.5710   0.2261\n",
      "19.   6.6975   7.5038   0.8062\n",
      "20.   5.8626   5.6932   0.1693\n",
      "21.   4.6746   5.6058   0.9312\n",
      "22.   0.5366   0.0198   0.5168\n",
      "23.   6.0206   8.3387   2.3181\n",
      "24.   2.9755   4.7744   1.7989\n",
      "25.   1.8002   2.5741   0.7739\n",
      "26.   4.3313   4.5165   0.1852\n",
      "27.  -1.9109  -2.7403   0.8294\n",
      "28.  -0.0021   1.1515   1.1536\n",
      "29.   5.1529   5.9018   0.7489\n",
      "30.   3.9543   3.7891   0.1652\n",
      "31.   0.4337   1.2685   0.8348\n",
      "32.   4.7625   5.5530   0.7905\n",
      "33.  -1.9882  -2.0136   0.0254\n",
      "34.   2.2149   1.5015   0.7134\n",
      "35.   5.1805   5.2350   0.0545\n",
      "36.   3.1284   4.5639   1.4355\n",
      "37.   2.2101   1.9282   0.2819\n",
      "38.   4.2769   4.5962   0.3193\n",
      "39.   4.2334   5.2806   1.0472\n",
      "40.   3.8978   3.5599   0.3379\n",
      "41.   2.8136   3.0898   0.2762\n",
      "42.   2.2715   1.6528   0.6187\n",
      "43.  -0.6057   2.8400   3.4457\n",
      "44.   0.6895   1.1316   0.4421\n",
      "45.   6.1928   4.8902   1.3026\n",
      "46.   1.7420   1.3215   0.4205\n",
      "47.   3.3061   1.7834   1.5226\n",
      "48.   3.2138   1.8463   1.3675\n",
      "49.   4.4883   4.9559   0.4676\n",
      "50.   0.9526   2.0155   1.0629\n",
      "51.   0.9573   0.8408   0.1165\n",
      "52.   2.7874   3.0606   0.2731\n",
      "53.   2.6914  -0.5848   3.2761\n",
      "54.   4.9684   5.2783   0.3100\n",
      "55.   4.7109   4.0391   0.6718\n",
      "56.  -2.5825  -2.9825   0.3999\n",
      "57.   4.3079   3.9982   0.3097\n",
      "58.   2.4513   2.7393   0.2880\n",
      "59.   1.2143   3.4052   2.1909\n",
      "60.   1.8006   2.6669   0.8664\n",
      "61.   5.1129   5.1812   0.0683\n",
      "62.   4.4742   4.9366   0.4625\n",
      "63.   0.6028  -0.6598   1.2626\n",
      "64.   6.4149   5.9360   0.4789\n",
      "65.   3.0765   3.7102   0.6338\n",
      "66.   4.6859   5.0608   0.3748\n",
      "67.   3.6380   3.1983   0.4397\n",
      "68.   6.5518   7.1180   0.5662\n",
      "69.   1.8367   1.5549   0.2818\n",
      "70.  -3.0283  -2.0854   0.9429\n",
      "71.   5.0604   4.7946   0.2657\n",
      "72.   5.4502   4.3958   1.0544\n",
      "73.   4.3874   6.3406   1.9532\n",
      "74.   4.5108   5.2373   0.7265\n",
      "75.   2.2770   2.3162   0.0392\n",
      "76.  -3.7292  -4.6625   0.9333\n",
      "77.   3.1233   3.8683   0.7450\n",
      "78.   3.4499   4.2483   0.7984\n",
      "79.   3.6947   4.3277   0.6330\n",
      "80.   3.7259   3.9203   0.1944\n",
      "81.  -0.8534   0.5869   1.4404\n",
      "82.   5.0000   5.5753   0.5753\n",
      "83.   3.0634   1.3687   1.6947\n",
      "84.   2.2520   1.5587   0.6933\n",
      "85.   3.0577   3.8650   0.8074\n",
      "86.   2.3127   1.8341   0.4786\n",
      "87.   4.3313   4.8029   0.4716\n",
      "88.   3.1425   0.6166   2.5259\n",
      "89.   0.8637   0.0272   0.8365\n",
      "90.   3.9881   4.5211   0.5330\n",
      "91.   3.9095   2.7614   1.1481\n",
      "92.   0.3278   0.1661   0.1617\n",
      "93.   2.3672   2.0981   0.2691\n",
      "94.   2.6247   0.7289   1.8958\n",
      "95.   6.0872   4.7481   1.3392\n",
      "96.   1.9855   1.6893   0.2962\n",
      "97.   3.0747   5.2273   2.1526\n",
      "98.   1.9057   2.3258   0.4201\n",
      "99.   3.8750   4.3257   0.4507\n",
      "100.   2.5326   3.0423   0.5097\n"
     ]
    }
   ],
   "source": [
    "print(f'{\"PREDICTED\":>12} {\"ACTUAL\":>8} {\"DIFF\":>8}')\n",
    "for i in range(100):\n",
    "    diff = np.abs(y_val[i].item()-y_test[i].item())\n",
    "    print(f'{i+1:2}. {y_val[i].item():8.4f} {y_test[i].item():8.4f} {diff:8.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Max Scale X and extract y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "col = dimension_w_drug.drop(columns=['LN_IC50']).columns\n",
    "X = scaler.fit_transform(dimension_w_drug.drop(columns=['LN_IC50']))\n",
    "X = pd.DataFrame(X, columns=col)\n",
    "y = dimension_w_drug['LN_IC50']\n",
    "\n",
    "scaled_df = X\n",
    "scaled_df['LN_IC50'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor Portion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,df):\n",
    " \n",
    "        col = df.drop(columns=['LN_IC50']).columns\n",
    "        x = df.drop(columns=['LN_IC50'])\n",
    "        y = df['LN_IC50']\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "    \n",
    "        self.x_train=torch.tensor(x,dtype=torch.float32)\n",
    "        self.y_train=torch.tensor(y,dtype=torch.float32)\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "   \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_train[idx],self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare full dataset\n",
    "full_dataset = MyDataset(scaled_df)\n",
    "\n",
    "# split dataset to train and test\n",
    "dim_np = np.array(scaled_df)\n",
    "dim_col = scaled_df.columns\n",
    "train, test = train_test_split(dim_np, train_size=0.8, shuffle=True, random_state=0)\n",
    "train = pd.DataFrame(train, columns=dim_col)\n",
    "test = pd.DataFrame(test, columns=dim_col)\n",
    "# declare train and test dataset\n",
    "train_dataset = MyDataset(train)\n",
    "test_dataset = MyDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_x_train = train.drop(columns=['LN_IC50'])\n",
    "xgb_y_train = train['LN_IC50']\n",
    "\n",
    "xgb_x_test = test.drop(columns=['LN_IC50'])\n",
    "xgb_y_test = test['LN_IC50']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_r = xgb.XGBRegressor()\n",
    "xgb_r.fit(xgb_x_train, xgb_y_train)\n",
    "\n",
    "y_pred = xgb_r.predict(xgb_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tensor = torch.tensor(y_pred)\n",
    "y_tensor = torch.tensor(xgb_y_test)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "loss_function(y_pred_tensor, y_tensor).item()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    " \n",
    "class predictor_model(nn.Module):\n",
    "  def __init__(self):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.layers = nn.Sequential(\n",
    "    nn.Linear(414, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare reset weights function for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "   if hasattr(layer, 'reset_parameters'):\n",
    "    print(f'Reset trainable parameters of layer = {layer}')\n",
    "    layer.reset_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = predictor_model()\n",
    "k_folds = 5\n",
    "n_epochs = 20\n",
    "torch.manual_seed(0)\n",
    "batch_size = 10\n",
    "lr = 0.0001\n",
    "kfold = KFold(n_splits=k_folds, shuffle = True)\n",
    "best_mse = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "device = torch.device('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader):\n",
    "    count = 0\n",
    "    model.train()\n",
    "    current_loss = 0.0\n",
    "    for i, data in enumerate(dataloader,0):\n",
    "        x, y = data\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        \n",
    "        output = model(x)\n",
    "        loss = loss_function(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        count = count + 1\n",
    "\n",
    "    overall_loss = current_loss/count\n",
    "\n",
    "    return overall_loss\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    count = 0\n",
    "    model.eval()\n",
    "    current_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader,0):\n",
    "            x, y = data\n",
    "        \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            output = model(x)\n",
    "            loss = loss_function(output, y)\n",
    "\n",
    "            current_loss += loss.item()\n",
    "            count = count + 1\n",
    "\n",
    "    overall_loss = current_loss/count\n",
    "\n",
    "    return overall_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_new(model, X_test, y_test,best_mse, history,best_weights):\n",
    "    model.eval()\n",
    "    output = model(X_test)\n",
    "    mse = loss_function(output, y_test)\n",
    "    mse = float(mse)\n",
    "    history.append(mse)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    return mse, best_mse, history, best_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "best_mse = np.inf\n",
    "history = []\n",
    "best_weights = None\n",
    "for epoch in range(0, n_epochs):\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    train_loss = train(model, train_loader)\n",
    "    train_loss_list.append(train_loss)\n",
    "    print(f'Training MSE loss for epoch {epoch+1}: ' + str(train_loss))\n",
    "\n",
    "    X_test = torch.tensor(np.array(xgb_x_test), dtype=torch.float32)\n",
    "    y_test = torch.tensor(np.array(xgb_y_test), dtype=torch.float32)\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    val_loss, best_mse, history, best_weights = validate_new(model, X_test, y_test, best_mse, history, best_weights)\n",
    "    val_loss_list.append(val_loss)\n",
    "    print(f'Validation MSE loss for epoch {epoch+1}: ' + str(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(0, n_epochs):\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    train_loss = train(model, train_loader)\n",
    "    train_loss_list.append(train_loss)\n",
    "    print(f'Training MSE loss for epoch {epoch+1}: ' + str(train_loss))\n",
    "\n",
    "    val_loss = validate(model, test_loader)\n",
    "    val_loss_list.append(val_loss)\n",
    "    print(f'Validation MSE loss for epoch {epoch+1}: ' + str(val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
