{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mdzak\\anaconda3\\envs\\fyp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import matplotlib\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "matplotlib.style.use('ggplot')\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = pd.read_csv('../datasets_transpose_csv/genes_transpose.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_ = genes.drop(columns=['Unnamed: 0'])\n",
    "genes_ = genes_.drop(columns=['CELL_LINE'])\n",
    "genes_np = genes_.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_train, genes_test = train_test_split(genes_np, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(917, 57820)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genes_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 57820)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genes_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000000003.10</th>\n",
       "      <th>ENSG00000000005.5</th>\n",
       "      <th>ENSG00000000419.8</th>\n",
       "      <th>ENSG00000000457.9</th>\n",
       "      <th>ENSG00000000460.12</th>\n",
       "      <th>ENSG00000000938.8</th>\n",
       "      <th>ENSG00000000971.11</th>\n",
       "      <th>ENSG00000001036.9</th>\n",
       "      <th>ENSG00000001084.6</th>\n",
       "      <th>ENSG00000001167.10</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSGR0000237531.1</th>\n",
       "      <th>ENSGR0000237801.1</th>\n",
       "      <th>ENSGR0000263835.1</th>\n",
       "      <th>ENSGR0000263980.1</th>\n",
       "      <th>ENSGR0000264510.1</th>\n",
       "      <th>ENSGR0000264819.1</th>\n",
       "      <th>ENSGR0000265350.1</th>\n",
       "      <th>ENSGR0000265658.1</th>\n",
       "      <th>ENSGR0000266731.1</th>\n",
       "      <th>ENSGR0000270726.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.38</td>\n",
       "      <td>9.76</td>\n",
       "      <td>24.51</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>54.86</td>\n",
       "      <td>118.50</td>\n",
       "      <td>38.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.99</td>\n",
       "      <td>16.76</td>\n",
       "      <td>13.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>170.91</td>\n",
       "      <td>93.00</td>\n",
       "      <td>18.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.51</td>\n",
       "      <td>2.58</td>\n",
       "      <td>10.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>30.78</td>\n",
       "      <td>22.16</td>\n",
       "      <td>14.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.39</td>\n",
       "      <td>3.25</td>\n",
       "      <td>5.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>45.05</td>\n",
       "      <td>19.45</td>\n",
       "      <td>7.91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.28</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.27</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.45</td>\n",
       "      <td>53.29</td>\n",
       "      <td>8.36</td>\n",
       "      <td>15.27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>47.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119.50</td>\n",
       "      <td>5.06</td>\n",
       "      <td>9.58</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.10</td>\n",
       "      <td>115.62</td>\n",
       "      <td>40.13</td>\n",
       "      <td>21.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>28.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.09</td>\n",
       "      <td>4.94</td>\n",
       "      <td>13.35</td>\n",
       "      <td>0.22</td>\n",
       "      <td>139.44</td>\n",
       "      <td>118.19</td>\n",
       "      <td>13.53</td>\n",
       "      <td>26.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>61.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>109.13</td>\n",
       "      <td>5.91</td>\n",
       "      <td>17.40</td>\n",
       "      <td>0.13</td>\n",
       "      <td>53.25</td>\n",
       "      <td>92.96</td>\n",
       "      <td>23.09</td>\n",
       "      <td>33.22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>8.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>139.23</td>\n",
       "      <td>15.96</td>\n",
       "      <td>17.45</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.23</td>\n",
       "      <td>19.75</td>\n",
       "      <td>20.37</td>\n",
       "      <td>12.47</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>16.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.20</td>\n",
       "      <td>13.76</td>\n",
       "      <td>11.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>24.06</td>\n",
       "      <td>47.14</td>\n",
       "      <td>15.85</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1019 rows Ã— 57820 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ENSG00000000003.10  ENSG00000000005.5  ENSG00000000419.8  \\\n",
       "0                   5.28                0.0              73.38   \n",
       "1                   7.01                0.0             108.99   \n",
       "2                  22.80                0.0              56.51   \n",
       "3                  22.88                0.0              45.39   \n",
       "4                  23.09                0.0              99.28   \n",
       "...                  ...                ...                ...   \n",
       "1014               47.96                0.0             119.50   \n",
       "1015               28.92                0.0              64.09   \n",
       "1016               61.08                0.0             109.13   \n",
       "1017                8.12                0.0             139.23   \n",
       "1018               16.81                0.0              56.20   \n",
       "\n",
       "      ENSG00000000457.9  ENSG00000000460.12  ENSG00000000938.8  \\\n",
       "0                  9.76               24.51               0.01   \n",
       "1                 16.76               13.32               0.00   \n",
       "2                  2.58               10.86               0.00   \n",
       "3                  3.25                5.26               0.00   \n",
       "4                  2.73                9.27               0.02   \n",
       "...                 ...                 ...                ...   \n",
       "1014               5.06                9.58               0.03   \n",
       "1015               4.94               13.35               0.22   \n",
       "1016               5.91               17.40               0.13   \n",
       "1017              15.96               17.45               0.06   \n",
       "1018              13.76               11.53               0.00   \n",
       "\n",
       "      ENSG00000000971.11  ENSG00000001036.9  ENSG00000001084.6  \\\n",
       "0                   0.08              54.86             118.50   \n",
       "1                   0.23             170.91              93.00   \n",
       "2                   0.06              30.78              22.16   \n",
       "3                   0.28              45.05              19.45   \n",
       "4                   0.45              53.29               8.36   \n",
       "...                  ...                ...                ...   \n",
       "1014                0.10             115.62              40.13   \n",
       "1015              139.44             118.19              13.53   \n",
       "1016               53.25              92.96              23.09   \n",
       "1017                1.23              19.75              20.37   \n",
       "1018                0.03              24.06              47.14   \n",
       "\n",
       "      ENSG00000001167.10  ...  ENSGR0000237531.1  ENSGR0000237801.1  \\\n",
       "0                  38.05  ...                0.0                0.0   \n",
       "1                  18.64  ...                0.0                0.0   \n",
       "2                  14.33  ...                0.0                0.0   \n",
       "3                   7.91  ...                0.0                0.0   \n",
       "4                  15.27  ...                0.0                0.0   \n",
       "...                  ...  ...                ...                ...   \n",
       "1014               21.08  ...                0.0                0.0   \n",
       "1015               26.15  ...                0.0                0.0   \n",
       "1016               33.22  ...                0.0                0.0   \n",
       "1017               12.47  ...                0.0                0.0   \n",
       "1018               15.85  ...                0.0                0.0   \n",
       "\n",
       "      ENSGR0000263835.1  ENSGR0000263980.1  ENSGR0000264510.1  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "1014                0.0                0.0                0.0   \n",
       "1015                0.0                0.0                0.0   \n",
       "1016                0.0                0.0                0.0   \n",
       "1017                0.0                0.0                0.0   \n",
       "1018                0.0                0.0                0.0   \n",
       "\n",
       "      ENSGR0000264819.1  ENSGR0000265350.1  ENSGR0000265658.1  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "1014                0.0                0.0                0.0   \n",
       "1015                0.0                0.0                0.0   \n",
       "1016                0.0                0.0                0.0   \n",
       "1017                0.0                0.0                0.0   \n",
       "1018                0.0                0.0                0.0   \n",
       "\n",
       "      ENSGR0000266731.1  ENSGR0000270726.1  \n",
       "0                   0.0                0.0  \n",
       "1                   0.0                0.0  \n",
       "2                   0.0                0.0  \n",
       "3                   0.0                0.0  \n",
       "4                   0.0                0.0  \n",
       "...                 ...                ...  \n",
       "1014                0.0                0.0  \n",
       "1015                0.0                0.0  \n",
       "1016                0.0                0.0  \n",
       "1017                0.0                0.0  \n",
       "1018                0.0                0.0  \n",
       "\n",
       "[1019 rows x 57820 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genes_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_train_t = torch.Tensor(genes_train)\n",
    "genes_test_t = torch.Tensor(genes_test)\n",
    "genes_full = torch.Tensor(genes_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genes_full.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 256\n",
    "lr = 0.05\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(genes_train_t, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(genes_test_t,batch_size=batch_size, shuffle=False)\n",
    "full_loader = DataLoader(genes_full,batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, level_2, level_3, level_4, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.enc_fc1 = nn.Sequential(\n",
    "                        nn.Linear(input_size, level_2),\n",
    "                        nn.BatchNorm1d(level_2),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.enc_fc2 = nn.Sequential(\n",
    "                        nn.Linear(level_2, level_3),\n",
    "                        nn.BatchNorm1d(level_3),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.enc_fc3 = nn.Sequential(\n",
    "                        nn.Linear(level_3, level_4),\n",
    "                        nn.BatchNorm1d(level_4),\n",
    "                        nn.ReLU())\n",
    "\n",
    "        self.enc_fc4_mean = nn.Sequential(\n",
    "                    nn.Linear(level_4, latent_dim),\n",
    "                    nn.BatchNorm1d(latent_dim))\n",
    "        \n",
    "        self.enc_fc4_log_var = nn.Sequential(\n",
    "                    nn.Linear(level_4, latent_dim),\n",
    "                    nn.BatchNorm1d(latent_dim))\n",
    "        \n",
    "        \n",
    "        # Decoder layers\n",
    "        self.dec_fc4 = nn.Sequential(\n",
    "                        nn.Linear(latent_dim, level_4),\n",
    "                        nn.BatchNorm1d(level_4),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.dec_fc3 = nn.Sequential(\n",
    "                        nn.Linear(level_4, level_3),\n",
    "                        nn.BatchNorm1d(level_3),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.dec_fc2 = nn.Sequential(\n",
    "                        nn.Linear(level_3, level_2),\n",
    "                        nn.BatchNorm1d(level_2),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.dec_fc1 = nn.Sequential(\n",
    "                    nn.Linear(level_2, input_size),\n",
    "                    nn.BatchNorm1d(input_size),\n",
    "                    nn.Sigmoid())\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        l2_layer = self.enc_fc1(x)\n",
    "        l3_layer = self.enc_fc2(l2_layer)\n",
    "        l4_layer = self.enc_fc3(l3_layer)\n",
    "        \n",
    "        mu = self.enc_fc4_mean(l4_layer)\n",
    "        logvar = self.enc_fc4_log_var(l4_layer)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        l4_layer = self.dec_fc4(z)\n",
    "        l3_layer = self.dec_fc3(l4_layer)\n",
    "        l2_layer = self.dec_fc2(l3_layer)\n",
    "        x_hat = self.dec_fc1(l2_layer)\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        z = z.to(device)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mu, logvar\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "input_size = 57820 #dimension of gene expressions\n",
    "level_2 = 4096\n",
    "level_3 = 2048\n",
    "level_4 = 1024\n",
    "latent_dim = 512 # target latent size\n",
    "model = VAE(input_size, level_2, level_3, level_4, latent_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Reconstruction Loss and KL Divergence Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_loss(x_hat, x): # Reconstruction Loss\n",
    "        lossFunc = torch.nn.MSELoss()\n",
    "        loss = lossFunc(x_hat, x)\n",
    "        return loss\n",
    "\n",
    "def kl_loss(mean, log_var): # KL Divergence\n",
    "    loss = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_recon = 0.0\n",
    "    total_kl = 0.0\n",
    "    for batch_index, sample in enumerate(dataloader):\n",
    "        data = sample\n",
    "        data = data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_hat, mu, logvar = model(data)\n",
    "        x_hat = x_hat.to(device)\n",
    "\n",
    "        train_recon_loss = recon_loss(x_hat, data)\n",
    "        train_kl_loss = kl_loss(mu, logvar)\n",
    "        loss = (train_recon_loss + train_kl_loss)\n",
    "        total_recon += train_recon_loss.item()\n",
    "        total_kl += train_kl_loss.item()\n",
    "        total_loss +=loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss = total_loss/len(dataloader.dataset)\n",
    "    return train_loss, total_recon/len(dataloader.dataset), total_kl/len(dataloader.dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_kl = 0.0\n",
    "    total_recon = 0.0\n",
    "    with torch.no_grad():\n",
    "        mean_store = torch.zeros(1, latent_dim).to(device)\n",
    "        for batch_index, sample in enumerate(dataloader):\n",
    "            data = sample\n",
    "            data = data.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_hat, mu, logvar = model(data)\n",
    "\n",
    "            val_recon_loss = recon_loss(x_hat, data)\n",
    "            val_kl = kl_loss(mu, logvar)\n",
    "            loss = (val_recon_loss + val_kl)\n",
    "            \n",
    "            total_kl += val_kl.item()\n",
    "            total_recon+= val_recon_loss.item()\n",
    "            total_loss +=loss.item()\n",
    "\n",
    "            mean_store = torch.cat((mean_store, mu), 0)\n",
    "\n",
    "    all_data_mean = mean_store[1:]\n",
    "    all_data_mean_np = all_data_mean.cpu().numpy()\n",
    "\n",
    "    input_path = \"C:/Users/mdzak/Desktop/GitHub/FYP_Zaki/results\"\n",
    "    input_path_name = input_path.split('/')[-1]\n",
    "    latent_space_path = '../results/' + input_path_name + str(latent_dim) + 'D_latent_space_gene_exp.tsv'\n",
    "\n",
    "    all_data_mean_df = pd.DataFrame(all_data_mean_np)\n",
    "    all_data_mean_df.to_csv(latent_space_path, sep='\\t')\n",
    "\n",
    "    val_avg_total_loss = total_loss/len(dataloader.dataset)\n",
    "    val_avg_kl = total_kl/len(dataloader.dataset)\n",
    "    val_avg_recon = total_recon/len(dataloader.dataset)\n",
    "\n",
    "    return val_avg_total_loss, val_avg_kl, val_avg_recon\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 20\n",
      "Training Loss (KL plus MSE): 708.3214\n",
      "Training Loss (MSE): 529.5356\n",
      "Training Loss (KL): 178.7858\n",
      "Val Loss: 2198262902637674775969792.0000\n",
      "Val KL Loss: 2198262902637674775969792.0000\n",
      "Val Recon Loss (MSE): 481.7674\n",
      "\n",
      "Epoch 2 of 20\n",
      "Training Loss (KL plus MSE): 635.1151\n",
      "Training Loss (MSE): 526.2105\n",
      "Training Loss (KL): 108.9046\n",
      "Val Loss: 29271.1119\n",
      "Val KL Loss: 28789.3337\n",
      "Val Recon Loss (MSE): 481.7781\n",
      "\n",
      "Epoch 3 of 20\n",
      "Training Loss (KL plus MSE): 587.6288\n",
      "Training Loss (MSE): 525.9501\n",
      "Training Loss (KL): 61.6787\n",
      "Val Loss: 3269.7618\n",
      "Val KL Loss: 2787.9913\n",
      "Val Recon Loss (MSE): 481.7705\n",
      "\n",
      "Epoch 4 of 20\n",
      "Training Loss (KL plus MSE): 556.3485\n",
      "Training Loss (MSE): 524.1376\n",
      "Training Loss (KL): 32.2109\n",
      "Val Loss: 893.2107\n",
      "Val KL Loss: 411.4673\n",
      "Val Recon Loss (MSE): 481.7434\n",
      "\n",
      "Epoch 5 of 20\n",
      "Training Loss (KL plus MSE): 541.3860\n",
      "Training Loss (MSE): 524.2798\n",
      "Training Loss (KL): 17.1062\n",
      "Val Loss: 592.8197\n",
      "Val KL Loss: 111.1272\n",
      "Val Recon Loss (MSE): 481.6925\n",
      "\n",
      "Epoch 6 of 20\n",
      "Training Loss (KL plus MSE): 534.1114\n",
      "Training Loss (MSE): 522.5768\n",
      "Training Loss (KL): 11.5345\n",
      "Val Loss: 538.8031\n",
      "Val KL Loss: 57.1127\n",
      "Val Recon Loss (MSE): 481.6905\n",
      "\n",
      "Epoch 7 of 20\n",
      "Training Loss (KL plus MSE): 537.5011\n",
      "Training Loss (MSE): 527.1660\n",
      "Training Loss (KL): 10.3351\n",
      "Val Loss: 522.0738\n",
      "Val KL Loss: 40.3824\n",
      "Val Recon Loss (MSE): 481.6914\n",
      "\n",
      "Epoch 8 of 20\n",
      "Training Loss (KL plus MSE): 534.7275\n",
      "Training Loss (MSE): 524.3978\n",
      "Training Loss (KL): 10.3297\n",
      "Val Loss: 511.1749\n",
      "Val KL Loss: 29.4827\n",
      "Val Recon Loss (MSE): 481.6922\n",
      "\n",
      "Epoch 9 of 20\n",
      "Training Loss (KL plus MSE): 535.8702\n",
      "Training Loss (MSE): 526.4773\n",
      "Training Loss (KL): 9.3929\n",
      "Val Loss: 502.0140\n",
      "Val KL Loss: 20.3229\n",
      "Val Recon Loss (MSE): 481.6911\n",
      "\n",
      "Epoch 10 of 20\n",
      "Training Loss (KL plus MSE): 530.2129\n",
      "Training Loss (MSE): 522.3593\n",
      "Training Loss (KL): 7.8537\n",
      "Val Loss: 493.3292\n",
      "Val KL Loss: 11.6427\n",
      "Val Recon Loss (MSE): 481.6865\n",
      "\n",
      "Epoch 11 of 20\n",
      "Training Loss (KL plus MSE): 539.4445\n",
      "Training Loss (MSE): 533.6389\n",
      "Training Loss (KL): 5.8056\n",
      "Val Loss: 488.9341\n",
      "Val KL Loss: 7.2517\n",
      "Val Recon Loss (MSE): 481.6824\n",
      "\n",
      "Epoch 12 of 20\n",
      "Training Loss (KL plus MSE): 532.4745\n",
      "Training Loss (MSE): 528.4618\n",
      "Training Loss (KL): 4.0126\n",
      "Val Loss: 486.2560\n",
      "Val KL Loss: 4.5802\n",
      "Val Recon Loss (MSE): 481.6757\n",
      "\n",
      "Epoch 13 of 20\n",
      "Training Loss (KL plus MSE): 528.2537\n",
      "Training Loss (MSE): 525.2224\n",
      "Training Loss (KL): 3.0313\n",
      "Val Loss: 485.1743\n",
      "Val KL Loss: 3.5021\n",
      "Val Recon Loss (MSE): 481.6722\n",
      "\n",
      "Epoch 14 of 20\n",
      "Training Loss (KL plus MSE): 525.7117\n",
      "Training Loss (MSE): 522.9960\n",
      "Training Loss (KL): 2.7156\n",
      "Val Loss: 484.9214\n",
      "Val KL Loss: 3.2495\n",
      "Val Recon Loss (MSE): 481.6719\n",
      "\n",
      "Epoch 15 of 20\n",
      "Training Loss (KL plus MSE): 527.2829\n",
      "Training Loss (MSE): 524.7628\n",
      "Training Loss (KL): 2.5201\n",
      "Val Loss: 484.5780\n",
      "Val KL Loss: 2.9077\n",
      "Val Recon Loss (MSE): 481.6703\n",
      "\n",
      "Epoch 16 of 20\n",
      "Training Loss (KL plus MSE): 526.0488\n",
      "Training Loss (MSE): 523.6446\n",
      "Training Loss (KL): 2.4042\n",
      "Val Loss: 484.3358\n",
      "Val KL Loss: 2.6644\n",
      "Val Recon Loss (MSE): 481.6713\n",
      "\n",
      "Epoch 17 of 20\n",
      "Training Loss (KL plus MSE): 528.6706\n",
      "Training Loss (MSE): 526.5303\n",
      "Training Loss (KL): 2.1403\n",
      "Val Loss: 483.9933\n",
      "Val KL Loss: 2.3199\n",
      "Val Recon Loss (MSE): 481.6735\n",
      "\n",
      "Epoch 18 of 20\n",
      "Training Loss (KL plus MSE): 529.6653\n",
      "Training Loss (MSE): 527.7423\n",
      "Training Loss (KL): 1.9230\n",
      "Val Loss: 483.6703\n",
      "Val KL Loss: 1.9982\n",
      "Val Recon Loss (MSE): 481.6721\n",
      "\n",
      "Epoch 19 of 20\n",
      "Training Loss (KL plus MSE): 524.9780\n",
      "Training Loss (MSE): 523.1887\n",
      "Training Loss (KL): 1.7893\n",
      "Val Loss: 483.3477\n",
      "Val KL Loss: 1.6788\n",
      "Val Recon Loss (MSE): 481.6689\n",
      "\n",
      "Epoch 20 of 20\n",
      "Training Loss (KL plus MSE): 529.5173\n",
      "Training Loss (MSE): 527.9928\n",
      "Training Loss (KL): 1.5246\n",
      "Val Loss: 483.2074\n",
      "Val KL Loss: 1.5391\n",
      "Val Recon Loss (MSE): 481.6682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss, train_recons_loss, train_kl_loss = train(model, train_loader)\n",
    "    val_epoch_loss, val_kl_loss, val_recon_loss = validate(model, full_loader)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    print(f\"Training Loss (KL plus MSE): {train_epoch_loss:.4f}\")\n",
    "    print(f\"Training Loss (MSE): {train_recons_loss:.4f}\")\n",
    "    print(f\"Training Loss (KL): {train_kl_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_epoch_loss:.4f}\")\n",
    "    print(f\"Val KL Loss: {val_kl_loss:.4f}\")\n",
    "    print(f\"Val Recon Loss (MSE): {val_recon_loss:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
