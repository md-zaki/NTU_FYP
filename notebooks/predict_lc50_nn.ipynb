{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mdzak\\anaconda3\\envs\\fyp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import matplotlib\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "matplotlib.style.use('ggplot')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dim = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = pd.read_csv('../results/results' + str(selected_dim) + 'D_latent_space_gene_exp.tsv',sep='\\t')\n",
    "cell_line_name = pd.read_csv('../results_clean/cell_line_name.csv')\n",
    "gdsc_drug = pd.read_csv('../results_clean/gdsc_drug_nodash.csv')\n",
    "dimension.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "gdsc_drug.drop(columns=['Unnamed: 0'], inplace= True)\n",
    "dimension['CELL_LINE_NAME'] = cell_line_name['CELL_LINE_NAME']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with drugs and one hot encode drug name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_w_drug = pd.merge(dimension, gdsc_drug, on='CELL_LINE_NAME')\n",
    "dimension_w_drug.drop(columns=['CELL_LINE_NAME'],inplace=True)\n",
    "dimension_w_drug = pd.get_dummies(dimension_w_drug, columns=['DRUG_NAME'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "col = dimension_w_drug.drop(columns=['LN_IC50']).columns\n",
    "X = scaler.fit_transform(dimension_w_drug.drop(columns=['LN_IC50']))\n",
    "X = pd.DataFrame(X, columns=col)\n",
    "y = dimension_w_drug['LN_IC50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# col = dimension_w_drug.drop(columns=['LN_IC50']).columns\n",
    "# X = dimension_w_drug.drop(columns=['LN_IC50'])\n",
    "# y = dimension_w_drug['LN_IC50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>DRUG_NAME_WZ4003</th>\n",
       "      <th>DRUG_NAME_Wee1 Inhibitor</th>\n",
       "      <th>DRUG_NAME_Wnt-C59</th>\n",
       "      <th>DRUG_NAME_XAV939</th>\n",
       "      <th>DRUG_NAME_YK-4-279</th>\n",
       "      <th>DRUG_NAME_ZM447439</th>\n",
       "      <th>DRUG_NAME_Zoledronate</th>\n",
       "      <th>DRUG_NAME_alpha-lipoic acid</th>\n",
       "      <th>DRUG_NAME_ascorbate (vitamin C)</th>\n",
       "      <th>DRUG_NAME_glutathione</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.098109</td>\n",
       "      <td>0.973217</td>\n",
       "      <td>0.934634</td>\n",
       "      <td>0.082488</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>0.057198</td>\n",
       "      <td>0.040924</td>\n",
       "      <td>0.983428</td>\n",
       "      <td>0.953679</td>\n",
       "      <td>0.993825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.098109</td>\n",
       "      <td>0.973217</td>\n",
       "      <td>0.934634</td>\n",
       "      <td>0.082488</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>0.057198</td>\n",
       "      <td>0.040924</td>\n",
       "      <td>0.983428</td>\n",
       "      <td>0.953679</td>\n",
       "      <td>0.993825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.098109</td>\n",
       "      <td>0.973217</td>\n",
       "      <td>0.934634</td>\n",
       "      <td>0.082488</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>0.057198</td>\n",
       "      <td>0.040924</td>\n",
       "      <td>0.983428</td>\n",
       "      <td>0.953679</td>\n",
       "      <td>0.993825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.098109</td>\n",
       "      <td>0.973217</td>\n",
       "      <td>0.934634</td>\n",
       "      <td>0.082488</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>0.057198</td>\n",
       "      <td>0.040924</td>\n",
       "      <td>0.983428</td>\n",
       "      <td>0.953679</td>\n",
       "      <td>0.993825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.098109</td>\n",
       "      <td>0.973217</td>\n",
       "      <td>0.934634</td>\n",
       "      <td>0.082488</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>0.057198</td>\n",
       "      <td>0.040924</td>\n",
       "      <td>0.983428</td>\n",
       "      <td>0.953679</td>\n",
       "      <td>0.993825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157613</th>\n",
       "      <td>0.096684</td>\n",
       "      <td>0.982266</td>\n",
       "      <td>0.919321</td>\n",
       "      <td>0.076251</td>\n",
       "      <td>0.038997</td>\n",
       "      <td>0.069472</td>\n",
       "      <td>0.093618</td>\n",
       "      <td>0.937792</td>\n",
       "      <td>0.913322</td>\n",
       "      <td>0.953113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157614</th>\n",
       "      <td>0.096684</td>\n",
       "      <td>0.982266</td>\n",
       "      <td>0.919321</td>\n",
       "      <td>0.076251</td>\n",
       "      <td>0.038997</td>\n",
       "      <td>0.069472</td>\n",
       "      <td>0.093618</td>\n",
       "      <td>0.937792</td>\n",
       "      <td>0.913322</td>\n",
       "      <td>0.953113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157615</th>\n",
       "      <td>0.096684</td>\n",
       "      <td>0.982266</td>\n",
       "      <td>0.919321</td>\n",
       "      <td>0.076251</td>\n",
       "      <td>0.038997</td>\n",
       "      <td>0.069472</td>\n",
       "      <td>0.093618</td>\n",
       "      <td>0.937792</td>\n",
       "      <td>0.913322</td>\n",
       "      <td>0.953113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157616</th>\n",
       "      <td>0.096684</td>\n",
       "      <td>0.982266</td>\n",
       "      <td>0.919321</td>\n",
       "      <td>0.076251</td>\n",
       "      <td>0.038997</td>\n",
       "      <td>0.069472</td>\n",
       "      <td>0.093618</td>\n",
       "      <td>0.937792</td>\n",
       "      <td>0.913322</td>\n",
       "      <td>0.953113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157617</th>\n",
       "      <td>0.096684</td>\n",
       "      <td>0.982266</td>\n",
       "      <td>0.919321</td>\n",
       "      <td>0.076251</td>\n",
       "      <td>0.038997</td>\n",
       "      <td>0.069472</td>\n",
       "      <td>0.093618</td>\n",
       "      <td>0.937792</td>\n",
       "      <td>0.913322</td>\n",
       "      <td>0.953113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157618 rows × 414 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0       0.098109  0.973217  0.934634  0.082488  0.003494  0.057198  0.040924   \n",
       "1       0.098109  0.973217  0.934634  0.082488  0.003494  0.057198  0.040924   \n",
       "2       0.098109  0.973217  0.934634  0.082488  0.003494  0.057198  0.040924   \n",
       "3       0.098109  0.973217  0.934634  0.082488  0.003494  0.057198  0.040924   \n",
       "4       0.098109  0.973217  0.934634  0.082488  0.003494  0.057198  0.040924   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "157613  0.096684  0.982266  0.919321  0.076251  0.038997  0.069472  0.093618   \n",
       "157614  0.096684  0.982266  0.919321  0.076251  0.038997  0.069472  0.093618   \n",
       "157615  0.096684  0.982266  0.919321  0.076251  0.038997  0.069472  0.093618   \n",
       "157616  0.096684  0.982266  0.919321  0.076251  0.038997  0.069472  0.093618   \n",
       "157617  0.096684  0.982266  0.919321  0.076251  0.038997  0.069472  0.093618   \n",
       "\n",
       "               7         8         9  ...  DRUG_NAME_WZ4003  \\\n",
       "0       0.983428  0.953679  0.993825  ...               0.0   \n",
       "1       0.983428  0.953679  0.993825  ...               0.0   \n",
       "2       0.983428  0.953679  0.993825  ...               0.0   \n",
       "3       0.983428  0.953679  0.993825  ...               0.0   \n",
       "4       0.983428  0.953679  0.993825  ...               0.0   \n",
       "...          ...       ...       ...  ...               ...   \n",
       "157613  0.937792  0.913322  0.953113  ...               0.0   \n",
       "157614  0.937792  0.913322  0.953113  ...               0.0   \n",
       "157615  0.937792  0.913322  0.953113  ...               0.0   \n",
       "157616  0.937792  0.913322  0.953113  ...               0.0   \n",
       "157617  0.937792  0.913322  0.953113  ...               0.0   \n",
       "\n",
       "        DRUG_NAME_Wee1 Inhibitor  DRUG_NAME_Wnt-C59  DRUG_NAME_XAV939  \\\n",
       "0                            0.0                0.0               0.0   \n",
       "1                            0.0                0.0               0.0   \n",
       "2                            0.0                0.0               0.0   \n",
       "3                            0.0                0.0               0.0   \n",
       "4                            0.0                0.0               0.0   \n",
       "...                          ...                ...               ...   \n",
       "157613                       0.0                0.0               0.0   \n",
       "157614                       0.0                0.0               0.0   \n",
       "157615                       0.0                0.0               0.0   \n",
       "157616                       0.0                0.0               0.0   \n",
       "157617                       0.0                0.0               0.0   \n",
       "\n",
       "        DRUG_NAME_YK-4-279  DRUG_NAME_ZM447439  DRUG_NAME_Zoledronate  \\\n",
       "0                      0.0                 0.0                    0.0   \n",
       "1                      0.0                 0.0                    0.0   \n",
       "2                      0.0                 0.0                    0.0   \n",
       "3                      0.0                 0.0                    0.0   \n",
       "4                      0.0                 0.0                    0.0   \n",
       "...                    ...                 ...                    ...   \n",
       "157613                 0.0                 0.0                    0.0   \n",
       "157614                 0.0                 0.0                    0.0   \n",
       "157615                 0.0                 0.0                    0.0   \n",
       "157616                 0.0                 0.0                    0.0   \n",
       "157617                 0.0                 0.0                    0.0   \n",
       "\n",
       "        DRUG_NAME_alpha-lipoic acid  DRUG_NAME_ascorbate (vitamin C)  \\\n",
       "0                               0.0                              0.0   \n",
       "1                               0.0                              0.0   \n",
       "2                               0.0                              0.0   \n",
       "3                               0.0                              0.0   \n",
       "4                               0.0                              0.0   \n",
       "...                             ...                              ...   \n",
       "157613                          0.0                              0.0   \n",
       "157614                          0.0                              1.0   \n",
       "157615                          0.0                              0.0   \n",
       "157616                          1.0                              0.0   \n",
       "157617                          0.0                              0.0   \n",
       "\n",
       "        DRUG_NAME_glutathione  \n",
       "0                         0.0  \n",
       "1                         0.0  \n",
       "2                         0.0  \n",
       "3                         0.0  \n",
       "4                         0.0  \n",
       "...                       ...  \n",
       "157613                    0.0  \n",
       "157614                    0.0  \n",
       "157615                    1.0  \n",
       "157616                    0.0  \n",
       "157617                    0.0  \n",
       "\n",
       "[157618 rows x 414 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         -2.902460\n",
       "1         -4.384381\n",
       "2          3.372862\n",
       "3          4.189364\n",
       "4         -3.693711\n",
       "            ...    \n",
       "157613     6.556376\n",
       "157614    12.036685\n",
       "157615    11.217061\n",
       "157616     8.823341\n",
       "157617     9.995125\n",
       "Name: LN_IC50, Length: 157618, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTOR (K Fold Cross Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# model = xgb.XGBRegressor()\n",
    "# cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "# scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "# scores = abs(scores)\n",
    "# print('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTOR (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,df):\n",
    " \n",
    "        col = df.drop(columns=['LN_IC50']).columns\n",
    "        x = scaler.fit_transform(df.drop(columns=['LN_IC50']))\n",
    "        x = pd.DataFrame(X, columns=col)\n",
    "        y = df['LN_IC50']\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "    \n",
    "        self.x_train=torch.tensor(x,dtype=torch.float32)\n",
    "        self.y_train=torch.tensor(y,dtype=torch.float32)\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "   \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_train[idx],self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(dimension_w_drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn as nn\n",
    " \n",
    "class predictor_model(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "    nn.Linear(414, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "   if hasattr(layer, 'reset_parameters'):\n",
    "    print(f'Reset trainable parameters of layer = {layer}')\n",
    "    layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds = 5\n",
    "n_epochs = 20\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "results = {}\n",
    "torch.manual_seed(3)\n",
    "\n",
    "kfold = KFold(n_splits=k_folds, shuffle = True)\n",
    "batch = 128\n",
    "best_mse = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Linear(in_features=414, out_features=200, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=200, out_features=1, bias=True)\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mdzak\\anaconda3\\envs\\fyp\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch: 8.048633575439453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mdzak\\anaconda3\\envs\\fyp\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 2\n",
      "Loss after mini-batch: 5.92170524597168\n",
      "Starting epoch 3\n",
      "Loss after mini-batch: 7.3650994300842285\n",
      "Starting epoch 4\n",
      "Loss after mini-batch: 9.361347198486328\n",
      "Starting epoch 5\n",
      "Loss after mini-batch: 8.78515338897705\n",
      "Starting epoch 6\n",
      "Loss after mini-batch: 5.473708629608154\n",
      "Starting epoch 7\n",
      "Loss after mini-batch: 6.977084159851074\n",
      "Starting epoch 8\n",
      "Loss after mini-batch: 5.985025405883789\n",
      "Starting epoch 9\n",
      "Loss after mini-batch: 9.056744575500488\n",
      "Starting epoch 10\n",
      "Loss after mini-batch: 6.7856950759887695\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Start print\n",
    "print('--------------------------------')\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=batch, sampler=train_subsampler)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=batch, sampler=test_subsampler)\n",
    "    \n",
    "    # Init the neural network\n",
    "    model = predictor_model()\n",
    "    model = model.to(device)\n",
    "    model.apply(reset_weights)\n",
    "    best_mse = np.inf\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr= 0.02)\n",
    "\n",
    "    # start training\n",
    "    for epoch in range(0, n_epochs):\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "        model.train()\n",
    "        # Set current loss value\n",
    "        current_loss = 0.0\n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, targets = data\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Print statistics\n",
    "            current_loss = loss.item()\n",
    "            if i % 500 == 499:\n",
    "                print('Loss after mini-batch: ' + str(current_loss))\n",
    "            current_loss = 0\n",
    "\n",
    "\n",
    "        # Process is complete.\n",
    "    print('Training process has finished. Saving trained model.')\n",
    "    # Saving the model\n",
    "    # save_path = f'./model-fold-{fold}.pth'\n",
    "    # torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    # Print about testing\n",
    "    print('Starting testing')\n",
    "\n",
    "\n",
    "    # Evaluation for this fold\n",
    "    with torch.no_grad():\n",
    "\n",
    "      # Iterate over the test data and generate predictions\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "        # Get inputs\n",
    "            inputs, targets = data\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            val_loss = loss_function(outputs, targets)\n",
    "            val_loss = float(val_loss)\n",
    "            history.append(val_loss)\n",
    "            if val_loss < best_mse:\n",
    "                best_mse = val_loss\n",
    "                best_weights = copy.deepcopy(model.state_dict())\n",
    "        print('Best MSE for fold ' + str(fold) + ' is: ' + str(best_mse))\n",
    "        results[fold] = best_mse\n",
    "\n",
    "# Print fold results\n",
    "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "sum = 0.0\n",
    "for key, value in results.items():\n",
    "    print(f'Fold {key}: {value} %')\n",
    "    sum += value\n",
    "print(f'Average: {sum/len(results.items())}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_np = np.array(dimension_w_drug)\n",
    "dim_col = dimension_w_drug.columns\n",
    "train, test = train_test_split(dim_np, train_size=0.8, shuffle=True, random_state=0)\n",
    "train = pd.DataFrame(train, columns=dim_col)\n",
    "test = pd.DataFrame(test, columns=dim_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train)\n",
    "test_dataset = MyDataset(test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch     1: 0.028\n",
      "Loss after mini-batch   129: 1.958\n",
      "Loss after mini-batch   257: 1.982\n",
      "Loss after mini-batch   385: 2.011\n",
      "Loss after mini-batch   513: 1.995\n",
      "Loss after mini-batch   641: 1.933\n",
      "Loss after mini-batch   769: 1.957\n",
      "Loss after mini-batch   897: 1.996\n",
      "Starting epoch 2\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   129: 1.891\n",
      "Loss after mini-batch   257: 1.966\n",
      "Loss after mini-batch   385: 1.974\n",
      "Loss after mini-batch   513: 1.996\n",
      "Loss after mini-batch   641: 1.939\n",
      "Loss after mini-batch   769: 1.954\n",
      "Loss after mini-batch   897: 1.989\n",
      "Starting epoch 3\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   129: 1.885\n",
      "Loss after mini-batch   257: 1.960\n",
      "Loss after mini-batch   385: 1.965\n",
      "Loss after mini-batch   513: 1.991\n",
      "Loss after mini-batch   641: 1.930\n",
      "Loss after mini-batch   769: 1.951\n",
      "Loss after mini-batch   897: 1.984\n",
      "Starting epoch 4\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   129: 1.877\n",
      "Loss after mini-batch   257: 1.955\n",
      "Loss after mini-batch   385: 1.959\n",
      "Loss after mini-batch   513: 1.984\n",
      "Loss after mini-batch   641: 1.922\n",
      "Loss after mini-batch   769: 1.949\n",
      "Loss after mini-batch   897: 1.983\n",
      "Starting epoch 5\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   129: 1.873\n",
      "Loss after mini-batch   257: 1.953\n",
      "Loss after mini-batch   385: 1.957\n",
      "Loss after mini-batch   513: 1.984\n",
      "Loss after mini-batch   641: 1.920\n",
      "Loss after mini-batch   769: 1.950\n",
      "Loss after mini-batch   897: 1.983\n",
      "Starting epoch 6\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   129: 1.873\n",
      "Loss after mini-batch   257: 1.954\n",
      "Loss after mini-batch   385: 1.960\n",
      "Loss after mini-batch   513: 1.983\n",
      "Loss after mini-batch   641: 1.920\n",
      "Loss after mini-batch   769: 1.950\n",
      "Loss after mini-batch   897: 1.983\n",
      "Starting epoch 7\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   129: 1.873\n",
      "Loss after mini-batch   257: 1.954\n",
      "Loss after mini-batch   385: 1.960\n",
      "Loss after mini-batch   513: 1.983\n",
      "Loss after mini-batch   641: 1.920\n",
      "Loss after mini-batch   769: 1.950\n",
      "Loss after mini-batch   897: 1.983\n",
      "Starting epoch 8\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   129: 1.873\n",
      "Loss after mini-batch   257: 1.954\n",
      "Loss after mini-batch   385: 1.960\n",
      "Loss after mini-batch   513: 1.983\n",
      "Loss after mini-batch   641: 1.920\n",
      "Loss after mini-batch   769: 1.950\n",
      "Loss after mini-batch   897: 1.983\n",
      "Starting epoch 9\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   129: 1.873\n",
      "Loss after mini-batch   257: 1.954\n",
      "Loss after mini-batch   385: 1.960\n",
      "Loss after mini-batch   513: 1.983\n",
      "Loss after mini-batch   641: 1.920\n",
      "Loss after mini-batch   769: 1.950\n",
      "Loss after mini-batch   897: 1.983\n",
      "Starting epoch 10\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   129: 1.873\n",
      "Loss after mini-batch   257: 1.954\n",
      "Loss after mini-batch   385: 1.960\n",
      "Loss after mini-batch   513: 1.983\n",
      "Loss after mini-batch   641: 1.920\n",
      "Loss after mini-batch   769: 1.950\n",
      "Loss after mini-batch   897: 1.983\n",
      "Starting epoch 11\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   129: 1.873\n",
      "Loss after mini-batch   257: 1.954\n",
      "Loss after mini-batch   385: 1.960\n",
      "Loss after mini-batch   513: 1.983\n",
      "Loss after mini-batch   641: 1.920\n",
      "Loss after mini-batch   769: 1.950\n",
      "Loss after mini-batch   897: 1.983\n",
      "Starting epoch 12\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   129: 1.873\n",
      "Loss after mini-batch   257: 1.954\n",
      "Loss after mini-batch   385: 1.960\n",
      "Loss after mini-batch   513: 1.983\n",
      "Loss after mini-batch   641: 1.920\n",
      "Loss after mini-batch   769: 1.950\n",
      "Loss after mini-batch   897: 1.983\n",
      "Starting epoch 13\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   129: 1.873\n",
      "Loss after mini-batch   257: 1.954\n",
      "Loss after mini-batch   385: 1.960\n",
      "Loss after mini-batch   513: 1.983\n",
      "Loss after mini-batch   641: 1.920\n",
      "Loss after mini-batch   769: 1.950\n",
      "Loss after mini-batch   897: 1.983\n",
      "Starting epoch 14\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   129: 1.873\n",
      "Loss after mini-batch   257: 1.954\n",
      "Loss after mini-batch   385: 1.960\n",
      "Loss after mini-batch   513: 1.983\n",
      "Loss after mini-batch   641: 1.920\n",
      "Loss after mini-batch   769: 1.950\n",
      "Loss after mini-batch   897: 1.983\n",
      "Starting epoch 15\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   129: 1.873\n",
      "Loss after mini-batch   257: 1.954\n",
      "Loss after mini-batch   385: 1.960\n",
      "Loss after mini-batch   513: 1.983\n",
      "Loss after mini-batch   641: 1.920\n",
      "Loss after mini-batch   769: 1.950\n",
      "Loss after mini-batch   897: 1.983\n",
      "Starting epoch 16\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   129: 1.873\n",
      "Loss after mini-batch   257: 1.954\n",
      "Loss after mini-batch   385: 1.960\n",
      "Loss after mini-batch   513: 1.983\n",
      "Loss after mini-batch   641: 1.920\n",
      "Loss after mini-batch   769: 1.950\n",
      "Loss after mini-batch   897: 1.983\n",
      "Starting epoch 17\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   129: 1.873\n",
      "Loss after mini-batch   257: 1.954\n",
      "Loss after mini-batch   385: 1.960\n",
      "Loss after mini-batch   513: 1.983\n",
      "Loss after mini-batch   641: 1.920\n",
      "Loss after mini-batch   769: 1.950\n",
      "Loss after mini-batch   897: 1.983\n",
      "Starting epoch 18\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   129: 1.873\n",
      "Loss after mini-batch   257: 1.954\n",
      "Loss after mini-batch   385: 1.960\n",
      "Loss after mini-batch   513: 1.983\n",
      "Loss after mini-batch   641: 1.920\n",
      "Loss after mini-batch   769: 1.950\n",
      "Loss after mini-batch   897: 1.983\n",
      "Starting epoch 19\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   129: 1.873\n",
      "Loss after mini-batch   257: 1.954\n",
      "Loss after mini-batch   385: 1.960\n",
      "Loss after mini-batch   513: 1.983\n",
      "Loss after mini-batch   641: 1.920\n",
      "Loss after mini-batch   769: 1.950\n",
      "Loss after mini-batch   897: 1.983\n",
      "Starting epoch 20\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   129: 1.873\n",
      "Loss after mini-batch   257: 1.954\n",
      "Loss after mini-batch   385: 1.960\n",
      "Loss after mini-batch   513: 1.983\n",
      "Loss after mini-batch   641: 1.920\n",
      "Loss after mini-batch   769: 1.950\n",
      "Loss after mini-batch   897: 1.983\n"
     ]
    }
   ],
   "source": [
    "model = predictor_model()\n",
    "model = model.to(device)\n",
    "best_mse = np.inf\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.02)\n",
    "\n",
    "# start training\n",
    "for epoch in range(0, n_epochs):\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "    model.train()\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        x, y = data\n",
    "        x, y = x.float(), y.float()\n",
    "        y = y.reshape((y.shape[0],1))\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_function(y_pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 128 == 0:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.78\n",
      "RMSE: 1.67\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"MSE: %.2f\" % best_mse)\n",
    "print(\"RMSE: %.2f\" % np.sqrt(best_mse))\n",
    "plt.plot(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
