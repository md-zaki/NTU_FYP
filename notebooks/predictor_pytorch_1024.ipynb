{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mdzak\\anaconda3\\envs\\fyp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import matplotlib\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "matplotlib.style.use('ggplot')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "import copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import CSV and merge with drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dim = 1024\n",
    "dimension = pd.read_csv('../results/results' + str(selected_dim) + 'D_latent_space_gene_exp.tsv',sep='\\t')\n",
    "cell_line_name = pd.read_csv('../results_clean/cell_line_name.csv')\n",
    "gdsc_drug = pd.read_csv('../results_clean/gdsc_drug_nodash.csv')\n",
    "dimension.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "gdsc_drug.drop(columns=['Unnamed: 0'], inplace= True)\n",
    "dimension['CELL_LINE_NAME'] = cell_line_name['CELL_LINE_NAME']\n",
    "dimension_w_drug = pd.merge(dimension, gdsc_drug, on='CELL_LINE_NAME')\n",
    "dimension_w_drug.drop(columns=['CELL_LINE_NAME'],inplace=True)\n",
    "# dimension_w_drug = pd.get_dummies(dimension_w_drug, columns=['DRUG_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "      <th>LN_IC50</th>\n",
       "      <th>DRUG_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-1.947630e-06</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>8.596951e-06</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>1.384662e-06</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-7.559261e-07</td>\n",
       "      <td>2.686879</td>\n",
       "      <td>LGK974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>3.244916e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-5.632875e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>3.438464e-06</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>4.930011e-06</td>\n",
       "      <td>5.515893</td>\n",
       "      <td>5-azacytidine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>1.274196e-05</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-7.977654e-06</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.155247e-05</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>1.835189e-05</td>\n",
       "      <td>2.400499</td>\n",
       "      <td>Sorafenib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-5.588725e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-4.927579e-07</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-6.613679e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-1.228852e-05</td>\n",
       "      <td>3.499021</td>\n",
       "      <td>PRT062607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>1.201331e-05</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-1.074133e-05</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>9.274476e-06</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>1.606120e-05</td>\n",
       "      <td>2.639762</td>\n",
       "      <td>AZD5438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157613</th>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>8.162905e-06</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-7.332826e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>2.017715e-05</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>4.866277e-06</td>\n",
       "      <td>2.482369</td>\n",
       "      <td>AGK2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157614</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-5.281099e-07</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-2.925420e-06</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>4.455080e-06</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-1.907468e-06</td>\n",
       "      <td>2.919147</td>\n",
       "      <td>Cisplatin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157615</th>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>5.563005e-06</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-8.932820e-06</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.004996e-05</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>1.334167e-05</td>\n",
       "      <td>4.440038</td>\n",
       "      <td>Serdemetan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157616</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>1.268423e-05</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-7.991030e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>8.460090e-06</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>1.755648e-05</td>\n",
       "      <td>3.617427</td>\n",
       "      <td>ABT737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157617</th>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>1.257368e-05</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-1.011486e-05</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>9.745207e-06</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>1.762885e-05</td>\n",
       "      <td>4.505697</td>\n",
       "      <td>Mirin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157618 rows Ã— 1026 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3             4         5  \\\n",
       "0       0.000003 -0.000005 -0.000004 -0.000004 -1.947630e-06 -0.000007   \n",
       "1      -0.000005  0.000006  0.000006  0.000006  3.244916e-06  0.000006   \n",
       "2      -0.000003  0.000008  0.000009  0.000009  1.274196e-05  0.000009   \n",
       "3      -0.000003  0.000004  0.000005  0.000003 -5.588725e-07  0.000004   \n",
       "4      -0.000007  0.000011  0.000012  0.000010  1.201331e-05  0.000011   \n",
       "...          ...       ...       ...       ...           ...       ...   \n",
       "157613 -0.000007  0.000008  0.000008  0.000008  8.162905e-06  0.000009   \n",
       "157614 -0.000003  0.000005  0.000003  0.000004 -5.281099e-07  0.000003   \n",
       "157615 -0.000006  0.000008  0.000007  0.000008  5.563005e-06  0.000008   \n",
       "157616 -0.000003  0.000007  0.000007  0.000009  1.268423e-05  0.000008   \n",
       "157617 -0.000006  0.000009  0.000011  0.000010  1.257368e-05  0.000010   \n",
       "\n",
       "               6         7         8         9  ...      1016          1017  \\\n",
       "0       0.000004  0.000005 -0.000008  0.000006  ... -0.000008  8.596951e-06   \n",
       "1      -0.000008 -0.000007  0.000009 -0.000007  ...  0.000006 -5.632875e-06   \n",
       "2      -0.000005 -0.000008  0.000009 -0.000008  ...  0.000008 -7.977654e-06   \n",
       "3      -0.000008 -0.000005  0.000007 -0.000005  ...  0.000003 -4.927579e-07   \n",
       "4      -0.000009 -0.000010  0.000014 -0.000011  ...  0.000011 -1.074133e-05   \n",
       "...          ...       ...       ...       ...  ...       ...           ...   \n",
       "157613 -0.000010 -0.000008  0.000013 -0.000008  ...  0.000009 -7.332826e-06   \n",
       "157614 -0.000007 -0.000005  0.000007 -0.000005  ...  0.000004 -2.925420e-06   \n",
       "157615 -0.000009 -0.000008  0.000011 -0.000008  ...  0.000008 -8.932820e-06   \n",
       "157616 -0.000004 -0.000007  0.000008 -0.000007  ...  0.000007 -7.991030e-06   \n",
       "157617 -0.000008 -0.000010  0.000012 -0.000010  ...  0.000010 -1.011486e-05   \n",
       "\n",
       "            1018      1019      1020          1021      1022          1023  \\\n",
       "0      -0.000004  0.000007 -0.000006  1.384662e-06 -0.000009 -7.559261e-07   \n",
       "1       0.000003 -0.000008  0.000006  3.438464e-06 -0.000010  4.930011e-06   \n",
       "2       0.000004 -0.000010  0.000008  1.155247e-05 -0.000020  1.835189e-05   \n",
       "3       0.000003 -0.000006  0.000004 -6.613679e-07  0.000002 -1.228852e-05   \n",
       "4       0.000011 -0.000013  0.000010  9.274476e-06 -0.000015  1.606120e-05   \n",
       "...          ...       ...       ...           ...       ...           ...   \n",
       "157613  0.000006 -0.000010  0.000009  2.017715e-05 -0.000010  4.866277e-06   \n",
       "157614  0.000004 -0.000005  0.000005  4.455080e-06 -0.000010 -1.907468e-06   \n",
       "157615  0.000005 -0.000009  0.000008  1.004996e-05 -0.000013  1.334167e-05   \n",
       "157616  0.000003 -0.000008  0.000007  8.460090e-06 -0.000029  1.755648e-05   \n",
       "157617  0.000006 -0.000012  0.000009  9.745207e-06 -0.000023  1.762885e-05   \n",
       "\n",
       "         LN_IC50      DRUG_NAME  \n",
       "0       2.686879         LGK974  \n",
       "1       5.515893  5-azacytidine  \n",
       "2       2.400499      Sorafenib  \n",
       "3       3.499021      PRT062607  \n",
       "4       2.639762        AZD5438  \n",
       "...          ...            ...  \n",
       "157613  2.482369           AGK2  \n",
       "157614  2.919147      Cisplatin  \n",
       "157615  4.440038     Serdemetan  \n",
       "157616  3.617427         ABT737  \n",
       "157617  4.505697          Mirin  \n",
       "\n",
       "[157618 rows x 1026 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension_w_drug = dimension_w_drug.sample(frac=1, random_state=33).reset_index(drop=True)\n",
    "dimension_w_drug"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate continuous and categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['DRUG_NAME']\n",
    "cont_cols = dimension_w_drug.drop(columns=['DRUG_NAME', 'LN_IC50']).columns\n",
    "label_cols = ['LN_IC50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
       "       ...\n",
       "       '1014', '1015', '1016', '1017', '1018', '1019', '1020', '1021', '1022',\n",
       "       '1023'],\n",
       "      dtype='object', length=1024)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DRUG_NAME']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in cat_cols:\n",
    "    dimension_w_drug[cat] = dimension_w_drug[cat].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             float64\n",
       "1             float64\n",
       "2             float64\n",
       "3             float64\n",
       "4             float64\n",
       "               ...   \n",
       "1021          float64\n",
       "1022          float64\n",
       "1023          float64\n",
       "LN_IC50       float64\n",
       "DRUG_NAME    category\n",
       "Length: 1026, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension_w_drug.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           LGK974\n",
       "1    5-azacytidine\n",
       "2        Sorafenib\n",
       "3        PRT062607\n",
       "4          AZD5438\n",
       "Name: DRUG_NAME, dtype: category\n",
       "Categories (286, object): ['123138', '123829', '150412', '5-Fluorouracil', ..., 'Zoledronate', 'alpha-lipoic acid', 'ascorbate (vitamin C)', 'glutathione']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension_w_drug['DRUG_NAME'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['123138', '123829', '150412', '5-Fluorouracil', '5-azacytidine',\n",
       "       '50869', '615590', '630600', '667880', '720427',\n",
       "       ...\n",
       "       'WZ4003', 'Wee1 Inhibitor', 'Wnt-C59', 'XAV939', 'YK-4-279', 'ZM447439',\n",
       "       'Zoledronate', 'alpha-lipoic acid', 'ascorbate (vitamin C)',\n",
       "       'glutathione'],\n",
       "      dtype='object', length=286)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension_w_drug['DRUG_NAME'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_name = dimension_w_drug['DRUG_NAME'].cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = np.stack([drug_name],1)\n",
    "cats = torch.tensor(cats, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[146],\n",
       "        [  4],\n",
       "        [237],\n",
       "        ...,\n",
       "        [235],\n",
       "        [ 16],\n",
       "        [168]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous variables to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.9615e-06, -4.5133e-06, -4.2397e-06,  ...,  1.3847e-06,\n",
       "         -8.8269e-06, -7.5593e-07],\n",
       "        [-4.6223e-06,  6.0358e-06,  6.3223e-06,  ...,  3.4385e-06,\n",
       "         -9.7063e-06,  4.9300e-06],\n",
       "        [-2.8903e-06,  7.6155e-06,  9.4516e-06,  ...,  1.1552e-05,\n",
       "         -2.0000e-05,  1.8352e-05],\n",
       "        ...,\n",
       "        [-6.0534e-06,  7.8305e-06,  7.0509e-06,  ...,  1.0050e-05,\n",
       "         -1.2573e-05,  1.3342e-05],\n",
       "        [-3.2886e-06,  6.8264e-06,  7.4798e-06,  ...,  8.4601e-06,\n",
       "         -2.9261e-05,  1.7556e-05],\n",
       "        [-6.0430e-06,  9.3195e-06,  1.0771e-05,  ...,  9.7452e-06,\n",
       "         -2.3450e-05,  1.7629e-05]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conts = np.stack([dimension_w_drug[col].values for col in cont_cols], 1)\n",
    "conts = torch.tensor(conts, dtype=torch.float)\n",
    "conts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.6869],\n",
       "        [5.5159],\n",
       "        [2.4005],\n",
       "        ...,\n",
       "        [4.4400],\n",
       "        [3.6174],\n",
       "        [4.5057]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.stack([dimension_w_drug[col].values for col in label_cols], 1)\n",
    "labels = torch.tensor(labels, dtype=torch.float)\n",
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(286, 50)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_szs = [len(dimension_w_drug[col].cat.categories) for col in cat_cols]\n",
    "emb_szs = [(size, min(50, (size+1)//2)) for size in cat_szs]\n",
    "emb_szs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_szs, n_cont, out_sz, layers, p=0.5):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "        \n",
    "        layerlist = []\n",
    "        n_emb = sum((nf for ni,nf in emb_szs))\n",
    "        n_in = n_emb + n_cont\n",
    "        \n",
    "        for i in layers:\n",
    "            layerlist.append(nn.Linear(n_in,i)) \n",
    "            layerlist.append(nn.ReLU(inplace=True))\n",
    "            layerlist.append(nn.BatchNorm1d(i))\n",
    "            layerlist.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "        layerlist.append(nn.Linear(layers[-1],out_sz))\n",
    "            \n",
    "        self.layers = nn.Sequential(*layerlist)\n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x_cat[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        \n",
    "        x_cont = self.bn_cont(x_cont)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(33)\n",
    "model = TabularModel(emb_szs, conts.shape[1], 1, [200,100], p=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(286, 50)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=1074, out_features=200, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Optimizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0) # Get name device with ID '0'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform train/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = conts.shape[0]\n",
    "test_size = int(batch_size * .2)\n",
    "\n",
    "cat_train = cats[:batch_size-test_size]\n",
    "cat_test = cats[batch_size-test_size:batch_size]\n",
    "con_train = conts[:batch_size-test_size]\n",
    "con_test = conts[batch_size-test_size:batch_size]\n",
    "y_train = labels[:batch_size-test_size]\n",
    "y_test = labels[batch_size-test_size:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([126095, 1024])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0142],\n",
       "        [ 1.1932],\n",
       "        [ 2.3768],\n",
       "        ...,\n",
       "        [ 4.4400],\n",
       "        [ 3.6174],\n",
       "        [ 4.5057]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6869],\n",
       "        [ 5.5159],\n",
       "        [ 2.4005],\n",
       "        ...,\n",
       "        [-1.4227],\n",
       "        [ 5.0628],\n",
       "        [-6.8808]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1  loss: 4.06632090\n",
      "epoch:  26  loss: 3.45487690\n",
      "epoch:  51  loss: 3.13960123\n",
      "epoch:  76  loss: 2.74727631\n",
      "epoch: 101  loss: 2.33467817\n",
      "epoch: 126  loss: 2.08860517\n",
      "epoch: 151  loss: 1.98550105\n",
      "epoch: 176  loss: 1.91268742\n",
      "epoch: 201  loss: 1.84727287\n",
      "epoch: 226  loss: 1.80065322\n",
      "epoch: 251  loss: 1.76378798\n",
      "epoch: 276  loss: 1.73173845\n",
      "epoch: 301  loss: 1.70868433\n",
      "epoch: 326  loss: 1.67802823\n",
      "epoch: 351  loss: 1.66467977\n",
      "epoch: 376  loss: 1.64594555\n",
      "epoch: 401  loss: 1.63780987\n",
      "epoch: 426  loss: 1.62024534\n",
      "epoch: 451  loss: 1.60554051\n",
      "epoch: 476  loss: 1.60007894\n",
      "epoch: 500  loss: 1.58470333\n",
      "\n",
      "Duration: 1292 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 500\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    i+=1\n",
    "    y_pred = model(cat_train, con_train)\n",
    "    loss = torch.sqrt(loss_fn(y_pred, y_train)) # RMSE\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # a neat trick to save screen space:\n",
    "    if i%25 == 1:\n",
    "        print(f'epoch: {i:3}  loss: {loss.item():10.8f}')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'epoch: {i:3}  loss: {loss.item():10.8f}') # print the last line\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epoch')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs), losses)\n",
    "plt.ylabel('RMSE Loss')\n",
    "plt.xlabel('epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.59683287\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_val = model(cat_test, con_test)\n",
    "    loss = torch.sqrt(loss_fn(y_val, y_test))\n",
    "print(f'RMSE: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PREDICTED   ACTUAL     DIFF\n",
      " 1.  -2.6255  -0.0142   2.6114\n",
      " 2.   3.9421   1.1932   2.7490\n",
      " 3.   3.0200   2.3768   0.6432\n",
      " 4.   3.5226   5.8819   2.3593\n",
      " 5.   3.4249   3.6806   0.2557\n",
      " 6.   1.9661   5.3496   3.3835\n",
      " 7.   4.2164   4.4454   0.2290\n",
      " 8.   3.3988   3.5117   0.1128\n",
      " 9.   2.4886  -0.0613   2.5499\n",
      "10.   1.0357   0.3543   0.6814\n",
      "11.   5.2798   4.5224   0.7574\n",
      "12.   3.8675   4.5157   0.6482\n",
      "13.   4.6439   6.0162   1.3723\n",
      "14.   3.4057   5.7954   2.3897\n",
      "15.   3.8820   4.7382   0.8562\n",
      "16.   1.4867  -0.0545   1.5412\n",
      "17.   2.3678   2.1559   0.2119\n",
      "18.   5.3248   8.0332   2.7084\n",
      "19.   5.3819   5.4961   0.1142\n",
      "20.   3.3323   4.7903   1.4581\n",
      "21.   1.2468  -0.1294   1.3762\n",
      "22.   2.9470   1.2184   1.7286\n",
      "23.   0.7092   0.8689   0.1597\n",
      "24.   1.3022   1.0359   0.2663\n",
      "25.   0.9250   1.9203   0.9953\n",
      "26.   5.1420   4.8219   0.3201\n",
      "27.   4.5862   4.1070   0.4792\n",
      "28.   3.4028   7.2403   3.8375\n",
      "29.   2.8843   2.6463   0.2380\n",
      "30.   0.0118  -0.1525   0.1643\n",
      "31.   3.2461   2.1100   1.1361\n",
      "32.   2.3109   4.5645   2.2536\n",
      "33.   5.4761   4.2704   1.2057\n",
      "34.  -3.1701  -3.9058   0.7358\n",
      "35.   4.5706   5.1756   0.6050\n",
      "36.   3.0488   2.1703   0.8786\n",
      "37.   0.9248  -0.6538   1.5786\n",
      "38.   1.9817   4.6825   2.7009\n",
      "39.   9.6527  10.8128   1.1601\n",
      "40.   3.5287   2.9954   0.5333\n",
      "41.   4.6255   4.2600   0.3654\n",
      "42.  -0.1773  -0.6616   0.4844\n",
      "43.   4.7735   4.3597   0.4138\n",
      "44.   4.4474   2.9767   1.4707\n",
      "45.   2.8420   4.5672   1.7252\n",
      "46.  -0.0880  -1.4572   1.3692\n",
      "47.   4.4434   4.2126   0.2308\n",
      "48.   4.1335   4.1256   0.0079\n",
      "49.   1.7148   4.7705   3.0557\n",
      "50.   4.6856  -1.7158   6.4014\n",
      "51.   3.4365   5.4410   2.0044\n",
      "52.   3.4363   3.1801   0.2562\n",
      "53.   3.2683   4.5552   1.2869\n",
      "54.   4.0998   3.3735   0.7264\n",
      "55.   0.2267   0.2321   0.0054\n",
      "56.   3.9907   3.5578   0.4329\n",
      "57.  -0.0392  -1.7567   1.7175\n",
      "58.   1.8276   4.8558   3.0281\n",
      "59.  -1.6566  -0.2520   1.4045\n",
      "60.   0.1684   1.7567   1.5883\n",
      "61.   3.4883   7.2025   3.7142\n",
      "62.  -2.9727   0.7184   3.6911\n",
      "63.  -0.0642  -0.4597   0.3955\n",
      "64.   0.6434  -1.0458   1.6892\n",
      "65.   3.3865   1.9478   1.4387\n",
      "66.   2.5291   0.7425   1.7866\n",
      "67.   2.0323   2.0813   0.0490\n",
      "68.   4.3822   7.1752   2.7930\n",
      "69.   4.9471   4.6230   0.3241\n",
      "70.   5.4983   4.4904   1.0079\n",
      "71.   1.5453   0.9857   0.5596\n",
      "72.   3.1505   3.6313   0.4808\n",
      "73.   3.1148   3.1969   0.0821\n",
      "74.   5.1607   4.9916   0.1690\n",
      "75.   3.6827   6.4046   2.7219\n",
      "76.   2.4587   4.2312   1.7725\n",
      "77.   0.8517   2.0933   1.2416\n",
      "78.   4.5079   5.3901   0.8822\n",
      "79.   3.3039   1.8693   1.4346\n",
      "80.  -0.4315   0.4460   0.8775\n",
      "81.  -3.1364  -2.2371   0.8993\n",
      "82.  -3.4331  -0.3685   3.0646\n",
      "83.   4.1889   2.3777   1.8112\n",
      "84.   1.9814   2.5200   0.5386\n",
      "85.   6.2086   6.4355   0.2269\n",
      "86.   1.5959   3.4798   1.8839\n",
      "87.  -3.8917  -5.6867   1.7950\n",
      "88.   2.4870   2.8142   0.3272\n",
      "89.   3.5855   5.4806   1.8951\n",
      "90.   1.4140   2.7529   1.3389\n",
      "91.   2.4272   2.5586   0.1314\n",
      "92.   2.2773   2.8840   0.6066\n",
      "93.  10.4015   9.0761   1.3254\n",
      "94.   4.8353   6.7056   1.8703\n",
      "95.   0.2844   1.4214   1.1370\n",
      "96.   4.7995   5.3591   0.5596\n",
      "97.   2.4454   2.1111   0.3342\n",
      "98.   3.4551   2.7930   0.6621\n",
      "99.   1.7692   4.5763   2.8071\n",
      "100.   4.4096   3.9726   0.4370\n"
     ]
    }
   ],
   "source": [
    "print(f'{\"PREDICTED\":>12} {\"ACTUAL\":>8} {\"DIFF\":>8}')\n",
    "for i in range(50):\n",
    "    diff = np.abs(y_val[i].item()-y_test[i].item())\n",
    "    print(f'{i+1:2}. {y_val[i].item():8.4f} {y_test[i].item():8.4f} {diff:8.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Max Scale X and extract y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "col = dimension_w_drug.drop(columns=['LN_IC50']).columns\n",
    "X = scaler.fit_transform(dimension_w_drug.drop(columns=['LN_IC50']))\n",
    "X = pd.DataFrame(X, columns=col)\n",
    "y = dimension_w_drug['LN_IC50']\n",
    "\n",
    "scaled_df = X\n",
    "scaled_df['LN_IC50'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>DRUG_NAME_Wee1 Inhibitor</th>\n",
       "      <th>DRUG_NAME_Wnt-C59</th>\n",
       "      <th>DRUG_NAME_XAV939</th>\n",
       "      <th>DRUG_NAME_YK-4-279</th>\n",
       "      <th>DRUG_NAME_ZM447439</th>\n",
       "      <th>DRUG_NAME_Zoledronate</th>\n",
       "      <th>DRUG_NAME_alpha-lipoic acid</th>\n",
       "      <th>DRUG_NAME_ascorbate (vitamin C)</th>\n",
       "      <th>DRUG_NAME_glutathione</th>\n",
       "      <th>LN_IC50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.098109</td>\n",
       "      <td>0.973217</td>\n",
       "      <td>0.934634</td>\n",
       "      <td>0.082488</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>0.057198</td>\n",
       "      <td>0.040924</td>\n",
       "      <td>0.983428</td>\n",
       "      <td>0.953679</td>\n",
       "      <td>0.993825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.902460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.098109</td>\n",
       "      <td>0.973217</td>\n",
       "      <td>0.934634</td>\n",
       "      <td>0.082488</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>0.057198</td>\n",
       "      <td>0.040924</td>\n",
       "      <td>0.983428</td>\n",
       "      <td>0.953679</td>\n",
       "      <td>0.993825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.384381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.098109</td>\n",
       "      <td>0.973217</td>\n",
       "      <td>0.934634</td>\n",
       "      <td>0.082488</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>0.057198</td>\n",
       "      <td>0.040924</td>\n",
       "      <td>0.983428</td>\n",
       "      <td>0.953679</td>\n",
       "      <td>0.993825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.372862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.098109</td>\n",
       "      <td>0.973217</td>\n",
       "      <td>0.934634</td>\n",
       "      <td>0.082488</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>0.057198</td>\n",
       "      <td>0.040924</td>\n",
       "      <td>0.983428</td>\n",
       "      <td>0.953679</td>\n",
       "      <td>0.993825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.189364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.098109</td>\n",
       "      <td>0.973217</td>\n",
       "      <td>0.934634</td>\n",
       "      <td>0.082488</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>0.057198</td>\n",
       "      <td>0.040924</td>\n",
       "      <td>0.983428</td>\n",
       "      <td>0.953679</td>\n",
       "      <td>0.993825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.693711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157613</th>\n",
       "      <td>0.096684</td>\n",
       "      <td>0.982266</td>\n",
       "      <td>0.919321</td>\n",
       "      <td>0.076251</td>\n",
       "      <td>0.038997</td>\n",
       "      <td>0.069472</td>\n",
       "      <td>0.093618</td>\n",
       "      <td>0.937792</td>\n",
       "      <td>0.913322</td>\n",
       "      <td>0.953113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.556376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157614</th>\n",
       "      <td>0.096684</td>\n",
       "      <td>0.982266</td>\n",
       "      <td>0.919321</td>\n",
       "      <td>0.076251</td>\n",
       "      <td>0.038997</td>\n",
       "      <td>0.069472</td>\n",
       "      <td>0.093618</td>\n",
       "      <td>0.937792</td>\n",
       "      <td>0.913322</td>\n",
       "      <td>0.953113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.036685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157615</th>\n",
       "      <td>0.096684</td>\n",
       "      <td>0.982266</td>\n",
       "      <td>0.919321</td>\n",
       "      <td>0.076251</td>\n",
       "      <td>0.038997</td>\n",
       "      <td>0.069472</td>\n",
       "      <td>0.093618</td>\n",
       "      <td>0.937792</td>\n",
       "      <td>0.913322</td>\n",
       "      <td>0.953113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.217061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157616</th>\n",
       "      <td>0.096684</td>\n",
       "      <td>0.982266</td>\n",
       "      <td>0.919321</td>\n",
       "      <td>0.076251</td>\n",
       "      <td>0.038997</td>\n",
       "      <td>0.069472</td>\n",
       "      <td>0.093618</td>\n",
       "      <td>0.937792</td>\n",
       "      <td>0.913322</td>\n",
       "      <td>0.953113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.823341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157617</th>\n",
       "      <td>0.096684</td>\n",
       "      <td>0.982266</td>\n",
       "      <td>0.919321</td>\n",
       "      <td>0.076251</td>\n",
       "      <td>0.038997</td>\n",
       "      <td>0.069472</td>\n",
       "      <td>0.093618</td>\n",
       "      <td>0.937792</td>\n",
       "      <td>0.913322</td>\n",
       "      <td>0.953113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.995125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157618 rows Ã— 415 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0       0.098109  0.973217  0.934634  0.082488  0.003494  0.057198  0.040924   \n",
       "1       0.098109  0.973217  0.934634  0.082488  0.003494  0.057198  0.040924   \n",
       "2       0.098109  0.973217  0.934634  0.082488  0.003494  0.057198  0.040924   \n",
       "3       0.098109  0.973217  0.934634  0.082488  0.003494  0.057198  0.040924   \n",
       "4       0.098109  0.973217  0.934634  0.082488  0.003494  0.057198  0.040924   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "157613  0.096684  0.982266  0.919321  0.076251  0.038997  0.069472  0.093618   \n",
       "157614  0.096684  0.982266  0.919321  0.076251  0.038997  0.069472  0.093618   \n",
       "157615  0.096684  0.982266  0.919321  0.076251  0.038997  0.069472  0.093618   \n",
       "157616  0.096684  0.982266  0.919321  0.076251  0.038997  0.069472  0.093618   \n",
       "157617  0.096684  0.982266  0.919321  0.076251  0.038997  0.069472  0.093618   \n",
       "\n",
       "               7         8         9  ...  DRUG_NAME_Wee1 Inhibitor  \\\n",
       "0       0.983428  0.953679  0.993825  ...                       0.0   \n",
       "1       0.983428  0.953679  0.993825  ...                       0.0   \n",
       "2       0.983428  0.953679  0.993825  ...                       0.0   \n",
       "3       0.983428  0.953679  0.993825  ...                       0.0   \n",
       "4       0.983428  0.953679  0.993825  ...                       0.0   \n",
       "...          ...       ...       ...  ...                       ...   \n",
       "157613  0.937792  0.913322  0.953113  ...                       0.0   \n",
       "157614  0.937792  0.913322  0.953113  ...                       0.0   \n",
       "157615  0.937792  0.913322  0.953113  ...                       0.0   \n",
       "157616  0.937792  0.913322  0.953113  ...                       0.0   \n",
       "157617  0.937792  0.913322  0.953113  ...                       0.0   \n",
       "\n",
       "        DRUG_NAME_Wnt-C59  DRUG_NAME_XAV939  DRUG_NAME_YK-4-279  \\\n",
       "0                     0.0               0.0                 0.0   \n",
       "1                     0.0               0.0                 0.0   \n",
       "2                     0.0               0.0                 0.0   \n",
       "3                     0.0               0.0                 0.0   \n",
       "4                     0.0               0.0                 0.0   \n",
       "...                   ...               ...                 ...   \n",
       "157613                0.0               0.0                 0.0   \n",
       "157614                0.0               0.0                 0.0   \n",
       "157615                0.0               0.0                 0.0   \n",
       "157616                0.0               0.0                 0.0   \n",
       "157617                0.0               0.0                 0.0   \n",
       "\n",
       "        DRUG_NAME_ZM447439  DRUG_NAME_Zoledronate  \\\n",
       "0                      0.0                    0.0   \n",
       "1                      0.0                    0.0   \n",
       "2                      0.0                    0.0   \n",
       "3                      0.0                    0.0   \n",
       "4                      0.0                    0.0   \n",
       "...                    ...                    ...   \n",
       "157613                 0.0                    0.0   \n",
       "157614                 0.0                    0.0   \n",
       "157615                 0.0                    0.0   \n",
       "157616                 0.0                    0.0   \n",
       "157617                 0.0                    0.0   \n",
       "\n",
       "        DRUG_NAME_alpha-lipoic acid  DRUG_NAME_ascorbate (vitamin C)  \\\n",
       "0                               0.0                              0.0   \n",
       "1                               0.0                              0.0   \n",
       "2                               0.0                              0.0   \n",
       "3                               0.0                              0.0   \n",
       "4                               0.0                              0.0   \n",
       "...                             ...                              ...   \n",
       "157613                          0.0                              0.0   \n",
       "157614                          0.0                              1.0   \n",
       "157615                          0.0                              0.0   \n",
       "157616                          1.0                              0.0   \n",
       "157617                          0.0                              0.0   \n",
       "\n",
       "        DRUG_NAME_glutathione    LN_IC50  \n",
       "0                         0.0  -2.902460  \n",
       "1                         0.0  -4.384381  \n",
       "2                         0.0   3.372862  \n",
       "3                         0.0   4.189364  \n",
       "4                         0.0  -3.693711  \n",
       "...                       ...        ...  \n",
       "157613                    0.0   6.556376  \n",
       "157614                    0.0  12.036685  \n",
       "157615                    1.0  11.217061  \n",
       "157616                    0.0   8.823341  \n",
       "157617                    0.0   9.995125  \n",
       "\n",
       "[157618 rows x 415 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor Portion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,df):\n",
    " \n",
    "        col = df.drop(columns=['LN_IC50']).columns\n",
    "        x = df.drop(columns=['LN_IC50'])\n",
    "        y = df['LN_IC50']\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "    \n",
    "        self.x_train=torch.tensor(x,dtype=torch.float32)\n",
    "        self.y_train=torch.tensor(y,dtype=torch.float32)\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "   \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_train[idx],self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare full dataset\n",
    "full_dataset = MyDataset(scaled_df)\n",
    "\n",
    "# split dataset to train and test\n",
    "dim_np = np.array(scaled_df)\n",
    "dim_col = scaled_df.columns\n",
    "train, test = train_test_split(dim_np, train_size=0.8, shuffle=True, random_state=0)\n",
    "train = pd.DataFrame(train, columns=dim_col)\n",
    "test = pd.DataFrame(test, columns=dim_col)\n",
    "# declare train and test dataset\n",
    "train_dataset = MyDataset(train)\n",
    "test_dataset = MyDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_x_train = train.drop(columns=['LN_IC50'])\n",
    "xgb_y_train = train['LN_IC50']\n",
    "\n",
    "xgb_x_test = test.drop(columns=['LN_IC50'])\n",
    "xgb_y_test = test['LN_IC50']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_r = xgb.XGBRegressor()\n",
    "xgb_r.fit(xgb_x_train, xgb_y_train)\n",
    "\n",
    "y_pred = xgb_r.predict(xgb_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0796635844580593"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_tensor = torch.tensor(y_pred)\n",
    "y_tensor = torch.tensor(xgb_y_test)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "loss_function(y_pred_tensor, y_tensor).item()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    " \n",
    "class predictor_model(nn.Module):\n",
    "  def __init__(self):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.layers = nn.Sequential(\n",
    "    nn.Linear(414, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare reset weights function for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "   if hasattr(layer, 'reset_parameters'):\n",
    "    print(f'Reset trainable parameters of layer = {layer}')\n",
    "    layer.reset_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = predictor_model()\n",
    "k_folds = 5\n",
    "n_epochs = 20\n",
    "torch.manual_seed(0)\n",
    "batch_size = 10\n",
    "lr = 0.0001\n",
    "kfold = KFold(n_splits=k_folds, shuffle = True)\n",
    "best_mse = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "device = torch.device('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader):\n",
    "    count = 0\n",
    "    model.train()\n",
    "    current_loss = 0.0\n",
    "    for i, data in enumerate(dataloader,0):\n",
    "        x, y = data\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        \n",
    "        output = model(x)\n",
    "        loss = loss_function(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        count = count + 1\n",
    "\n",
    "    overall_loss = current_loss/count\n",
    "\n",
    "    return overall_loss\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    count = 0\n",
    "    model.eval()\n",
    "    current_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader,0):\n",
    "            x, y = data\n",
    "        \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            output = model(x)\n",
    "            loss = loss_function(output, y)\n",
    "\n",
    "            current_loss += loss.item()\n",
    "            count = count + 1\n",
    "\n",
    "    overall_loss = current_loss/count\n",
    "\n",
    "    return overall_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_new(model, X_test, y_test,best_mse, history,best_weights):\n",
    "    model.eval()\n",
    "    output = model(X_test)\n",
    "    mse = loss_function(output, y_test)\n",
    "    mse = float(mse)\n",
    "    history.append(mse)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    return mse, best_mse, history, best_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Training MSE loss for epoch 1: 7.6096666574961755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mdzak\\anaconda3\\envs\\fyp\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([31524])) that is different to the input size (torch.Size([31524, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE loss for epoch 1: 7.695577144622803\n",
      "Starting epoch 2\n",
      "Training MSE loss for epoch 2: 7.609565179198073\n",
      "Validation MSE loss for epoch 2: 7.6954169273376465\n",
      "Starting epoch 3\n",
      "Training MSE loss for epoch 3: 7.609464114384526\n",
      "Validation MSE loss for epoch 3: 7.695092678070068\n",
      "Starting epoch 4\n",
      "Training MSE loss for epoch 4: 7.6093551812742595\n",
      "Validation MSE loss for epoch 4: 7.69457483291626\n",
      "Starting epoch 5\n",
      "Training MSE loss for epoch 5: 7.609242495368509\n",
      "Validation MSE loss for epoch 5: 7.693907737731934\n",
      "Starting epoch 6\n",
      "Training MSE loss for epoch 6: 7.6091383731872995\n",
      "Validation MSE loss for epoch 6: 7.693153381347656\n",
      "Starting epoch 7\n",
      "Training MSE loss for epoch 7: 7.609040229364051\n",
      "Validation MSE loss for epoch 7: 7.692287921905518\n",
      "Starting epoch 8\n",
      "Training MSE loss for epoch 8: 7.608942730673428\n",
      "Validation MSE loss for epoch 8: 7.6913533210754395\n",
      "Starting epoch 9\n",
      "Training MSE loss for epoch 9: 7.6088531989354875\n",
      "Validation MSE loss for epoch 9: 7.690409183502197\n",
      "Starting epoch 10\n",
      "Training MSE loss for epoch 10: 7.608768327724619\n",
      "Validation MSE loss for epoch 10: 7.68941593170166\n",
      "Starting epoch 11\n",
      "Training MSE loss for epoch 11: 7.608682632446289\n",
      "Validation MSE loss for epoch 11: 7.688345432281494\n",
      "Starting epoch 12\n",
      "Training MSE loss for epoch 12: 7.60859625963362\n",
      "Validation MSE loss for epoch 12: 7.687211990356445\n",
      "Starting epoch 13\n",
      "Training MSE loss for epoch 13: 7.608507413651348\n",
      "Validation MSE loss for epoch 13: 7.686023712158203\n",
      "Starting epoch 14\n",
      "Training MSE loss for epoch 14: 7.60841904547345\n",
      "Validation MSE loss for epoch 14: 7.684844970703125\n",
      "Starting epoch 15\n",
      "Training MSE loss for epoch 15: 7.608332468094739\n",
      "Validation MSE loss for epoch 15: 7.683629989624023\n",
      "Starting epoch 16\n",
      "Training MSE loss for epoch 16: 7.608245772715756\n",
      "Validation MSE loss for epoch 16: 7.682466983795166\n",
      "Starting epoch 17\n",
      "Training MSE loss for epoch 17: 7.60816561813045\n",
      "Validation MSE loss for epoch 17: 7.681407451629639\n",
      "Starting epoch 18\n",
      "Training MSE loss for epoch 18: 7.608094915173359\n",
      "Validation MSE loss for epoch 18: 7.680484771728516\n",
      "Starting epoch 19\n",
      "Training MSE loss for epoch 19: 7.6080276467979076\n",
      "Validation MSE loss for epoch 19: 7.679516792297363\n",
      "Starting epoch 20\n",
      "Training MSE loss for epoch 20: 7.607955715477345\n",
      "Validation MSE loss for epoch 20: 7.678501605987549\n"
     ]
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "best_mse = np.inf\n",
    "history = []\n",
    "best_weights = None\n",
    "for epoch in range(0, n_epochs):\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    train_loss = train(model, train_loader)\n",
    "    train_loss_list.append(train_loss)\n",
    "    print(f'Training MSE loss for epoch {epoch+1}: ' + str(train_loss))\n",
    "\n",
    "    X_test = torch.tensor(np.array(xgb_x_test), dtype=torch.float32)\n",
    "    y_test = torch.tensor(np.array(xgb_y_test), dtype=torch.float32)\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    val_loss, best_mse, history, best_weights = validate_new(model, X_test, y_test, best_mse, history, best_weights)\n",
    "    val_loss_list.append(val_loss)\n",
    "    print(f'Validation MSE loss for epoch {epoch+1}: ' + str(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mdzak\\anaconda3\\envs\\fyp\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\mdzak\\anaconda3\\envs\\fyp\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE loss for epoch 1: 7.6495434621973795\n",
      "Validation MSE loss for epoch 1: 7.6443190583152845\n",
      "Starting epoch 2\n",
      "Training MSE loss for epoch 2: 7.5672776602639935\n",
      "Validation MSE loss for epoch 2: 7.629202788986935\n",
      "Starting epoch 3\n",
      "Training MSE loss for epoch 3: 7.559499932010197\n",
      "Validation MSE loss for epoch 3: 7.625607852118377\n",
      "Starting epoch 4\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(0, n_epochs):\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    train_loss = train(model, train_loader)\n",
    "    train_loss_list.append(train_loss)\n",
    "    print(f'Training MSE loss for epoch {epoch+1}: ' + str(train_loss))\n",
    "\n",
    "    val_loss = validate(model, test_loader)\n",
    "    val_loss_list.append(val_loss)\n",
    "    print(f'Validation MSE loss for epoch {epoch+1}: ' + str(val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
